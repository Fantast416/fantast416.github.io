<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Fantast&#39;s Blog</title>
  
  
  <link href="https://blog.slks.xyz/atom.xml" rel="self"/>
  
  <link href="https://blog.slks.xyz/"/>
  <updated>2022-01-28T15:36:01.413Z</updated>
  <id>https://blog.slks.xyz/</id>
  
  <author>
    <name>Fantast</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>java系列笔记1——java基础(更新中)</title>
    <link href="https://blog.slks.xyz/2022/01/28/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B01%E2%80%94java%E5%9F%BA%E7%A1%80/"/>
    <id>https://blog.slks.xyz/2022/01/28/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B01%E2%80%94java%E5%9F%BA%E7%A1%80/</id>
    <published>2022-01-28T15:34:19.000Z</published>
    <updated>2022-01-28T15:36:01.413Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一第一个java程序">一、第一个Java程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Hello, world!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​ java规定，某个类定义的<code>public static void main(String[] args)</code>是java程序的固定入口方法，因此，java程序总是从<code>main</code>方法开始执行。</p><h3 id="二如何运行java程序">二、如何运行Java程序：</h3><p>1、用javac把Hello.java编译成字节码文件Hello.class，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> javac Hello.java</span></span><br></pre></td></tr></table></figure><p>2、然后用java命令执行这个字节码文件。javac是编译器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> java Hello</span></span><br><span class="line">Hello, world!</span><br></pre></td></tr></table></figure><p>给虚拟机传递的参数<code>Hello</code>是我们定义的类名，虚拟机自动查找对应的class文件并执行</p><p><strong>注意</strong>：</p><p>一个Java源码只能定义一个<code>public</code>类型的class，并且class名称和文件名要完全一致；</p><p>使用<code>javac</code>可以将<code>.java</code>源码编译成<code>.class</code>字节码；</p><p>使用<code>java</code>可以运行一个已编译的Java程序，参数是类名。</p><h3 id="三程序基本结构">三、程序基本结构：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 可以用来自动创建文档的注释</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 向屏幕输出文本:</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Hello, world!&quot;</span>);</span><br><span class="line">        <span class="comment">/* 多行注释开始</span></span><br><span class="line"><span class="comment">        注释内容</span></span><br><span class="line"><span class="comment">        注释结束 */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="comment">// class定义结束</span></span><br></pre></td></tr></table></figure><p><code>public</code>是访问修饰符，表示该<code>class</code>是公开的。不写<code>public</code>，也能正确编译，但是这个类将无法从命令行执行。</p><h3 id="四数据类型">四、数据类型</h3><p><strong>基本数据类型：</strong></p><ul><li>整数类型：byte，short，int，long</li><li>浮点数类型：float，double</li><li>字符类型：char</li><li>布尔类型：boolean</li></ul><p><strong>引用类型</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">String s = <span class="string">&quot;hello&quot;</span>;</span><br></pre></td></tr></table></figure><p>引用类型的变量类似于C语言的指针，它内部存储一个“地址”，指向某个对象在内存的位置</p><p><strong>常量</strong>：</p><p>如果加上<code>final</code>修饰符，这个变量就变成了常量：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="keyword">double</span> PI = <span class="number">3.14</span>; <span class="comment">// PI是一个常量</span></span><br></pre></td></tr></table></figure><p><strong>var关键字:</strong></p><p>如果想省略变量类型，可以使用<code>var</code>关键字</p><h3 id="五浮点数运算">五、浮点数运算：</h3><p>​ 由于浮点数存在运算误差，所以比较两个浮点数是否相等常常会出现错误的结果。正确的比较方法是判断两个浮点数之差的绝对值是否小于一个很小的数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">double</span> r = Math.abs(x - y);</span><br><span class="line"><span class="comment">// 再判断绝对值是否足够小:</span></span><br><span class="line"><span class="keyword">if</span> (r &lt; <span class="number">0.00001</span>) &#123;</span><br><span class="line">    <span class="comment">// 可以认为相等</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 不相等</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>整数运算在除数为<code>0</code>时会报错，而浮点数运算在除数为<code>0</code>时，不会报错，但会返回几个特殊值：</p><ul><li><code>NaN</code>表示Not a Number</li><li><code>Infinity</code>表示无穷大</li><li><code>-Infinity</code>表示负无穷大</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">double</span> d1 = <span class="number">0.0</span> / <span class="number">0</span>; <span class="comment">// NaN</span></span><br><span class="line"><span class="keyword">double</span> d2 = <span class="number">1.0</span> / <span class="number">0</span>; <span class="comment">// Infinity</span></span><br><span class="line"><span class="keyword">double</span> d3 = -<span class="number">1.0</span> / <span class="number">0</span>; <span class="comment">// -Infinity</span></span><br></pre></td></tr></table></figure><h3 id="六字符类型">六、字符类型：</h3><p>一个<code>char</code>保存一个Unicode字符：</p><p>因为Java在内存中总是使用Unicode表示字符，所以，一个英文字符和一个中文字符都用一个<code>char</code>类型表示，它们都占用两个字节。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">int n1 = &#x27;A&#x27;; // 字母“A”的Unicodde编码是65</span><br><span class="line">int n2 = &#x27;中&#x27;; // 汉字“中”的Unicode编码是20013</span><br><span class="line">还可以直接用转义字符\u+Unicode编码来表示一个字符：</span><br><span class="line">// 注意是十六进制:</span><br><span class="line">char c3 = &#x27;\u0041&#x27;; // &#x27;A&#x27;，因为十六进制0041 = 十进制65</span><br><span class="line">char c4 = &#x27;\u4e2d&#x27;; // &#x27;中&#x27;，因为十六进制4e2d = 十进制20013</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果用<code>+</code>连接字符串和其他数据类型，会将其他数据类型先自动转型为字符串，再连接：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> age = <span class="number">25</span>;</span><br><span class="line">        String s = <span class="string">&quot;age is &quot;</span> + age;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从Java 13开始，字符串可以用<code>"""..."""</code>表示多行字符串（Text Blocks）了。</p><p>多行字符串前面共同的空格会被去掉，即：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String s = <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                   SELECT * FROM</span></span><br><span class="line"><span class="string">                     users</span></span><br><span class="line"><span class="string">                   WHERE id &gt; 100</span></span><br><span class="line"><span class="string">                   ORDER BY name DESC</span></span><br><span class="line"><span class="string">                   &quot;</span><span class="string">&quot;&quot;</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>不可见特性：</strong></p><p>Java的字符串除了是一个引用类型外，还有个重要特点，就是字符串不可变。考察以下代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String s = <span class="string">&quot;hello&quot;</span>;</span><br><span class="line">        System.out.println(s); <span class="comment">// 显示 hello</span></span><br><span class="line">        s = <span class="string">&quot;world&quot;</span>;</span><br><span class="line">        System.out.println(s); <span class="comment">// 显示 world</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>观察执行结果，难道字符串<code>s</code>变了吗？其实变的不是字符串，而是变量<code>s</code>的“指向”。</p><p><strong>空值null:</strong></p><p>引用类型的变量可以指向一个空值<code>null</code>，它表示不存在，即该变量不指向任何对象。例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String s1 = <span class="keyword">null</span>; <span class="comment">// s1是null</span></span><br><span class="line">String s2; <span class="comment">// 没有赋初值值，s2也是null</span></span><br></pre></td></tr></table></figure><h3 id="七数组">七、数组：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span>[] ns = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">5</span>];</span><br><span class="line"><span class="keyword">int</span>[] ns = &#123; <span class="number">68</span>, <span class="number">79</span>, <span class="number">91</span>, <span class="number">85</span>, <span class="number">62</span> &#125;;</span><br><span class="line">System.out.println(ns.length); <span class="comment">// 5</span></span><br></pre></td></tr></table></figure><h3 id="八流程控制手段">八、流程控制手段</h3><h4 id="输出">1、输出</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">System.out.print(<span class="string">&quot;A,&quot;</span>); <span class="comment">//输出后不换行</span></span><br><span class="line">System.out.println() <span class="comment">//输出后自动换行</span></span><br><span class="line">System.out.printf(<span class="string">&quot;%.2f\n&quot;</span>, d); <span class="comment">// 格式化输出，显示两位小数3.14</span></span><br></pre></td></tr></table></figure><h4 id="输入">2、输入：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in); <span class="comment">// 创建Scanner对象</span></span><br><span class="line">        System.out.print(<span class="string">&quot;Input your name: &quot;</span>); <span class="comment">// 打印提示</span></span><br><span class="line">        String name = scanner.nextLine(); <span class="comment">// 读取一行输入并获取字符串</span></span><br><span class="line">        System.out.print(<span class="string">&quot;Input your age: &quot;</span>); <span class="comment">// 打印提示</span></span><br><span class="line">        <span class="keyword">int</span> age = scanner.nextInt(); <span class="comment">// 读取一行输入并获取整数</span></span><br><span class="line">        System.out.printf(<span class="string">&quot;Hi, %s, you are %d\n&quot;</span>, name, age); <span class="comment">// 格式化输出</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>通过<code>import</code>语句导入<code>java.util.Scanner</code></li><li>创建<code>Scanner</code>对象并传入<code>System.in</code></li><li>有了<code>Scanner</code>对象后，要读取用户输入的字符串，使用<code>scanner.nextLine()</code>，要读取用户输入的整数，使用<code>scanner.nextInt()</code>。<code>Scanner</code>会自动转换数据类型，因此不必手动转换。</li></ul><h3 id="九判断引用类型变量内容是否相等">九、判断引用类型变量内容是否相等：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String s1 = <span class="string">&quot;hello&quot;</span>;</span><br><span class="line">        String s2 = <span class="string">&quot;HELLO&quot;</span>.toLowerCase();</span><br><span class="line">        System.out.println(s1);</span><br><span class="line">        System.out.println(s2);</span><br><span class="line">        <span class="keyword">if</span> (s1 == s2) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;s1 == s2&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;s1 != s2&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 输出结果是 s1 != s2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>要判断引用类型的变量内容是否相等，必须使用<code>equals()</code>方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s1.equals(s2)</span><br></pre></td></tr></table></figure><h3 id="十新switch表达式">十、新Switch表达式：</h3><p>​ 从Java 12开始，<code>switch</code>语句升级为更简洁的表达式语法，使用类似模式匹配（Pattern Matching）的方法，保证只有一种路径会被执行，并且不需要<code>break</code>语句</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">String fruit = <span class="string">&quot;apple&quot;</span>;</span><br><span class="line"><span class="keyword">switch</span> (fruit) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;apple&quot;</span> -&gt; System.out.println(<span class="string">&quot;Selected apple&quot;</span>);</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;pear&quot;</span> -&gt; System.out.println(<span class="string">&quot;Selected pear&quot;</span>);</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;mango&quot;</span> -&gt; &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Selected mango&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;Good choice!&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">default</span> -&gt; System.out.println(<span class="string">&quot;No fruit selected&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="十一switch中的yield返回值">十一、Switch中的Yield返回值：</h3><p>大多数时候，在<code>switch</code>表达式内部，我们会返回简单的值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String fruit = <span class="string">&quot;orange&quot;</span>;</span><br><span class="line">        <span class="keyword">int</span> opt = <span class="keyword">switch</span> (fruit) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;apple&quot;</span> -&gt; <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;pear&quot;</span>, <span class="string">&quot;mango&quot;</span> -&gt; <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">default</span> -&gt; &#123;</span><br><span class="line">                <span class="keyword">int</span> code = fruit.hashCode();</span><br><span class="line">                yield code; <span class="comment">// switch语句返回值</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        System.out.println(<span class="string">&quot;opt = &quot;</span> + opt);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="十二for-each-循环">十二、for each 循环</h3><p>和<code>for</code>循环相比，<code>for each</code>循环的变量n不再是计数器，而是直接对应到数组的每个元素。<code>for each</code>循环的写法也更简洁。但是，<code>for each</code>循环无法指定遍历顺序，也无法获取数组的索引。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] ns = &#123; <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span> &#125;;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> n : ns) &#123;</span><br><span class="line">            System.out.println(n);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="十三">十三、</h3>]]></content>
    
    
    <summary type="html">本篇笔记为java系列笔记基础内容，包含java程序基本结构、数据类型、浮点数、字符、数组、输入输出、引用类型变量、Switch表达式、Yield返回值、For each循环等多个需要注意的基础知识点</summary>
    
    
    
    <category term="java系列笔记" scheme="https://blog.slks.xyz/categories/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="java" scheme="https://blog.slks.xyz/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>《UniFormer:Unifying Convolution and Self-attention for Visual Recognition》(更新中)</title>
    <link href="https://blog.slks.xyz/2022/01/27/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B06%E2%80%94%E2%80%94%E3%80%8AUniFormer%20Unifying%20Convolution%20and%20Self%20Attention%20for%20Visual%20Recognition%E3%80%8B/"/>
    <id>https://blog.slks.xyz/2022/01/27/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B06%E2%80%94%E2%80%94%E3%80%8AUniFormer%20Unifying%20Convolution%20and%20Self%20Attention%20for%20Visual%20Recognition%E3%80%8B/</id>
    <published>2022-01-27T10:12:19.000Z</published>
    <updated>2022-01-28T15:18:23.490Z</updated>
    
    <content type="html"><![CDATA[<h4 id="论文名称uniformer-unifying-convolution-and-self-attention-for-visual-recognition">论文名称：《UniFormer: Unifying Convolution and Self-attention for Visual Recognition》</h4><h4 id="论文地址-httpsarxiv.orgpdf2201.09450.pdf">论文地址： https://arxiv.org/pdf/2201.09450.pdf</h4><h2 id="关键词">1、关键词：</h2><h2 id="领域背景">2、领域背景—：</h2><h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2><p>​ CNN能够通过小范围的卷积，降低局部冗余，但在捕获全局依赖中很有限制。</p><p>​ ViT能够通过Self-Attention很有效的捕获长距离的依赖，但是盲目的去计算所有tokens之间的相似关系，会带来很大的计算冗余。</p><h2 id="主要设计思想">4、主要设计思想：</h2><p>​ 提出了UniFormer，将3D卷积和时空自注意力机制结合在一个简洁的<a href="https://so.csdn.net/so/search?q=transformer&amp;spm=1001.2101.3001.7020">transformer</a>结构中。UniFormer与其他transformer的区别主要在于：</p><ul><li>relation aggregator分别处理 video redundancy and dependency，而不是在所有层都使用自注意力机制。在浅层，aggregator利用一个小的learnable matrix学习局部的关系，通过聚合小的3D邻域的token信息极大地减少计算量。在深层，aggregator通过相似性比较学习全局关系，可以灵活的建立远距离视频帧token之间的长程依赖关系。</li><li>aggregator在所有层中同时编码时空信息</li><li>层级化构建网络结构</li></ul><h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2><h3 id="uniformer-block">1) UniFormer Block</h3><p>​ 下面图片就是UniFormer Block的整体架构，其中在标注维度的地方，所有标红的字符都是仅对于视频输入有效，代表了输入视频的帧数，如果输入是个图像，那么这些标红的值应该都 = 1.</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127163758510.png" alt="image-20220127163758510" /><figcaption aria-hidden="true">image-20220127163758510</figcaption></figure><ul><li><strong>概述</strong>：Uniformer Block 的整体架构如上所示，分割成了几个阶段，每个阶段中由三个核心的模块重复堆叠L次组成，下面就是单词堆叠中，所有的模块的大致介绍：<ul><li>Dynamic Position Embedding( DPE )</li><li>Multi-Head Relation Aggregator ( MHRA )</li><li>Feed-Forward Network ( FFN )</li><li>我们首先引入 DPE 将位置信息动态集成到所有Tokens（等式1）。它支持任意输入分辨率，并充分利用Tokens的顺序以获得更好的视觉识别效果。 然后，我们使用 MHRA ，其利用每个Token的上下文token，通过关系学习的方式，来增强每个Token（等式 2）。 通过在浅层和深层灵活设计Token的相似性，我们的 MHRA 可以巧妙地统一卷积和自注意力机制，以减少局部冗余并学习全局依赖性。 最后，我们像传统的 ViTs 一样添加 FFN，它由两个线性层和一个非线性函数GELU组成（等式 3）。 通道数先扩大4倍再恢复，因此每个token会被单独增强.</li></ul></li><li><strong>输入</strong>：<span class="math inline">\(X_{in} \in R^{C \times T \times H \times W}\)</span>, T 为视频帧数，当输入为图像时，T = 1</li><li><strong>输出</strong>：<span class="math inline">\(Z\)</span>特征空间向量</li><li><strong>公式表达</strong>：<ul><li><span class="math inline">\(X = DPE(X_{in}) + X_{in}\)</span></li><li><span class="math inline">\(Y = MHRA(Norm(X)) + X\)</span></li><li><span class="math inline">\(Z = FFN(Norm(Y)) + Y\)</span></li></ul></li></ul><h3 id="multi-head-relation-aggregator">2) Multi-Head Relation Aggregator</h3><ul><li>概述：该模块可以巧妙地统一卷积和自注意力机制，以减少局部冗余并学习全局依赖性。MHRA使用multi-head机制来计算token间的关系，公式表达如下：</li><li>公式表达：<ul><li>MHRA模块的第n个head ： <span class="math inline">\(R_n(X) = A_nV_n(X)\)</span>，输入向量<span class="math inline">\(X \in R^{C \times T \times H \times W}\)</span>，我们会将其首先Reshape成一个token的序列<span class="math inline">\(X \in R^{L \times C}\)</span>, 其中 <span class="math inline">\(L = T \times H \times W\)</span></li><li>总体： <span class="math inline">\(MHRA(X) = Concat(R_1(X);R_2(X);...;R_N(X))U\)</span>，<span class="math inline">\(U \in R^{C \times C}\)</span>是一个可学习矩阵，用于聚合N个Head的内容</li></ul></li><li>单个Head内部变换细节介绍：<ul><li>每个RA包含token context encoding 和 token affinity learning两步</li><li>1、我们应用线性变换将原始标记编码为上下文标记：<span class="math inline">\(V_n(X) \in R^{L \times \frac{C}{N}}\)</span></li><li>2、An是 token相似度，RA可以在其指导下来概括上下文信息，进行总结</li></ul></li><li>输入：</li><li>输出：</li><li>网络结构：</li></ul><h3 id="dynamic-position-embedding">3) Dynamic Position Embedding</h3><ul><li>概述：</li><li>输入：</li><li>输出：</li><li>网络结构：</li></ul>]]></content>
    
    
    <summary type="html">提出了UniFormer，将3D卷积和时空自注意力机制结合在一个简洁的transformer结构中。</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://blog.slks.xyz/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="CNN" scheme="https://blog.slks.xyz/tags/CNN/"/>
    
    <category term="Transformer" scheme="https://blog.slks.xyz/tags/Transformer/"/>
    
    <category term="Visual Recognition" scheme="https://blog.slks.xyz/tags/Visual-Recognition/"/>
    
  </entry>
  
  <entry>
    <title>《HandWritting Transformers》(更新中)</title>
    <link href="https://blog.slks.xyz/2022/01/26/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B05%E2%80%94%E2%80%94%E3%80%8AHandWrittng%20Transformers%E3%80%8B/"/>
    <id>https://blog.slks.xyz/2022/01/26/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B05%E2%80%94%E2%80%94%E3%80%8AHandWrittng%20Transformers%E3%80%8B/</id>
    <published>2022-01-26T10:12:19.000Z</published>
    <updated>2022-01-28T13:10:25.840Z</updated>
    
    <content type="html"><![CDATA[<h4 id="论文名称handwriting-transformers">论文名称：《<strong>Handwriting Transformers</strong>》</h4><h4 id="论文地址-httpsarxiv.orgabs2104.03964">论文地址： https://arxiv.org/abs/2104.03964</h4><h2 id="关键词">1、关键词：</h2><p>​ 手写字体生成（英文）、GAN、Transformer</p><h2 id="领域背景手写字体生成">2、领域背景—手写字体生成：</h2><p>​ 自动的手写文字生成对于一些书写障碍的人十分重要。通常使用的方法是利用GAN来进行离线的手写文字图像生成。</p><h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2><p>​ 两类生成方法： 基于笔画的在线生成方法（需要记录时序数据） 和 基于图像的离线生成方法。</p><ul><li><p>GANwriting ：该方法利用在少量信息中提取样式特征和预定义固定长度的文本内容来进行文本生成。</p></li><li><p>我们的方法：与GANwritting相似，我们的方法也是在少量的风格样例中去提取风格特征，但又与 GANwriting 不同，我们的方法具有生成任意长度的风格化文本的灵活性。 我们能够同时捕获全局和局部的书写风格。</p></li></ul><h2 id="主要设计思想">4、主要设计思想：</h2><p>两个主要改进的Motivation:</p><p>1、除了字/行级别的样式-内容纠缠之外，字符级别的样式和内容之间的纠缠有助于模仿特定字符的写作风格以及泛化到词汇外的内容。</p><p>2、为了生成准确的风格文本图像，需要模仿全局的（例如墨水宽度、倾斜度等）和局部的（例如字符风格、连字等）风格特征。</p><h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2><p><strong>问题公式化描述：</strong></p><ul><li><span class="math inline">\(i \in W\)</span>, <span class="math inline">\(W\)</span>包含M个作者，<span class="math inline">\(i\)</span>代表某一个作者</li><li>一个手写文字图像集合 P</li><li>输入的文本内容<span class="math inline">\(A = \{a_j\}_{j=1}^Q\)</span>，视为一个Word String的集合</li><li>每个WordString 包含长度不定的字符，字符来自于字符集C</li><li>字符集C包含 字母表、标点、数字</li><li><span class="math inline">\(\hat X_i^t\)</span>代表依据新的文本内容t，生成的作者<span class="math inline">\(i\)</span>的新图像</li></ul><h3 id="整体架构overall-pipeline">1) 整体架构（Overall Pipeline）</h3><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127103226508.png" alt="image-20220127103226508" /><figcaption aria-hidden="true">image-20220127103226508</figcaption></figure><ul><li><strong>概述</strong>：先利用CNN，将风格图像变换至高维特征空间，然后进入Transformer Encoder中，生成特征潜在向量Z，输入到Transformer Decoder中，与Query Words进行结合，解码输出F，再通过CNN Decoder，最终生成图像，然后进入到不同的判别器中，基于不同的角度，定义了4个损失函数，进行函数的优化。</li><li>Query Words进入到Transformer Decoder中前，需要进行词嵌入编码，对每个字符，定义了一个可学习的词嵌入向量，<span class="math inline">\(q_c \in R^{512}\)</span>，这样一种基于字符的表示，以及基于transformer的序列处理方式，帮助模型能够生成任意长度的手写单词，并且能够更好的泛化到词汇表外的数据。<ul><li><span class="math inline">\(G_\theta\)</span>生成器，用于合成手写文本图像</li><li><span class="math inline">\(D_\Psi\)</span>判别器，用于确保生成图像的 手写风格的真实性（即确保看上去是手写图像）</li><li><span class="math inline">\(R_\phi\)</span>识别器，用于保持文本内容正确</li><li><span class="math inline">\(S_\eta\)</span>风格分类器，用于确保迁移的手写风格正确性（即确保生成的风格与Style Example一致）</li></ul></li><li><strong>输入</strong>： <span class="math inline">\(a_j \in A\)</span> &amp;&amp; <span class="math inline">\(X_i^s\)</span>文本 + 风格图像示例</li><li><strong>输出</strong>： <span class="math inline">\(\hat X_i^t\)</span>新生成的风格化手写字符图像（文本内容为 t ）</li></ul><h3 id="encoder-network-tepsilon">2) Encoder Network <span class="math inline">\(T\epsilon\)</span></h3><ul><li><strong>概述</strong>：将风格手写示例图编码至风格特征空间向量</li><li><strong>输入</strong>：<span class="math inline">\(X_i^s\)</span>风格手写示例图</li><li><strong>输出</strong>：<span class="math inline">\(Z \in R^{N \times d}\)</span>风格特征空间向量</li><li><strong>网络结构</strong>：<ul><li><strong>Part1 : CNN Encoder</strong><ul><li>采用<strong>ResNet18</strong>: 为每个style image生成低分辨率的激活图<span class="math inline">\(h_{ij} \in R^{h \times w \times d}\)</span></li><li>将<span class="math inline">\(h_{ij}\)</span>在空间维度上展平，得到一个Sequence, <span class="math inline">\(n \times d\)</span>，其中<span class="math inline">\(n = h \times w\)</span>, 这个序列可以被视为风格图像某区域的描述子</li><li>同时，因为总共我们会提供P张风格示例图像，将所有风格示例图像中提取出的Sequence进行拼接，得到一个Tensor，<span class="math inline">\(H_i \in R^{N \times d}\)</span>其中，<span class="math inline">\(N = n \times P\)</span></li></ul></li><li><strong>Part2: Transformer-Based Encoder</strong><ul><li>L层，每一层都由multihead-self-attention 和 MLP模块组成。重复L遍，很标准的Transformer Encoder（论文中没有细讲有无Residual）</li><li>为保留提供的输入序列的未知信息，采用固定位置编码</li></ul></li></ul></li><li><strong>意义</strong>：对局部的和全局的手写风格图像特征建模，例如倾斜、歪斜、字符形状、连字、墨水宽度等</li></ul><h3 id="decoder-network-t_d">3) Decoder Network <span class="math inline">\(T_d\)</span></h3><ul><li><strong>概述</strong>：结合输入的字符序列，生成风格化的手写字符图像</li><li><strong>输入</strong>：<span class="math inline">\(Z \in R^{N \times d}\)</span>风格特征空间向量 + <span class="math inline">\(A\)</span>输入的字符序列</li><li><strong>输出</strong>：<span class="math inline">\(\hat X_i^t\)</span>新生成的风格化手写字符图像（文本内容为 t ）</li><li><strong>网络结构</strong>：<ul><li><strong>Part1: Query Words 处理</strong><ul><li>将Query Words 编码成 Query Embedding，其为可学习的位置编码参数，加入到之后Transformer-Based Decoder中。简单来讲，这一步的作用就是要让每一个Query Embedding都在提供的样式图像中，能够去查找感兴趣的区域，从而进一步推断所有查询字符的风格属性。</li><li>如下图所示，t对应的位置编码，需要去学习的就是在我们提供的样例风格图像中，寻找字符t相关的位置（感兴趣的区域）。</li><li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127112528447.png" title="fig:" alt="image-20220127112528447" /></li></ul></li><li><strong>Part2: Transformer Based Decoder:</strong><ul><li>注意1：和self-attention不同，其key &amp; value 来源于Encoder的输出， query向量来源于Decoder自己的层。</li><li>注意2：在所有的Transformer Based Decoder中，我们是并行的去处理每一个Query Embeddings的，每个Query Embedding就代表一个字符。</li><li>将上一步的Query Embeddings 经过连续的 Transformer Based Decoder以后，会累积一些风格信息，产生一个输出 <span class="math inline">\(F_{a_j} \in R^{m_j \times d}\)</span>. <span class="math inline">\(m_j\)</span>是某个单词的字符数</li><li>然后，我们会将<span class="math inline">\(N(0,1)\)</span>的噪音，加入到 <span class="math inline">\(F_{a_j}\)</span>中，来模拟自然情况下的外部干扰变化。</li><li><strong>举例而言</strong>：<ul><li>一个m个字符的单词</li><li>我们会将m个Embeddings Vectors连接起来，然后再通过一个FC层，得到一个<span class="math inline">\(m_j \times 8192\)</span>的矩阵，我们将其Reshape成 <span class="math inline">\(512 \times 4 \times 4m_j\)</span>,然后输入到CNN Decoder中。</li></ul></li></ul></li><li><strong>Part3: CNN Decoder</strong>:<ul><li>4 个残差模块 + tanh激活函数</li><li>获得最终的输出图像</li></ul></li></ul></li></ul><h3 id="training-loss-objectives">4) Training &amp; Loss Objectives</h3><ul><li><p><strong>概述</strong>：总共由4个Loss函数组成，每个Loss负责不同的部分，在网络结构概述中已经有所提及。</p><ul><li>1、<span class="math inline">\(D_\Psi\)</span>判别器，用于确保生成图像的 手写风格的真实性（即确保看上去是手写图像）</li></ul><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127115520938.png" alt="image-20220127115520938" /><figcaption aria-hidden="true">image-20220127115520938</figcaption></figure><ul><li><p>2、<span class="math inline">\(R_\phi\)</span>手写文本识别器，用于保持文本内容正确，使用CRNN搭建，使用CTC Loss函数，比较query words和识别输出的差别。识别器 Rφ 仅针对真实的、标记的、手写样本进行优化训练，但它用于鼓励 Gθ 生成具有准确内容的可读文本。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127115740239.png" alt="image-20220127115740239" /><figcaption aria-hidden="true">image-20220127115740239</figcaption></figure></li><li><p>3、<span class="math inline">\(S_\eta\)</span>风格分类器，用于确保迁移的手写风格正确性（即确保生成的风格与Style Example一致） ，其能够预测一个给定的手写图像的作者。使用Cross-Entropy来定义Loss函数：其只在真实的样例上利用如下损失函数进行训练：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127115916996.png" alt="image-20220127115916996" /><figcaption aria-hidden="true">image-20220127115916996</figcaption></figure></li><li><p>4、利用Cycle Loss 来确保编码的style features由循环一致性。这个损失函数能够让decoder最大程度上在解码阶段保留风格信息，使得我们能够从生成的图像中重建出最开始的风格特征序列。给定生成的图像 <span class="math inline">\(\hat X_i^t\)</span>，我们使用 编码器<span class="math inline">\(T\epsilon\)</span>来重建风格特征向量<span class="math inline">\(\hat Z\)</span>。如下所示的循环损失<span class="math inline">\(L_c\)</span>用于最小化<span class="math inline">\(Z\)</span>和 <span class="math inline">\(\hat Z\)</span>之间的L1差距</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127120112639.png" alt="image-20220127120112639" /><figcaption aria-hidden="true">image-20220127120112639</figcaption></figure><p>循环损失对解码器施加了正则化，以一致地模仿生成的样式文本图像中的写作风格。</p></li></ul></li><li><p>总的来说，我们以端到端的方式训练我们的 HWT 模型，损失目标如下 <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127120324671.png" alt="image-20220127120324671" /></p></li><li><p>同时，我们观察到平衡网络 Sη 和 Rφ 的梯度有助于使用我们的损失公式进行训练。根据 [3]，我们将 ∇Sη 和 ∇Rφ 归一化，使其具有与对抗性损失梯度相同的标准偏差 (σ)</p></li><li><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127120433481.png" alt="image-20220127120433481" /><figcaption aria-hidden="true">image-20220127120433481</figcaption></figure></li><li><p><span class="math inline">\(\alpha\)</span>是一个超参数，在训练我们的模型的时候被固定为1</p></li></ul>]]></content>
    
    
    <summary type="html">自动的手写文字生成对于一些书写障碍的人十分重要。通常使用的方法是利用GAN来进行离线的手写文字图像生成。</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://blog.slks.xyz/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="GAN" scheme="https://blog.slks.xyz/tags/GAN/"/>
    
    <category term="HandWritting Generation" scheme="https://blog.slks.xyz/tags/HandWritting-Generation/"/>
    
    <category term="Transformer" scheme="https://blog.slks.xyz/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Tmux常用命令</title>
    <link href="https://blog.slks.xyz/2022/01/26/%E5%B7%A5%E5%85%B7%E7%B1%BB%E7%AC%94%E8%AE%B0/tmux_command/"/>
    <id>https://blog.slks.xyz/2022/01/26/%E5%B7%A5%E5%85%B7%E7%B1%BB%E7%AC%94%E8%AE%B0/tmux_command/</id>
    <published>2022-01-26T07:47:19.000Z</published>
    <updated>2022-01-26T07:05:09.568Z</updated>
    
    <content type="html"><![CDATA[<h3 id="tmux常用命令">Tmux常用命令</h3><ul><li><p>创建并指定session名字 tmux new -s $session_name</p></li><li><p>删除session Ctrl+b :kill-session</p></li><li><p>临时退出session Ctrl+b d</p></li><li><p>列出session tmux ls</p></li><li><p>进入已存在的session tmux a -t $session_name</p></li><li><p>删除所有session Ctrl+b :kill-server</p></li><li><p>删除指定session tmux kill-session -t $session_name</p></li><li><p>开启光标</p></li></ul><p>​ ctrl + b 按下后松开 再立马按 [</p><ul><li>关闭光标</li></ul><p>​ ctrl + q 按下后松开 再立马按 [</p>]]></content>
    
    
    <summary type="html">描述了Tmux中的常用命令</summary>
    
    
    
    <category term="工具类使用笔记" scheme="https://blog.slks.xyz/categories/%E5%B7%A5%E5%85%B7%E7%B1%BB%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Tmux" scheme="https://blog.slks.xyz/tags/Tmux/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础系列笔记14——ResNet详解及为什么能解决深度网络退化问题</title>
    <link href="https://blog.slks.xyz/2022/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B014%E2%80%94ResNet%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E8%A7%A3%E5%86%B3%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E9%80%80%E5%8C%96%E9%97%AE%E9%A2%98/"/>
    <id>https://blog.slks.xyz/2022/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B014%E2%80%94ResNet%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E8%A7%A3%E5%86%B3%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E9%80%80%E5%8C%96%E9%97%AE%E9%A2%98/</id>
    <published>2022-01-23T12:26:19.000Z</published>
    <updated>2022-01-28T12:21:06.080Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一深度网络退化背景">一、深度网络退化背景</h2><p>​ 对于卷积神经网络，深度是一个很重要的因素。深度卷积网络自然的整合了低中高不同层次的特征，特征的层次可以靠加深网络的层次来丰富。因此在构建卷积网络时，网络的深度越高，可抽取的特征层次就越丰富越抽象。所以一般我们会倾向于使用更深层次的网络结构，以便取得更高层次的特征。但是更深层的网络结构有的时候并不会带来更好的结果，层数一旦过多以后，就会导致表现明显下降，这就是深度网络的退化问题。</p><p>​ 深度网络的退化问题到底是由于什么导致的呢？过拟合？还是梯度消失？梯度爆炸？</p><p>​ 其实都不是。如下图论文中显示的所示，显然其在训练集上的误差也很大，就不可能是过拟合问题。而梯度消失或梯度爆炸，通过加入BN层，就能够通过规整数据分布来解决这个问题，所以应当也不是梯度消失或爆炸的问题。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS82Y2M3OThjNC0wMTNmLTRkNjQtYmY5Yi0yZDg0YWExOTQzNzYucG5n.png" alt="aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS82Y2M3OThjNC0wMTNmLTRkNjQtYmY5Yi0yZDg0YWExOTQzNzYucG5n" /><figcaption aria-hidden="true">aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS82Y2M3OThjNC0wMTNmLTRkNjQtYmY5Yi0yZDg0YWExOTQzNzYucG5n</figcaption></figure><p>​ 那么根本原因是什么呢？</p><p>​ 在<strong>MobileNet V2</strong>的论文中提到，由于非线性激活函数Relu的存在，每次输入到输出的过程都几乎是不可逆的，这也造成了许多<strong>不可逆的信息损失</strong>。我们试想一下，一个特征的一些有用的信息损失了，那他的表现还能做到持平吗？显然不可能做到持平。随着网络层数的加深，造成了许多不可逆的信息损失，最终导致了深度网络的退化问题。</p><h2 id="二resnet提出初衷与详解">二、ResNet提出初衷与详解</h2><p>​ 我们选择加深网络的层数，是希望深层的网络的表现能比浅层好，或者是<strong>希望它的表现至少和浅层网络持平（相当于直接复制浅层网络的特征）</strong>，可实际的结果却让我们大吃一惊（深度网络退化），这是为什么呢？</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS81OGU1NDU3Yy1lZWEwLTRmMjctOGVlMS04ZDJhMTk4YmJkOTcucG5n.png" alt="aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS81OGU1NDU3Yy1lZWEwLTRmMjctOGVlMS04ZDJhMTk4YmJkOTcucG5n" /><figcaption aria-hidden="true">aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS81OGU1NDU3Yy1lZWEwLTRmMjctOGVlMS04ZDJhMTk4YmJkOTcucG5n</figcaption></figure><p>​ 如图所示，这是一个直观的例子，我们把右边的网络理解为左边浅层网络加深了三层（框起来的部分），假如我们希望右边的深层网络与左边的浅层网络持平，即是希望框起来的三层跟没加一样，也就是加的三层的输入等于输出。我们假设这三层的输入为x，输出为H(x)，那么深层网络与浅层网络表现持平的直观理解即是H(x)=x，这种让输出等于输入的方式，就是ResNet论文中提到的<strong>恒等映射（identity mapping)</strong>。</p><p>​ 所以<strong>ResNet的初衷，就是让网络拥有这种恒等映射的能力，能够在加深网络的时候，至少能保证深层网络的表现至少和浅层网络持平</strong>。</p><p>​ 所以，现在我们知道，<strong>如果想要让深度神经网络不退化，根本的原因就是如何去做到恒等映射</strong>。然而现有的神经网络很难拟合潜在的恒等映射函数，H(x) = x，因为神经网络内部总会做一些参数的调整。但我们如果把恒等映射作为网络的一部分，将网络设计为H(x) = F(x) + x的形式，即如残差结构中所示那样，网络的输出是由x 和 F(x) 相加得到的，那么就可以把问题转化为 让中间多出来的那三层，去学习一个残差函数F(x) = H(x) - x。只要学习到的残差函数，能够使得F(x) = 0，那么就构成了一个恒等映射。最终网络的结果就不会比失去这几层要差。<strong>并且，让网络去拟合一个残差比拟合一个恒等映射容易得多（原因见后）</strong></p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS9mNGI5ZjRjYi04NWE4LTRmNjQtYjc0Ny1lMzU0ZDdjNmM2YmUucG5n.png" alt="aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS9mNGI5ZjRjYi04NWE4LTRmNjQtYjc0Ny1lMzU0ZDdjNmM2YmUucG5n" /><figcaption aria-hidden="true">aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS9mNGI5ZjRjYi04NWE4LTRmNjQtYjc0Ny1lMzU0ZDdjNmM2YmUucG5n</figcaption></figure><p>​ ResNet中，<strong>shortcut connection</strong>，<strong>通过跳接在激活函数前，将上一层（或几层）的输出与本层输出相加，将求和的结果输入到激活函数作为本层的输出</strong></p><p>​</p><h2 id="三为什么resnet能解决深度网络退化问题">三、为什么ResNet能解决深度网络退化问题？</h2><p>1、加了残差结构后就是给了输入x多一个选择，在神经网络学习到这层的参数是冗余的时候它可以选择直接走这条“跳接”曲线，跳过这个冗余层，而不需要再去拟合参数使得输出H(x)等于x。</p><p>2、因为学习残差的计算量比学习恒等映射小。假设普通网络为A，残差网络为B，输入为2，输出为2（输入和输出一样是为了模拟冗余层需要恒等映射的情况），那么普通网络就是A (2) = 2，而残差网络就是B ( 2 ) = F ( 2 ) + 2 = 2，显然残差网络中的F ( 2 ) = 0 。我们知道网络中权重一般会初始化成0附近的数，那么我们就很容易理解，为什么让F(2)（拟合0会比A (2) = 2 容易了</p><p>3、我们知道ReLU能够将负数激活为0，而正数输入等于输出。这相当于过滤了负数的线性变化，让F(x)=0变得更加容易。</p><p>4、我们知道残差网络可以表示成H ( x ) = F ( x ) + x ，这就说明了在求输出H ( x ) 对输入x 的倒数（梯度），也就是在反向传播的时候，H ′ ( x ) = F ′ ( x ) + 1，残差结构的这个常数1也能保证在求梯度的时候梯度不会消失。</p><h2 id="四一些细节问题">四、一些细节问题：</h2><p>​ 在ResNet中，残差连接的相加，指的是逐元素相加，在ReSNet的网络示意图中，有的Skip-Connection是实线，有的是虚线，<strong>虚线的代表这些模块前后的维度不一致，因为去掉残差结构的Plain网络还是和VGG一样，也就是每隔n层进行下采样但深度翻倍（VGG通过池化层下采样，ResNet通过卷积）</strong>：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS81YjZhMTNiZi00ZTU1LTRlMzYtOWY1NC05YzAzYzhkMmVkY2EucG5n.png" alt="aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS81YjZhMTNiZi00ZTU1LTRlMzYtOWY1NC05YzAzYzhkMmVkY2EucG5n" /><figcaption aria-hidden="true">aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS81YjZhMTNiZi00ZTU1LTRlMzYtOWY1NC05YzAzYzhkMmVkY2EucG5n</figcaption></figure><ul><li><p>空间上不一致时，需要给输入的X做一个线性的映射：调整一下H*W维度</p></li><li><p>深度上不一致时，有两种解决方法，一种是在跳接过程中加一个1×1的卷积层进行升维，另一种则是直接补零（先做下采样）。</p></li><li><p>针对比较深的神经网络，作者也考虑到计算量，会先用1×1的卷积将输入的256维降到64维，然后通过1×1恢复。这样做的目的是减少参数量和计算量。</p></li></ul><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS85M2RiMDgzYi05NTk3LTRjZmUtODJmZC04NzRlNTI2NjViNmIucG5n.png" alt="aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS85M2RiMDgzYi05NTk3LTRjZmUtODJmZC04NzRlNTI2NjViNmIucG5n" /><figcaption aria-hidden="true">aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS85M2RiMDgzYi05NTk3LTRjZmUtODJmZC04NzRlNTI2NjViNmIucG5n</figcaption></figure><p>引用：https://blog.csdn.net/cristiano20/article/details/104309948</p>]]></content>
    
    
    <summary type="html">对于卷积神经网络，深度是一个很重要的因素。深度卷积网络自然的整合了低中高不同层次的特征，特征的层次可以靠加深网络的层次来丰富。因此在构建卷积网络时，网络的深度越高，可抽取的特征层次就越丰富越抽象。所以一般我们会倾向于使用更深层次的网络结构，以便取得更高层次的特征。但是更深层的网络结构有的时候并不会带来更好的结果，层数一旦过多以后，就会导致表现明显下降，这就是深度网络的退化问题。</summary>
    
    
    
    <category term="机器学习基础系列笔记" scheme="https://blog.slks.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="ResNet" scheme="https://blog.slks.xyz/tags/ResNet/"/>
    
    <category term="Degradation" scheme="https://blog.slks.xyz/tags/Degradation/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础系列笔记13——DropOut详解及为什么能够防止过拟合</title>
    <link href="https://blog.slks.xyz/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B013%E2%80%94DropOut%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E5%A4%9F%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <id>https://blog.slks.xyz/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B013%E2%80%94DropOut%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E5%A4%9F%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88/</id>
    <published>2022-01-21T15:25:19.000Z</published>
    <updated>2022-01-28T12:21:01.184Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一dropout是什么">一、DropOut是什么？</h2><p>​ 过拟合是Deep Neural Networks(DNN)网络存在的问题之一。过拟合的特点是模型对训练数据的拟合非常好，但对测试数据的拟合却非常差，具体表现为loss和在训练集上的错误率非常低，而在验证集或测试集上却都要高很多。针对解决过拟合问题设计出来的方法很多，dropout就是其中一种最简单，也是最有效的方法。</p><p>​ 在训练DNN网络的过程中，对于每一个神经元，以p的概率被随机的drop out，也就是将其值置零。这样，在该轮前传和反传的过程中，该神经元将失去作用，相当于不存在，如下图所示。DropOut整体来说，是在训练过程中以一定的概率的使神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/dropout.jpeg" alt="dropout" /><figcaption aria-hidden="true">dropout</figcaption></figure><h2 id="二dropout阶段在训练阶段和测试阶段的区别">二、DropOut阶段在训练阶段和测试阶段的区别：</h2><h3 id="训练阶段">1、训练阶段：</h3><p>​ 在前向传播时，假设有这一层n个神经元，则我们可以假设每个神经元的概率都是0~1(可以通过python得到随机的值)，然后小于p的就失活，即不参与训练。</p><p>​ 在反向传播时，也只对参与训练的神经元进行参数更新。</p><p>​ 每次训练的时候，又是n个神经元，重新进行dropout</p><p>​ <strong>Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。</strong></p><h3 id="测试阶段">2、测试阶段：</h3><p>​ 在测试时，应该用整个训练好的模型，不需要进行dropout。</p><p>​ 参与学习的节点和那些被隐藏的节点需要以一定的概率p加权求和，综合计算得到网络的输出。</p><p>​ 即预测的时候，每一个单元的参数要预乘以p。为什么要预乘以p呢？</p><p>​ 因为我们训练的时候会随机的丢弃一些神经元，但是预测的时候就没办法随机丢弃了。<strong>如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，这是实际系统不能接受的</strong>，用户可能认为模型预测不准。那么<strong>一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的</strong>。</p><p>​ <strong>比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是<span class="math inline">\(p \times x+(1-p) \times 0 = p \times x\)</span>。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望。</strong></p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220121204115930.png" alt="image-20220121204115930" /><figcaption aria-hidden="true">image-20220121204115930</figcaption></figure><h2 id="三dropout如何防止过拟合">三、DropOut如何防止过拟合？</h2><p><strong>（1）数据层面</strong></p><p>​ 对于每一个dropout后的网络，进行训练时，相当于做了Data Augmentation。比如，对于某一层，dropout一些单元后，形成的结果是(1.5，0，2.5，0，1，2，0)，其中0是被drop的单元，那么总能找到一个样本，使得结果也是如此。这样每一次dropout其实都相当于增加了样本。</p><p><strong>（2）模型层面</strong></p><ul><li><p><strong>在较大程度上减小了网络的大小：</strong>在这个“残缺”的网络中，让神经网络学习数据中的局部特征（即部分分布式特征），但这些特征也足以进行输出正确的结果。</p></li><li><p><strong>取平均的作用：</strong> 如果正常的模型（没有dropout），我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。（例如 3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果）。<strong>这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。每次训练随机dropout掉不同的隐藏神经元，网络结构已经不同，这就类似在训练不同的网络，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</strong></p></li><li><p><strong>减少神经元之间共适应关系：</strong> 因为dropout导致两个神经元不一定每次都在一个网络中出现，这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况， 迫使网络去学习更加鲁棒的特征。换句话说，假如神经网络是在做出某种预测，<strong>它不应该对一些特定的线索片段太过敏感</strong>，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。</p></li></ul><p>引用：</p><p>https://zhuanlan.zhihu.com/p/175142160</p><p>https://www.zhihu.com/question/402485242/answer/1553947864</p>]]></content>
    
    
    <summary type="html">过拟合是Deep Neural Networks(DNN)网络存在的问题之一。过拟合的特点是模型对训练数据的拟合非常好，但对测试数据的拟合却非常差，具体表现为loss和在训练集上的错误率非常低，而在验证集或测试集上却都要高很多。针对解决过拟合问题设计出来的方法很多，dropout就是其中一种最简单，也是最有效的方法。在训练DNN网络的过程中，对于每一个神经元，以p的概率被随机的drop out，也就是将其值置零。这样，在该轮前传和反传的过程中，该神经元将失去作用，相当于不存在，如下图所示。DropOut整体来说，是在训练过程中以一定的概率的使神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。</summary>
    
    
    
    <category term="机器学习基础系列笔记" scheme="https://blog.slks.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="OverFitting" scheme="https://blog.slks.xyz/tags/OverFitting/"/>
    
    <category term="DropOut" scheme="https://blog.slks.xyz/tags/DropOut/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础系列笔记12——BatchNorm详解及为什么能防止过拟合</title>
    <link href="https://blog.slks.xyz/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B012%E2%80%94BatchNorm%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <id>https://blog.slks.xyz/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B012%E2%80%94BatchNorm%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88/</id>
    <published>2022-01-21T14:01:19.000Z</published>
    <updated>2022-01-28T12:20:56.321Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一详解batchnorm原理">一、详解BatchNorm原理：</h2><p>​ BatchNorm是一种能够加速深度神经网络收敛，避免过拟合的方法，那么为什么呢？首先我们需要探讨一下这个问题，为什么深度神经网络<strong>随着网络深度加深，训练起来越困难，收敛越来越慢？</strong></p><p>在回答这个问题前需要首先了解两个概念：</p><p>​ <strong>1、独立同分布（IID）</strong>：即假设训练数据和测试数据是满足相同分布的。它是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障</p><p>​ <strong>2、Covariate shift</strong>：<strong>如果ML系统实例集合&lt;X,Y&gt;中的输入值X的分布老是变，这不符合IID假设</strong>，网络模型很难<strong>稳定的学规律</strong>。</p><p>​ 所以，之所以深度神经网络随着网络深度加深，训练越来越困难是因为，对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是<strong>在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift（ICS）”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层</strong></p><p>​ BatchNorm的基本思想就是能不能<strong>让每个隐层节点的激活输入分布固定下来呢</strong>？这样就避免了“Internal Covariate Shift”问题了。</p><p>所以BN实质上就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的一种方法。</p><p>​ BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的<strong>激活输入值随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近</strong>（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这<strong>导致反向传播时低层神经网络的梯度消失</strong>，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>，<strong>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是<strong>这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p><p>​ <strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</strong>因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。BN说到底就是这么个机制，方法很简单，道理很深刻。</p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cDovL3FsLm1hZ2ljLXNldmVuLnRvcC91cGxvYWQvMjAyMC8zL2ltYWdlLTVkMjU4MDQxNGI0YTRkNjhiODEzMDMxMDZlMzY5YzNiLnBuZw.png" /></p><h2 id="二batchnorm在训练阶段和测试阶段的做法与意义">二、BatchNorm在训练阶段和测试阶段的做法与意义：</h2><h3 id="训练阶段">1、训练阶段：</h3><p>​ 首先计算均值和方差（每次训练给一个批量，计算批量的均值方差），然后归一化，然后缩放和平移，完事！</p><h3 id="测试阶段">2、测试阶段：</h3><p>​ 每次只输入一张图片，这怎么计算批量的均值和方差，于是，就有了代码中下面两行，在训练的时候实现计算好mean、 var，测试的时候直接拿来用就可以了，不用计算均值和方差。</p><p>​ 用训练集来估计总体均值 μ 和总体标准差 σ 。</p><p>​ 较为简单的做法就是把每个mini-batch的均值和方差都保存下来，然后训练完了求均值的均值，方差的均值即可。</p><p>​ 在测试阶段应用BatchNorm的含义，应该就是要让测试集的测试精度与整个训练网络保持一致。</p><h2 id="三batchnorm的两个参数gamma和beta有什么作用">三、BatchNorm的两个参数<span class="math inline">\({\gamma}\)</span>和<span class="math inline">\(\beta\)</span>有什么作用？</h2><figure><img src="https://www.zhihu.com/equation?tex=y%3D\frac%7Bx-\mathrm%7BE%7D%5Bx%5D%7D%7B\sqrt%7B\operatorname%7BVar%7D%5Bx%5D%2B\epsilon%7D%7D+*+\gamma%2B\beta+" alt="[公式]" /><figcaption aria-hidden="true">[公式]</figcaption></figure><h3 id="如果只做归一化为什么是学不到任何东西的">1、如果只做归一化，为什么是学不到任何东西的？</h3><p>​ 如果在每一层之后都归一化成0-1的高斯分布（减均值除方差）那么数据的分布一直都是高斯分布，数据分布都是固定的了，这样即使加更多层就没有意义了，<strong>深度网络就是想学习数据的分布发现规律性，BN就是不让学习的数据分布偏离太远</strong></p><h3 id="两个参数的作用">2、两个参数的作用</h3><p>​ 为了减小Internal Covariate Shift，对神经网络的每一层做归一化不就可以了，假设将每一层输出后的数据都归一化到0均值，1方差，满足正胎分布，但是，此时有一个问题，<strong>如果每一层的数据分布都是标准正太分布，导致其完全学习不到输入数据的特征，因为，费劲心思学习到的特征分布被归一化了，因此，直接对每一层做归一化显然是不合理的。</strong>但是如果稍作修改，加入可训练的参数做归一化，那就是BatchNorm 实现的了。</p><p>​ 接下来详细介绍一下这额外的两个参数，之前也说过如果直接做归一化不做其他处理，神经网络是学不到任何东西的，但是加入这两个参数后，事情就不一样了。先考虑特殊情况下，如果γ 和β 分别等于此batch的标准差和均值，那么<span class="math inline">\(y_i\)</span>就还原到归一化前的x了吗，也即是缩放平移到了归一化前的分布，相当于batch norm没有起作用，$ β$ 和γ 分别称之为 平移参数和缩放参数 。这样就<strong>保证了每一次数据经过归一化后还保留的有学习来的特征，同时又能完成归一化这个操作，加速训练</strong>。</p><p>引用：https://www.cnblogs.com/hoojjack/p/12350707.html</p>]]></content>
    
    
    <summary type="html">BatchNorm是一种能够加速深度神经网络收敛，避免过拟合的方法，那么为什么呢？首先我们需要探讨一下这个问题，为什么深度神经网络随着网络深度加深，训练起来越困难，收敛越来越慢？</summary>
    
    
    
    <category term="机器学习基础系列笔记" scheme="https://blog.slks.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="OverFitting" scheme="https://blog.slks.xyz/tags/OverFitting/"/>
    
    <category term="Batch Normalization" scheme="https://blog.slks.xyz/tags/Batch-Normalization/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础系列笔记11——Training Data、Validation Data、Testing Data含义及作用</title>
    <link href="https://blog.slks.xyz/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B011%E2%80%94Training%20Data%E3%80%81Validation%20Data%E3%80%81Testing%20Data%E5%90%AB%E4%B9%89%E5%8F%8A%E4%BD%9C%E7%94%A8/"/>
    <id>https://blog.slks.xyz/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B011%E2%80%94Training%20Data%E3%80%81Validation%20Data%E3%80%81Testing%20Data%E5%90%AB%E4%B9%89%E5%8F%8A%E4%BD%9C%E7%94%A8/</id>
    <published>2022-01-21T08:01:19.000Z</published>
    <updated>2022-01-28T12:20:49.962Z</updated>
    
    <content type="html"><![CDATA[<p>​<br />我们在进行一个机器学习的任务的时候，往往会将所有数据划分成三块——Training Data、Validation Data、Testing Data,它们各自在训练的过程中扮演何种角色呢？</p><h4 id="一trainvaltest的含义与作用">一、Train、Val、Test的含义与作用：</h4><p>顾名思义，三个数据集合它们的简单含义如下：</p><ul><li>训练集(train)：训练模型，用来拟合模型的数据集；</li><li>验证集(val)：评估模型，训练过程中提供相对于train的无偏估计的数据集，同时用来调整超参数和特征选择，实际参与训练</li><li>测试集(test)：最终模型训练好之后，用来提供相对于train+valid的无偏估计的数据集。</li></ul><p>​ 一般我们会将最开始划分的Training Set分割为Training Data和Validation Data两个集合，一般而言比例为9：1。我们使用划分后的Training Data进行训练，在每个Epoch结束后使用训练期间机器没有见到过的Validation进行验证，依据验证集得到的Loss值来进行模型好坏的衡量。</p><p>​ 话句话说，Validation Data　其实就是用来避免过拟合的，在训练过程中，我们通常用它来确定一些超参数（比如根据validation data上的accuracy来确定early stopping的epoch大小、根据validation data确定learning rate等等）。</p><p>​ 那为啥不直接在Testing data上做这些呢？因为如果在Testing data做这些，那么随着训练的进行，我们的网络实际上就是在一点一点地overfitting我们的Testing data，导致最后得到的Testing accuracy没有任何参考意义。因此，Training data的作用是计算梯度更新权重，Validation data在每个Epoch结束后进行验证，Testing data则给出一个accuracy以判断网络的好坏。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/20210202115236662.png" alt="20210202115236662" /><figcaption aria-hidden="true">20210202115236662</figcaption></figure><p><strong>如上所示的训练划分容易带来一些显而易见的问题：</strong></p><ul><li>如果样本数量太少，验证集和测试集更少，无法在统计学上代表数据</li><li>划分数据前时，进行不同的随机打乱则得到的模型性能差别可能很大，可能训练集中的数据都偏向于某一类，而验证集的数据偏向于另一类</li></ul><h4 id="二n-fold-cross-validation">二、N-Fold Cross Validation</h4><p>此时，就可以使用常见的叫做N-Fold Cross Validation，（K折交叉验证）：</p><p>​ 如下图所示,我们将Training Set分为N个集合(示例中为3个),其中N-1个集合用于训练,1个集合用于验证,然后每轮Epoch中,都执行N遍,每一遍都拿不同的集合用于训练与验证,然后计算一遍Loss值,最终选取平均Loss最小的那一组参数进行模型的更新.</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114111918585.png" alt="image-20220114111918585" /><figcaption aria-hidden="true">image-20220114111918585</figcaption></figure>]]></content>
    
    
    <summary type="html">我们在进行一个机器学习的任务的时候，往往会将所有数据划分成三块——Training Data、Validation Data、Testing Data,它们各自在训练的过程中都扮演着某种角色</summary>
    
    
    
    <category term="机器学习基础系列笔记" scheme="https://blog.slks.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Training Set" scheme="https://blog.slks.xyz/tags/Training-Set/"/>
    
    <category term="Validation Set" scheme="https://blog.slks.xyz/tags/Validation-Set/"/>
    
    <category term="Testing Set" scheme="https://blog.slks.xyz/tags/Testing-Set/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础系列笔记10——L1、L2正则化以及为什么正则化能够防止过拟合</title>
    <link href="https://blog.slks.xyz/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B010%E2%80%94L1%20L2%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AD%A3%E5%88%99%E5%8C%96%E8%83%BD%E5%A4%9F%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <id>https://blog.slks.xyz/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B010%E2%80%94L1%20L2%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AD%A3%E5%88%99%E5%8C%96%E8%83%BD%E5%A4%9F%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88/</id>
    <published>2022-01-21T07:01:19.000Z</published>
    <updated>2022-01-28T12:20:42.966Z</updated>
    
    <content type="html"><![CDATA[<p>​ 在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）。其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在Training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集，对训练集外的数据却不work。</p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/OhX9eFQ.jpg" /></p><p>​ 在ML2021课程系列笔记2中，提及了一些防止过拟合的内容，本篇用于详细解释其中正则化的部分：</p><h3 id="一什么是l1l2正则化">一、什么是L1、L2正则化？</h3><h4 id="l1-正则化">L1 正则化：</h4><p>​ 简单而言，L1正则化就是在Loss函数后面增加一个正则化项，L1正则化的公式如下,</p><p>​ <span class="math inline">\(C_0\)</span>为原来的损失函数，即所有权重w的绝对值的和. n是训练集的样本大小，λ 是正则项系数，C为加了正则化后的损失函数</p><p>​ <span class="math inline">\(C = C_0 + \frac{\lambda}{n} \sum_{w}|w|\)</span></p><h4 id="l2-正则化">L2 正则化：</h4><p>​ 简单而言，L2正则化也是在Loss函数后面增加一个正则化项，L2正则化的公式如下,与上含义类似。</p><p>​ <span class="math inline">\(C = C_0 + \frac{\lambda}{2n} \sum_w w^2\)</span></p><h3 id="二l1l2正则化如何避免overfitting">二、L1、L2正则化如何避免OverFitting？</h3><h4 id="l1l2正则化能降低权重值w">1、L1、L2正则化能降低权重值w</h4><p>​ 我们以L2正则化项为例，进行解释，首先我们让C对偏置项b和对权重系数w进行求导，得到如下：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/mebEC90.jpg" alt="mebEC90" /><figcaption aria-hidden="true">mebEC90</figcaption></figure><p>​ 我们发现，C对b求导与正则化项无关，C对w求导得到得结果与正则化项有关。</p><p>​ 最终所反映到梯度下降优化参数上，就是如下图所示得情况：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/qM83geg.jpg" alt="mebEC90" /><figcaption aria-hidden="true">mebEC90</figcaption></figure><p>​ <strong>如果没有正则化项，那么更新得w前得系数应当为1，而现在由于因为η、λ、n都是正的，所以 1−ηλ/n小于1，它的效果是减小w，这也就是权重衰减（weight decay）的由来</strong>。</p><p>​ <strong>到此为止，我们发现L2正则化项，其能够使得减小w。（其实这一点比较直观的也能看出来，因为Loss函数中加入了一项<span class="math inline">\(w^2\)</span>的求和，也就是说如果权重值w过大，Loss函数值会上升，这就意味着这一个正则化项惩罚了权值矩阵使其不能取太大值。）</strong></p><p>​ 那么，关键问题是，<strong>为什么权重矩阵w小，能够防止过拟合呢？</strong></p><h4 id="较高的权重w往往意味着过拟合的函数">2、较高的权重w往往意味着过拟合的函数：</h4><p>​ 我们会发现：<strong>过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。</strong></p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/RsR5cOK.png" alt="RsR5cOK" /><figcaption aria-hidden="true">RsR5cOK</figcaption></figure><p>​</p><p>​ 更进一步的解释：当权重系数很小的时候，容易出现像左图一样，高偏差拟合能力很差的情况，随着权重系数逐渐增大，就会像右侧的图进行发展。如果权重系数很大，往往意味着在某些很小的区间中，函数值的变化会非常的剧烈，导致一些高方差的结果，也就是函数对于训练数据过度拟合了。</p><p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220121122935964.png" alt="image-20220121122935964" /></p><p>​</p>]]></content>
    
    
    <summary type="html">在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）。其直观的表现为，随着训练过程的进行，模型复杂度增加，在Training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集，对训练集外的数据却不work。在ML2021课程系列笔记2中，提及了一些防止过拟合的内容，本篇用于详细解释其中正则化的部分：</summary>
    
    
    
    <category term="机器学习基础系列笔记" scheme="https://blog.slks.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="OverFitting" scheme="https://blog.slks.xyz/tags/OverFitting/"/>
    
    <category term="Regularization" scheme="https://blog.slks.xyz/tags/Regularization/"/>
    
  </entry>
  
  <entry>
    <title>《DG-Font Deformable Generative Networks for Unsupervised Font Generation》</title>
    <link href="https://blog.slks.xyz/2022/01/20/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B04%E2%80%94%E2%80%94%E3%80%8ADG-Font%20Deformable%20Generative%20Networks%20for%20Unsupervised%20Font%20Generation%E3%80%8B/"/>
    <id>https://blog.slks.xyz/2022/01/20/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B04%E2%80%94%E2%80%94%E3%80%8ADG-Font%20Deformable%20Generative%20Networks%20for%20Unsupervised%20Font%20Generation%E3%80%8B/</id>
    <published>2022-01-20T10:12:19.000Z</published>
    <updated>2022-01-28T12:14:12.529Z</updated>
    
    <content type="html"><![CDATA[<h4 id="论文名称dg-font-deformable-generative-networks-for-unsupervised-font-generationcvpr2021">论文名称：《DG-Font: Deformable Generative Networks for Unsupervised Font Generation》CVPR2021</h4><h4 id="论文地址-httpsopenaccess.thecvf.comcontentcvpr2021htmlxie_dg-font_deformable_generative_networks_for_unsupervised_font_generation_cvpr_2021_paper.html">论文地址： https://openaccess.thecvf.com/content/CVPR2021/html/Xie_DG-Font_Deformable_Generative_Networks_for_Unsupervised_Font_Generation_CVPR_2021_paper.html</h4><h2 id="关键词">1、关键词：</h2><p>​ Font-Generation、Deformable Convolution Skip Connection、Unsupervised Learning</p><h2 id="领域背景">2、领域背景：</h2><p>​ 字体生成是一个具有挑战的任务，现存的大部分方法都是通过<strong>有监督学习</strong>的方法进行字体的生成，他们需要大量的<strong>paired data</strong>（例如对应风格的字符图像），然而大量的这些数据需要花费非常昂贵的代价去进行收集。</p><p>​ 字体生成目标是自动的能够生成某种特定字体的字符，并且创造一个字体字符集。</p><p>​ 在传统的字符集的创造方式中，严重依赖于专家设计者，独立的去绘制每一个字体的图像，这对于一些基于语标的语言（比如中文、日文、韩文）很不友好。专家们需要有大量的工作量去进行设计，字体生成领域就是为了解决这样一个问题，使用神经网络去学习从一种风格到另一种风格的映射关系，从而生成特定语言的某种风格的一个字符集。</p><p>​ 与字体生成最相关的<strong>图像生成领域 （Image-To-Image Translation）</strong>，在通常的image-to-image的转换模型中通常将<strong>style</strong>定义为<strong>纹理和颜色</strong>，而字体的<strong>style</strong>往往是指<strong>字体的格式几何形状、笔画粗细、笔尖和连笔书写的模式等内容。（geometric transformation, stroke thickness, tips, and joined-up writing pattern）</strong>，故而没法直接应用到字体的生成上。同时在图像中往往采用的<strong>AdaIN-based</strong>方法（这种方法是在统计学上对齐特征来迁移图像的纹理和颜色特征）对于字体这种变换局部的特征模式的任务是不合适的。</p><p>​ 同时，对于image-to-image的生成任务而言，一系列的<strong>无监督</strong>的方法，都是使用对抗<strong>训练结合Consistent Contrains</strong>来进行的。如果使用image-to-image的方法直接应用到字体生成任务中的话，即使Consistent Constraints会帮助我们<strong>保留一个字符图片内容的结构</strong>，但是他们仍然会导致<strong>诸如模糊、丢失笔画</strong>等问题。</p><h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2><h4 id="image-to-image-translation">1) Image-To-Image Translation</h4><p>​ <strong>image-to-image迁移的任务，就是学习一个从source domain到target domain的映射关系。其可以用于艺术风格迁移、语义分割、图像动画等等。</strong></p><ul><li>Pix2Pix是基于Conditional GAN的第一个做Image-to-Image的迁移任务。</li><li>Cycle-GAN通过Cycle Consistency 做到了无监督学习。</li></ul><p>​ <strong>这类工作无法直接应用至Font-Generation中，原因在Part2的领域背景最后已经做了简略的描述</strong></p><h4 id="font-generation">2) Font-Generation</h4><p>​ <strong>字体生成目标是自动的能够生成某种特定字体的字符，并且创造一个字体字符集。</strong></p><p>​ <strong>一般而言，从前的方法有两大条路径：</strong></p><ul><li><strong>基于paired data进行训练</strong><ul><li>EMD和SAVAE设计的神经网络，分割开了字体的内容和风格（content &amp; style）的表示，可以生成新的风格的字符内容。</li><li>zi2zi和rewrite这两篇论文，通过上千对匹配的字符，基于GAN进行了有监督学习。其之后，很多文章基于zi2zi进行了生成质量的改进。</li></ul></li><li><strong>基于辅助标识（例如笔画、部首等内容）</strong><ul><li><strong>这类方法往往都依赖于先验知识，并且只能应用到特定的书写系统中去。 并且这些方法仍旧需要数千匹配的数据以及辅助注释</strong></li><li>《Scfont: Structure-guided chinese font generation via deep stacked networks.》给每个笔画打上标签，来通过书写轨迹的合成，生成字的图像</li><li>DM-FONT 提出解纠缠策略来解纠缠复杂的字形结构，这有助于在富文本设计中捕获局部细节</li><li>CalliGAN 进一步将字符分解为组件，并提供包括笔画顺序在内的低级结构信息来指导生成过程。</li><li>《RD-GAN: few/zero-shot chinese character styletransfer via radical decomposition and rendering》 使用对字符偏旁部首的分解，来达到字体的生成</li><li>其他一些的方法也通过提取字符的骨架和笔画的算法来进行生成</li></ul></li></ul><p>​ <strong>相比之下，该篇论文提出的DG-FONT不需要其他的辅助标识，并且是无监督的形式进行的</strong></p><h4 id="deformable-convolution">3) Deformable Convolution</h4><p>​ <strong>介绍链接：https://blog.slks.xyz/2022/01/07/basic5/</strong></p><p>​ <strong>可变形卷积Deformable Convolution</strong>，<strong>其加强了CNN的变换建模能力</strong>，它通过额外的偏移量增加了模块中的空间采样位置。 可变形卷积已被应用于解决几个高级视觉任务，例如对象检测、视频对象检测采样、语义分割和人体姿态估计。</p><p>​ 最近，一些方法尝试在图像生成任务中应用可变形卷积。 TDAN[48] 通过使用可变形卷积对齐两个连续帧并输出高分辨率帧来解决视频超分辨率任务。</p><p>​ <strong>在我们提出的 DG-Font 中，可变形卷积的偏移量是通过 learned latent style code来进行估计的。</strong>（具体内容见后细节）</p><h2 id="主要设计思想">4、主要设计思想：</h2><p>​ 作者提出了可变形生成网络（Deformable Generative Networks）来做<strong>非监督的字体生成</strong>。其利用提供的目标字体图像（<strong>style image input</strong>）来将一种字体的字符变形和转换为另一种字体。</p><p>​ DG-FONT 分离了字体的style和content，然后再将两个domain的表示进行融合，生成新的字体的字符。</p><p>​ 其核心模块为一个叫做<strong>FDSC（feature deformation skip connection）的东西</strong>，可以用来预测一个位移映射map然后使用位移映射map去对low-level的特征图做变形卷积。然后FDSC的输出被送入一个混合器，然后生成最终的结果。</p><p>​ FDSC模块将会<strong>对内容图像的低层级特征进行变换，其能够保留文字本身的模式</strong>，比如笔画和偏旁部首信息。因为对于内容相同的两种不同风格的字体，<strong>它们的每一笔画通常都是对应的</strong>（如下图所示）。利用字体的空间关系，利用FDSC进行空间变形，<strong>有效地保证了生成的字符具有完整的结构</strong>。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116172624229.png" alt="image-20220116172624229" /><figcaption aria-hidden="true">image-20220116172624229</figcaption></figure><p>​</p><p>​ 同时，为了区分不同的风格，模型还使用了一个多任务标识符判别器Multi-Task Discriminator，以保证每个风格都可以被独立判定。</p><h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116160758876.png" alt="image-20220116160758876" /><figcaption aria-hidden="true">image-20220116160758876</figcaption></figure><h3 id="整体架构overall-pipeline">1) 整体架构（Overall Pipeline）</h3><ul><li><p><strong>概述</strong>：如上图所示，整个网络包含一个 Style Encoder、一个Content Encoder、一个Mixer、和两个FDSC模块。</p><ul><li><p><strong>Style Encoder</strong> 从输入图像中学习Style Representation。具体而言，其将一个Style Image作为输入，将其映射至一个Style Latent Vector <span class="math inline">\(Z_S\)</span> 。</p></li><li><p><strong>Content Encoder</strong> 提取Content Images的结构特征，将其 映射到一个空间特征图 <span class="math inline">\(Z_C\)</span></p></li><li><p><strong>Deformable Convolution</strong>能够使得<em>Content Encoder</em>去识别到相同内容的字中Style-Invariant的特征</p></li><li><p><strong>Mixer</strong> 通过混合<span class="math inline">\(Z_C\)</span> 和<span class="math inline">\(Z_S\)</span> 来生成输出字符。其使用<strong>AdaIN</strong>方法将style特征注入Mixer中。</p></li><li><p><strong>FDSC</strong> 模块能够将变形的<em>Low-Level</em>特征从<em>Content Encoder</em>中传输到Mixer中</p></li><li><p><strong>Multi-Task Discriminator</strong> ：当字符图像从生成网络生成后，该判别器用来对每个单独的 Style 同时执行判断任务。对于每一个style来说， Discriminator的输出是一个二元分类器， 判断其是真实图片还是生成图片。同时，因为在训练集中，有多种不同的字体风格，所以判别器的输出是一个数组，它的长度是所有风格的数量，数组里的每个元素是一个二元向量【例如，假设总共有5个风格，最终判别器输出的应当是如下的一个向量：</p><p>​ [ [ 1 0 ] , [ 0 0 ] , [ 0 0 ] , [ 0 0 ] , [ 0 0 ] ]</p><p>】</p></li></ul></li><li><p><strong>输入</strong>：<strong>Style Image</strong>【风格A，汉字1】、<strong>Content Image</strong>【风格B，汉字2】</p></li><li><p><strong>输出</strong>：<strong>Output Image</strong>【风格A，汉字2】</p></li></ul><h3 id="style-encoder">2) Style Encoder</h3><ul><li><strong>概述</strong>：从输入图像中学习 Style Representation。具体而言，其将一个Style Image作为输入，将其映射至一个Style Latent Vector <span class="math inline">\(Z_S\)</span> 。</li><li><strong>输入</strong>：Style Image $ I_s R^{H*W}$</li><li><strong>输出</strong>：Style Latent Vector <span class="math inline">\(Z_s \in R\)</span></li><li><strong>网络结构</strong>：如下所示<ul><li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220119151744473.png" title="fig:" alt="image-20220119151744473" /></li></ul></li><li><strong>公式表达</strong>：</li><li><strong>实现细节（官方代码）</strong><ul><li>其在源代码中，其为GuidingNet文件部分，可供选择的项有Vgg11，Vgg13，Vgg16，Vgg19等。这个StyleEncoder网络将输入的StyleImage进行特征的提取以后，得到特征向量，在Decoder中使用AdaIn融合之前，源代码中还经过了一个MLP模块。然后，利用MLP模块输出的内容，对AdaIN2d里面的weight和bias进行初始化，最终达到</li></ul></li></ul><h3 id="content-encoder">3) Content Encoder</h3><ul><li><p><strong>概述</strong>：提取Content Images的结构特征，将其 映射到一个空间特征图 <span class="math inline">\(Z_C\)</span></p></li><li><p><strong>输入</strong>：Content Image $ I_c R^{H*W}$</p></li><li><p><strong>输出</strong>：Content Latent Vector <span class="math inline">\(Z_c \in R\)</span></p></li><li><p><strong>网络结构细节</strong>：</p><ul><li><p>in_channel = 3,out_channel=64,kernel_size=7 x 7,stride = 1,padding = 3 的变形卷积层 + IN(64) 归一化 + Activation(ReLu)</p></li><li><p>得到FDSC-1模块的输入 skip1 [32, 64, 80, 80]</p></li><li><p>in_channel = 64,out_channel=128,kernel_size =4 x 4,stride=2,padding=1 的变形卷积层 + IN(128) 归一化 + Activation(ReLu)</p></li><li><p>得到FDSC-2模块的输入 skip2 [30,128,40,40]</p></li><li><p>in_channel =128,out_channel=256,kernel_size =4 x 4,stride=2,padding=1 的变形卷积层 + IN(256) 归一化 + Activation(ReLu)</p></li><li><p>N个ResBlock</p><ul><li><p>每个ResBlock如下：</p></li><li><p>Input---&gt;Conv2D---&gt;Conv2D---&gt; + ---&gt; output</p><p><span class="math inline">\(\downarrow\)</span>-------------------------------------------<span class="math inline">\(\uparrow\)</span></p></li><li><p>Conv2D,in_dim=256,out_dim=256,kernel_size=3x3,stride=1,padding=1 普通卷积层</p></li></ul></li></ul></li><li><p><strong>举例</strong>：</p><ul><li>假设输入图像为 80 * 80 ，Batch_Size = 32 , 那么：skip1,skip2和最终的output <span class="math inline">\(Z_c\)</span>的大小分别如下：</li><li>skip1.shape: torch.Size([32, 64, 80, 80])</li><li>skip2.shape: torch.Size([32, 128, 40, 40])</li><li>ouput.shape: torch.Size([32, 256, 20, 20])</li></ul></li><li><p><strong>意义</strong>：提取Content Images的结构特征，应用变形卷积层能够保持字体笔画结构</p></li></ul><h3 id="feature-deformation-skip-connection-fdsc-module">4) Feature Deformation Skip Connection ( FDSC Module)</h3><ul><li><p><strong>概述</strong>：其由一个变形卷积层组成，具体的作用作者在书写时写在了Mixer的卷积层后的内容里</p></li><li><p><strong>实现细节</strong></p><ul><li><p>首先，其输入来自于Content Encoder，也就是文中提及的skip1和skip2，我们以skip1为例继续讲解：</p></li><li><p>其次，它会将Skip1和Mixer中经过了Conv以后的内容A，Concat一起，然后将这个Concat完的东西放入变形卷积模块中，得到一个新计算的Concat_Pre,最后将这个Concat_Pre和A再Concat到一起，得到最终的输出。</p></li><li><p>这就是为什么在论文的示意图中：这个FDSC模块有来回的箭头表示：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_19_15_31_59_728.png" alt="屏幕捕获_2022_01_19_15_31_59_728" /><figcaption aria-hidden="true">屏幕捕获_2022_01_19_15_31_59_728</figcaption></figure></li></ul></li></ul><h3 id="mixer">5) Mixer</h3><ul><li><p><strong>概述</strong>：通过混合<span class="math inline">\(Z_C\)</span> 和<span class="math inline">\(Z_S\)</span> 来生成输出字符。其使用<strong>AdaIN</strong>方法将style特征注入Mixer中。</p></li><li><p><strong>输入</strong>： <span class="math inline">\(Z_c \in R\)</span> 即ContentEncoder的输出，256通道的特征图 [32,256,20,20]</p></li><li><p><strong>输出</strong>：</p></li><li><p><strong>网络结构及细节</strong>：</p><ul><li>2个ResBlock，输出仍然为256通道的特征图 [32,256,20,20]</li><li>Upsample上采样 ，特征图大小变为 40 x 40</li><li>Conv2dBlock，in_channel = 256,out_channel=128,kernel_size =5 x 5,stride=1,padding=2 的卷积层 + AdIN(128) 归一化 + Activation(ReLu), 经过该层后，计算得到的大小应当为 [32,128,40,40]<ul><li>然后，需要将此层输出的output和skip2在Channel通道Concat起来，得到deformable_concat [32,256,40,40]</li><li>然后将deformable_concat与skip2 输入 FDSC的变形卷积模块中，得到 concat_pre [32,128,40,40] 和 offset2 [32,18,40,40]</li><li>最后将cancat_pre和最开始的output Concat起来，得到该步的最终输出，大小为 [32，256，40，40]</li></ul></li><li>Upsample上采样，特征图大小变为80 x 80</li><li>Conv2dBlock，in_channel = 256,out_channel=64,kernel_size =5 x 5,stride=1,padding=2 的卷积层 + AdIN(64) 归一化 + Activation(ReLu), 经过该层后，计算得到的大小应当为 [32,64,80,80]<ul><li>然后，需要将此层输出的output和skip1在Channel通道Concat起来，得到deformable_concat [32,128,80,80]</li><li>然后将deformable_concat与skip1 输入 变形卷积模块中，得到 Concat_pre [32,128,40,40] 和 offset2 [32,18,40,40]</li><li>最后将cancat_pre和最开始的outputConcat起来，得到该步的最终输出，大小为 [32，256，40，40]</li></ul></li><li>Conv2dBlock，in_channel = 128,out_channel=3,kernel_size =7 x 7,stride=1,padding=3 的卷积层 + Activation(Tanh), 经过该层后，计算得到的大小应当为 [32,3,80,80]</li></ul></li></ul><h3 id="multi-task-discriminator">6) Multi-Task Discriminator</h3><ul><li><p><strong>概述</strong>：当字符图像从生成网络生成后，该判别器用来对每个单独的 Style 同时执行判断任务。对于每一个style来说， Discriminator的输出是一个二元分类器， 判断其是真实图片还是生成图片。</p></li><li><p><strong>输入</strong>：</p><ul><li>- x: images of shape (batch, 3, image_size, image_size).，例如为 4个 3 * 64 * 64的图像</li><li>- y: domain indices of shape (batch). 例如 y_in 为 4个需要其判断的 domain的标号</li></ul></li><li><p><strong>输出</strong>：各个需要判断的得分情况</p></li><li><p><strong>示例</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">D = Discriminator(<span class="number">64</span>, <span class="number">4</span>)  <span class="comment"># 初始化判别器参数 64 为图像大小，4为该判别器需要判别区分的domain的数量</span></span><br><span class="line">x_in = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>)  <span class="comment"># x_in 为 2个 3*64*64的图像  y_in 为 2个需要其判断的 domain的标号</span></span><br><span class="line">y_in = torch.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">2</span>, )) <span class="comment"># 假设为[1,3] 就是要让D去判断，第一个图是否属于domain1，第二个图是否属于domain3……</span></span><br><span class="line">out, feat = D(x_in, y_in)</span><br><span class="line"><span class="built_in">print</span>(out, feat)    </span><br><span class="line"><span class="built_in">print</span>(out.shape, feat.shape)  <span class="comment"># out为Discriminator打的分数（内部简化过）,feat为没处理过的原始输出，差别见下示例</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">out。shape = 1 * 2 ,out的第一个值相当于取了feat中[1,1,1,1]因为是第一个图片，并且是判断是否属于domain1的值，即为1.7796</span></span><br><span class="line"><span class="string">out的第二个值相当于取了feat中[2,3,1,1]因为是第二个图片，并且是判断是否属于domain3的值，即为0.9986</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">out: [1.7796, 0.9986]    </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">feat。shape = 2 * 4 * 1 * 1 ，其有2张需要判断的图像，第一张图像有4个值，分别是根据domain0-domain3打的分数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">feat:tensor([[[[-0.8181]],  </span></span><br><span class="line"><span class="string">         [[ 1.7796]],</span></span><br><span class="line"><span class="string">         [[ 3.0122]],</span></span><br><span class="line"><span class="string">         [[ 0.9614]]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[[-0.5011]],</span></span><br><span class="line"><span class="string">         [[ 0.5076]],</span></span><br><span class="line"><span class="string">         [[ 3.4774]],</span></span><br><span class="line"><span class="string">         [[ 0.9986]]]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></li><li><p><strong>网络细节（官方代码）</strong></p><ul><li>与论文附录一致：</li><li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220119155654674.png" title="fig:" alt="image-20220119155654674" /></li></ul></li></ul><h2 id="dg-font网络的训练验证以及主函数见下一篇dg-font代码详解">6、DG-Font网络的训练、验证以及主函数（见下一篇DG-Font代码详解）：</h2><p><strong>注：此部分结合了官方源码的内容，讲述训练的整体过程</strong></p>]]></content>
    
    
    <summary type="html">作者提出了可变形生成网络（Deformable Generative Networks）来做非监督的字体生成。其利用提供的目标字体图像（style image input）来将一种字体的字符变形和转换为另一种字体。DG-FONT 分离了字体的style和content，然后再将两个domain的表示进行融合，生成新的字体的字符。其核心模块为一个叫做FDSC（feature deformation skip connection）的东西，可以用来预测一个位移映射map然后使用位移映射map去对low-level的特征图做变形卷积。然后FDSC的输出被送入一个混合器，然后生成最终的结果。FDSC模块将会对内容图像的低层级特征进行变换，其能够保留文字本身的模式，比如笔画和偏旁部首信息。因为对于内容相同的两种不同风格的字体，它们的每一笔画通常都是对应的（如下图所示）。利用字体的空间关系，利用FDSC进行空间变形，有效地保证了生成的字符具有完整的结构。</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://blog.slks.xyz/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Font Generation" scheme="https://blog.slks.xyz/tags/Font-Generation/"/>
    
    <category term="Deformable Convolution Skip Connection" scheme="https://blog.slks.xyz/tags/Deformable-Convolution-Skip-Connection/"/>
    
    <category term="Unsupervised Learning" scheme="https://blog.slks.xyz/tags/Unsupervised-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch学习笔记10——PyTorch 中，nn与nn.functional有什么区别？（搬运）</title>
    <link href="https://blog.slks.xyz/2022/01/18/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010%E2%80%94%E2%80%94nn.Functional%E5%92%8Cnn%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://blog.slks.xyz/2022/01/18/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010%E2%80%94%E2%80%94nn.Functional%E5%92%8Cnn%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2022-01-18T07:51:19.000Z</published>
    <updated>2022-01-28T12:18:29.891Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch-中nn-与-nn.functional-有什么区别">PyTorch 中，nn 与 nn.functional 有什么区别？</h1><p>​ 注：在阅读代码以及Pytorch文档的时候发现，nn和nn.functional有很多相同的函数，文档中也有许多引用，故而搜索了一下有何区别，该文为搬运文，以下为引用说明，避免产生误会。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">作者：肥波喇齐</span><br><span class="line">链接：https://www.zhihu.com/question/66782101/answer/579393790</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br></pre></td></tr></table></figure><h4 id="两者的相同之处">1、两者的相同之处：</h4><ul><li><code>nn.Xxx</code>和<code>nn.functional.xxx</code>的实际功能是相同的，即<code>nn.Conv2d</code>和<code>nn.functional.conv2d</code> 都是进行卷积，<code>nn.Dropout</code> 和<code>nn.functional.dropout</code>都是进行dropout,………………；</li><li>运行效率也是近乎相同。</li></ul><p>​ <code>nn.functional.xxx</code>是函数接口，而<code>nn.Xxx</code>是<code>nn.functional.xxx</code>的类封装，并且<strong><code>nn.Xxx</code>都继承于一个共同祖先<code>nn.Module</code>。</strong>这一点导致<code>nn.Xxx</code>除了具有<code>nn.functional.xxx</code>功能之外，内部附带了<code>nn.Module</code>相关的属性和方法，例如<code>train(), eval(),load_state_dict, state_dict</code>等。</p><h4 id="两者的差别之处">2、两者的差别之处：</h4><ul><li><strong>两者的调用方式不同。</strong></li></ul><p><code>nn.Xxx</code> 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.rand(<span class="number">64</span>, <span class="number">3</span>, <span class="number">244</span>, <span class="number">244</span>)</span><br><span class="line">conv = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">out = conv(inputs)</span><br></pre></td></tr></table></figure><p><code>nn.functional.xxx</code>同时传入输入数据和weight, bias等其他参数 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weight = torch.rand(<span class="number">64</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">bias = torch.rand(<span class="number">64</span>) </span><br><span class="line">out = nn.functional.conv2d(inputs, weight, bias, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li><p><strong><code>nn.Xxx</code>继承于<code>nn.Module</code>， 能够很好的与<code>nn.Sequential</code>结合使用， 而<code>nn.functional.xxx</code>无法与<code>nn.Sequential</code>结合使用。</strong></p></li><li><p><strong><code>nn.Xxx</code>不需要你自己定义和管理weight；而<code>nn.functional.xxx</code>需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。</strong></p><p>例如：使用<code>nn.Xxx</code>定义一个CNN</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.cnn1 = nn.Conv2d(in_channels=<span class="number">1</span>,  out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>,padding=<span class="number">0</span>)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        self.cnn2 = nn.Conv2d(in_channels=<span class="number">16</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>,  padding=<span class="number">0</span>)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">4</span> * <span class="number">4</span> * <span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.maxpool1(self.relu1(self.cnn1(x)))</span><br><span class="line">        out = self.maxpool2(self.relu2(self.cnn2(out)))</span><br><span class="line">        out = self.linear1(out.view(x.size(<span class="number">0</span>), -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>使用<code>nn.function.xxx</code>定义一个与上面相同的CNN。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.cnn1_weight = nn.Parameter(torch.rand(<span class="number">16</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        self.bias1_weight = nn.Parameter(torch.rand(<span class="number">16</span>))</span><br><span class="line">        </span><br><span class="line">        self.cnn2_weight = nn.Parameter(torch.rand(<span class="number">32</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        self.bias2_weight = nn.Parameter(torch.rand(<span class="number">32</span>))</span><br><span class="line">        </span><br><span class="line">        self.linear1_weight = nn.Parameter(torch.rand(<span class="number">4</span> * <span class="number">4</span> * <span class="number">32</span>, <span class="number">10</span>))</span><br><span class="line">        self.bias3_weight = nn.Parameter(torch.rand(<span class="number">10</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = F.conv2d(x, self.cnn1_weight, self.bias1_weight)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = F.max_pool2d(out)</span><br><span class="line">        </span><br><span class="line">        out = F.conv2d(x, self.cnn2_weight, self.bias2_weight)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = F.max_pool2d(out)</span><br><span class="line">        </span><br><span class="line">        out = F.linear(x, self.linear1_weight, self.bias3_weight)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></li></ul><p>​ 上面两种定义方式得到CNN功能都是相同的，但PyTorch官方推荐：具有学习参数的（例如，conv2d, linear, batch_norm)采用<code>nn.Xxx</code>方式，没有学习参数的（例如，maxpool, loss func, activation func）等根据个人选择使用<code>nn.functional.xxx</code>或者<code>nn.Xxx</code>方式。</p><p>​ 但关于<strong>dropout</strong>，个人强烈推荐使用<code>nn.Xxx</code>方式，因为一般情况下只有训练阶段才进行dropout，在eval阶段都不会进行dropout。使用<code>nn.Xxx</code>方式定义dropout，在调用<code>model.eval()</code>之后，model中所有的dropout layer都关闭，但以<code>nn.function.dropout</code>方式定义dropout，在调用<code>model.eval</code>之后并不能关闭dropout。</p>]]></content>
    
    
    <summary type="html">PyTorch 中，nn与nn.functional的区别与联系</summary>
    
    
    
    <category term="Pytorch学习笔记" scheme="https://blog.slks.xyz/categories/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Pytorch" scheme="https://blog.slks.xyz/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础系列笔记9——归一化方法FRN(Filter Response Normalization)</title>
    <link href="https://blog.slks.xyz/2022/01/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B09%E2%80%94FRN/"/>
    <id>https://blog.slks.xyz/2022/01/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B09%E2%80%94FRN/</id>
    <published>2022-01-18T05:46:19.000Z</published>
    <updated>2022-01-28T12:20:35.062Z</updated>
    
    <content type="html"><![CDATA[<h4 id="一简介">一、简介</h4><p>​ <strong>在先前的文章中，链接：https://blog.slks.xyz/2022/01/16/basic7/，讲解了BN、LN、IN、CIN、GN等多种不同的Normalization，由于在DG-Font的阅读过程中，又发现了一个新的归一化方法叫做FRN，故而在此篇略做记录。</strong></p><p>​ <strong>FRN</strong>是谷歌提出的一种新的归一化方法，和GN一样不依赖batch，故而FRN层不仅消除了模型训练过程中对batch的依赖，而且当batch size较大时性能优于BN。</p><p>​ <strong>原论文名称：</strong>《Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks》</p><p>​ <strong>原论文地址：</strong>https://arxiv.org/abs/1911.09737</p><h4 id="二结构">二、结构：</h4><p>​ 如下所示，FRN层整体结构包括归一化层FRN（Filter Response Normalization）和激活层TLU（Thresholded Linear Unit）。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220118102128474.png" alt="image-20220118102128474" /><figcaption aria-hidden="true">image-20220118102128474</figcaption></figure><p>​ 整个结构并不复杂，就是通过一个公式计算，最后经过一个阈值限制输出</p><p>​ 其中FRN的操作是对每个样例的每个channel单独进行归一化的，这里<span class="math inline">\(x\)</span>就是一个N（=HxW）维度的向量，所以FRN没有BN层对batch依赖的问题。BN层采用归一化方法是减去均值然后除以标准差，而FRN却不同，这里没有减去均值操作，公式中的<span class="math inline">\(v^2\)</span>是<span class="math inline">\({x}\)</span>的二次范数的平均值。这种归一化方式类似BN可以用来消除中间操作（卷积和非线性激活）带来的尺度问题，有助于模型训练。 公式里的<span class="math inline">\({\epsilon}\)</span>是一个很小的正常量，一般为<span class="math inline">\(1^{-6}\)</span>以防止除0。</p><p>​ 一般情况下网络的特征图大小N(=HxW)较大，但是有时候可能会出现1x1的特征图的情况，比如InceptionV3和VGG网络，此时就<span class="math inline">\({\epsilon}\)</span>比较关键，</p><p>​ 归一化之后同样需要进行缩放和平移变换，这里的<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>也是可学习的参数（参数为长度是C的向量, 即为特征数目，也就是通道数):</p><p>​ <span class="math display">\[{y = \gamma \hat{x} + \beta}\]</span></p><p>​ FRN缺少去均值的操作，这可能使得归一化的结果任意地偏移0，如果FRN之后是ReLU激活层，可能产生很多0值，这对于模型训练和性能是不利的。为了解决这个问题，FRN之后采用的阈值化的ReLU，即TLU：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_18_10_32_35_986.png" alt="屏幕捕获_2022_01_18_10_32_35_986" /><figcaption aria-hidden="true">屏幕捕获_2022_01_18_10_32_35_986</figcaption></figure><p>​ 这里的<span class="math inline">\(\tau\)</span>是一个可学习的参数。原论文中发现FRN之后采用TLU对于提升性能是至关重要的。</p><h4 id="三代码实现coding">三、代码实现Coding</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FRN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_features, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FRN, self).__init__()</span><br><span class="line">        self.tau = nn.Parameter(torch.zeros(<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment">## input x  shape [B,C,H,W] (batch_size , channel , height , width)</span></span><br><span class="line">        x1 = torch.mean(x**<span class="number">2</span>, dim=[<span class="number">2</span>, <span class="number">3</span>], keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">## x1       shape [B,C,1,1] (batch_size , channel , 1 , 1)</span></span><br><span class="line">        x2 = x1 + self.eps</span><br><span class="line">        <span class="comment">## x2     shape [B,C,1,1] (batch_size , channel , 1 , 1),  【+ 为逐元素相加，不改变维度】</span></span><br><span class="line">        x3 = torch.rsqrt(x2)</span><br><span class="line">        <span class="comment">## x3       shape [B,C,1,1] (batch_size , channel , 1 , 1),  【sqrt 为逐元素操作，不改变为维度】</span></span><br><span class="line">        x4 = x * x3</span><br><span class="line">        <span class="comment">## x4       shape [B,C,H,W] (batch_size , channel , height , width)  </span></span><br><span class="line">        <span class="comment">##          广播机制,逐元素相乘  [B,C,H,W] * [B,C,1,1] = [B,C,H,W]</span></span><br><span class="line">        output = torch.<span class="built_in">max</span>(self.gamma * x4 + self.beta, self.tau)</span><br><span class="line">        <span class="comment">## output   shape [B,C,H,W] (batch_size , channel , height , width)  torch.max 逐元素操作</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment">## 相关注解：</span></span><br><span class="line"><span class="comment">## 输入的 x.shape = [B,C,H,W] ( Batch_Size \ Channel \ Height \ Width )</span></span><br><span class="line"><span class="comment">## torch.rsqrt()  对每个元素取平方根后再取倒数，并不会影响如维度等因素。</span></span><br><span class="line"><span class="comment">## torch.mean() 其中dim=[2,3] 代表按第2、3维求平均值, 即在单个实例、单个Channel上求平均</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">FRN是谷歌提出的一种新的归一化方法，和GN一样不依赖batch，故而FRN层不仅消除了模型训练过程中对batch的依赖，而且当batch size较大时性能优于BN。</summary>
    
    
    
    <category term="机器学习基础系列笔记" scheme="https://blog.slks.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Normalization" scheme="https://blog.slks.xyz/tags/Normalization/"/>
    
  </entry>
  
  <entry>
    <title>Torch/Numpy的广播机制介绍</title>
    <link href="https://blog.slks.xyz/2022/01/18/Python%E7%AC%94%E8%AE%B0/python_import_module/"/>
    <id>https://blog.slks.xyz/2022/01/18/Python%E7%AC%94%E8%AE%B0/python_import_module/</id>
    <published>2022-01-18T05:39:19.000Z</published>
    <updated>2022-01-28T12:21:36.245Z</updated>
    
    <content type="html"><![CDATA[<p>Numpy以及Tensor的广播机制介绍（以Torch为例，两者一致）</p><h3 id="广播机制在何处会出现">1、广播机制在何处会出现？</h3><p>​ 广播针对的运算是element wise类型的运算，即元素对元素类型的运算</p><p>​ <strong>Element-wise的计算符号包括如下：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">算数运算：+, -, *, /, //, %, divmod(), ** or pow(), &lt;&lt;, &gt;&gt;, &amp;, ^, |</span><br><span class="line"></span><br><span class="line">比较运算：==, &lt;, &gt;, &lt;=, &gt;=, !=</span><br></pre></td></tr></table></figure><h3 id="广播机制的规则与出现的原因">2、广播机制的规则与出现的原因</h3><p>​ 正常来说，两个做Element wise类型运算的变量，其相应维度的长度要相等，如下所示：这种形式做Element-Wise的运算是非常简单且易理解的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">b = torch.rand(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">c = a * b</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">a:tensor([[0.9912, 0.3736],</span></span><br><span class="line"><span class="string">        [0.0708, 0.6939]])</span></span><br><span class="line"><span class="string">b:tensor([[0.5788, 0.6296],</span></span><br><span class="line"><span class="string">        [0.9746, 0.8540]])</span></span><br><span class="line"><span class="string">c:tensor([[0.5737, 0.2352],</span></span><br><span class="line"><span class="string">        [0.0690, 0.5925]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​ 那么当两个Tensor的对应维度不对齐的时候，<strong>为了避免用户使用代码for循环来操作填补数组，导致一些低效率的行为出现</strong>，所以其提供了一种广播机制，其实质就是一种处理规则，在不对齐的维度上，长度较短的自动做值复制来扩充长度，从而使得两个Tensor在该维度上一致。</p><h3 id="广播机制起效的情况">3、广播机制起效的情况</h3><p>​ 但是，实际上，并不是所有的不同维度的Tensor相乘时，都会触发广播机制。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">b = torch.rand(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>​ 那么，广播机制在何时才会起效呢？其只对如下情况起作用：</p><p>​ <strong>两个Tensor，它们如果在某一个维度上长度不同的话，必定有一个Tensor在这个维度上的长度为1，广播机制才会起效。即 1 vs M 的情况</strong></p><p>​ <strong>具体计算规则：</strong>长度为1的Tensor在维度上会复制该元素并扩充至长度为M，当这个维度完成对齐，接着重复检查上一层维度，如此反复，直至所有维度都检查完。</p><p>​ <strong>示例</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">1</span>,<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.rand(<span class="number">3</span>,<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">c = a * b</span><br><span class="line"><span class="built_in">print</span>(c.shape)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([3, 5, 3])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>​ <strong>示意图</strong>：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/20200308204131296.png" alt="20200308204131296" /><figcaption aria-hidden="true">20200308204131296</figcaption></figure>]]></content>
    
    
    <summary type="html">本文讲解了Torch和Numpy中进行Element-Wise的计算的时候，采用的广播机制是什么，如何执行。</summary>
    
    
    
    <category term="python基础扩充笔记" scheme="https://blog.slks.xyz/categories/python%E5%9F%BA%E7%A1%80%E6%89%A9%E5%85%85%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Pytorch" scheme="https://blog.slks.xyz/tags/Pytorch/"/>
    
    <category term="Auto BroadCasting" scheme="https://blog.slks.xyz/tags/Auto-BroadCasting/"/>
    
  </entry>
  
  <entry>
    <title>Python导入自定义模块（同目录、子目录、跨目录）</title>
    <link href="https://blog.slks.xyz/2022/01/18/Python%E7%AC%94%E8%AE%B0/python_broadcast_module/"/>
    <id>https://blog.slks.xyz/2022/01/18/Python%E7%AC%94%E8%AE%B0/python_broadcast_module/</id>
    <published>2022-01-18T03:33:19.000Z</published>
    <updated>2022-01-28T12:21:32.108Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基本格式">1、基本格式：</h3><p>​ <strong>from 文件名 import 类名</strong></p><h3 id="分情形讨论引入方式">2、分情形讨论引入方式：</h3><p>​ 假设现有如下目录结构：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">├── model0.py</span><br><span class="line">├── main.py</span><br><span class="line">├── model1/</span><br><span class="line">│   └── model1_main.py</span><br><span class="line">└── model2/</span><br><span class="line">    └── model2_main.py</span><br></pre></td></tr></table></figure><h4 id="同级目录引入">1）同级目录引入</h4><p><strong>main.py</strong> 中需要导入 <strong>model0.py</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> model0</span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line"><span class="keyword">from</span> model0 <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><p>两者都是可以的，同级目录下引入十分简便，直接import即可</p><h4 id="子目录引入">2）子目录引入</h4><p><strong>main.py</strong> 中需要导入 <strong>model1_main.py</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们首先需要在model1/下建立__init__.py空文件，让编译器认为这是一个模块。</span><br></pre></td></tr></table></figure><p>建立后，<strong>目录结构应当如下所示：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── model0.py</span><br><span class="line">├── main.py</span><br><span class="line">├── model1/</span><br><span class="line">│   └── model1_main.py</span><br><span class="line">│   └── __init__.py</span><br><span class="line">└── model2/</span><br><span class="line">    └── model2_main.py</span><br></pre></td></tr></table></figure><p>然后进行引入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> model1.model1_main</span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line"><span class="keyword">from</span> model1.model1_main <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><h4 id="跨目录引入">3）跨目录引入</h4><p><strong>model1_main.py</strong>导入<strong>model2/model2_main.py</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">同理我们首先需要在model2/下建立__init__.py空文件，让编译器认为这是一个模块。</span><br></pre></td></tr></table></figure><p>建立后，<strong>目录结构应当如下所示：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── model0.py</span><br><span class="line">├── main.py</span><br><span class="line">├── model1/</span><br><span class="line">│   └── model1_main.py</span><br><span class="line">│   └── __init__.py</span><br><span class="line">└── model2/</span><br><span class="line">│   └── model2_main.py</span><br><span class="line">│   └── __init__.py</span><br></pre></td></tr></table></figure><p>然后在model1_main文件中进行引入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> model2.model2_main</span><br></pre></td></tr></table></figure><h4 id="更为一般的介绍">4）更为一般的介绍：</h4><p>sys模块是python内置的，我们导入跨自定义模块的步骤一般如下：</p><p>​ 首先要确保被导入的模块文件夹内有 __init__.py文件，确保其被识别为一个模块，然后再执行下面步骤：</p><ol type="1"><li><strong>先导入sys模块</strong></li><li>然后通过<code>sys.path.append(path)</code> 函数来导入自定义模块所在的目录</li><li><strong>导入自定义模块</strong>。</li></ol><p>​ 只不过在同级目录以及子目录下进行引入，不需要这么复杂，可以有更简单的方法，而在跨目录引入时，就需要采用这种方案。</p>]]></content>
    
    
    <summary type="html">本文讲解了Python导入自定义模块的方式，分为同目录、子目录、跨目录三种情况，同时也讲解了引入的一般形式。</summary>
    
    
    
    <category term="python基础扩充笔记" scheme="https://blog.slks.xyz/categories/python%E5%9F%BA%E7%A1%80%E6%89%A9%E5%85%85%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Python" scheme="https://blog.slks.xyz/tags/Python/"/>
    
    <category term="Module Import" scheme="https://blog.slks.xyz/tags/Module-Import/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础系列笔记8——图像风格迁移方法AdaIN</title>
    <link href="https://blog.slks.xyz/2022/01/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B08%E2%80%94%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E7%AE%97%E6%B3%95%20AdaIN%20/"/>
    <id>https://blog.slks.xyz/2022/01/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B08%E2%80%94%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E7%AE%97%E6%B3%95%20AdaIN%20/</id>
    <published>2022-01-17T04:33:19.000Z</published>
    <updated>2022-01-28T12:20:28.535Z</updated>
    
    <content type="html"><![CDATA[<p><strong>AdaIN</strong>是一种经典的图片风格迁移算法，在 2017 年ICCV中提出。主要用于将一张图片(风格图) 中的风格、纹理迁移到另一张图片 (内容图)，同时要保留内容图的主体结构。如下所示：</p><p>​ <strong>论文名称：《Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization》</strong></p><p>​ <strong>论文链接：https://arxiv.org/abs/1703.06868</strong></p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/b219ebc4b74543a95e9180d3cb15748ab9011436.jpeg" alt="b219ebc4b74543a95e9180d3cb15748ab9011436" /><figcaption aria-hidden="true">b219ebc4b74543a95e9180d3cb15748ab9011436</figcaption></figure><h2 id="一adain简介">一、AdaIN简介</h2><p>​ 这篇论文的主要目标是实现<strong>实时的、任意风格的风格迁移（style transfer）</strong>，核心方法就是其提出的自适应实例标准化（<strong>Adaptive Instance Normalization，AdaIN</strong>），通过<strong>将内容图像（content image）特征的均值和方差对齐到风格图像（style image）的均值和方差</strong>来实现风格迁移。</p><p>​ 此外，这个方法还给用户非常多的控制权，包括如下：</p><ul><li>内容和风格的折中（trade off）<ul><li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116183935766.png" title="fig:" alt="image-20220116183935766" /></li></ul></li><li>风格插值（混合风格迁移）<ul><li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116183943127.png" title="fig:" alt="image-20220116183943127" /></li></ul></li><li>是否保留颜色<ul><li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116183918123.png" title="fig:" alt="image-20220116183918123" /></li></ul></li><li>对图像的特定区域进行风格迁移<ul><li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116183906461.png" title="fig:" alt="image-20220116183906461" /></li></ul></li></ul><h2 id="二前备知识">二、前备知识：</h2><p>​ 你需要熟悉Batch Normaliztion（BN）、Layer Norm（LN）、Instance Norm（IN）、Group Norm（GN）、Conditional Instance Norm（CIN）等概念。</p><p>​ 下图为特征图张量，可以直观看出BN，LN，IN，GN等规范化方法的区别。N为样本维度，C为通道维度，H为height，W即width，代表特征图的尺寸。</p><figure><img src="https://img-blog.csdnimg.cn/2019061216413530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNTU3Njg4MQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>​ 具体每种Normalization的方法介绍可以参见我的这一篇Blog：</p><p>​ https://blog.slks.xyz/2022/01/16/basic7/</p><p>​</p><h2 id="三adain具体介绍">三、AdaIN具体介绍</h2><h5 id="输入content-input-x-style-input-y">输入：Content Input x &amp;&amp; Style Input y</h5><p>​ AdaIN 简单地将 x 的通道均值和方差对齐以匹配 y 的均值和方差。 不像BN、IN或CIN，AdaIN没有需要从网络中进行学习的仿射变换参数，它能够自适应的从style input中计算得到仿射变换的参数。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117103448955.png" alt="image-20220117103448955" /><figcaption aria-hidden="true">image-20220117103448955</figcaption></figure><p>​ 我们简单地用 σ(y) 缩放归一化的Content Input，并用 µ(y) 移动它。 与 IN 类似，这些统计数据是跨空间位置计算的。也就是说，其实对于单个实例的单个特征通道计算的均值和方差数据。</p><p>​ 直观地说，让我们考虑一个检测特定风格笔触的特征通道。</p><p>​ 具有这种风格的图像会对它的特征部分产生较高的平均激活。AdaIN 产生的输出将对该特征具有相同的高平均激活，同时其也保留内容图像的空间结构。 然后我们可以使用前馈解码器将风格特征转换回图像空间。同时，这个特征通道的方差可以编码更细微的风格信息，这些信息也传递到 AdaIN 输出和最终输出图像。</p><p>​ 简而言之，AdaIN 通过传输特征统计数据（特别是通道均值和方差）在特征空间中执行风格迁移。我们的 AdaIN 层就像一个 IN 层一样简单，几乎不增加计算成本。</p><h2 id="四网络结构">四、网络结构</h2><p>​ 在论文中，其使用VGG-19来编码内容和风格，在浅层空间将特征图通过AdaIN层，进行上述仿射变换，解码器根据变换后的特征图试图重建图像，通过反向传播训练解码器，使得解码器输出越来越真实的图像。整体架构如下所示：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/2019061217185282.png" alt="2019061217185282" /><figcaption aria-hidden="true">2019061217185282</figcaption></figure><p>​ 更为具体的代码可见以下链接地址：</p><p>​ https://github.com/xunhuang1995/AdaIN-style</p>]]></content>
    
    
    <summary type="html">AdaIN是一种经典的图片风格迁移算法，在2017年ICCV中提出。主要用于将一张图片(风格图)中的风格、纹理迁移到另一张图片(内容图)，同时要保留内容图的主体结构。</summary>
    
    
    
    <category term="机器学习基础系列笔记" scheme="https://blog.slks.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Style Transfer" scheme="https://blog.slks.xyz/tags/Style-Transfer/"/>
    
    <category term="AdaIN" scheme="https://blog.slks.xyz/tags/AdaIN/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础系列笔记7——各种归一化Normalization方法（含BN、LN、IN、CIN、GN）</title>
    <link href="https://blog.slks.xyz/2022/01/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B07%E2%80%94BN%E3%80%81LN%E3%80%81IN%E3%80%81CIN%E3%80%81GN/"/>
    <id>https://blog.slks.xyz/2022/01/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B07%E2%80%94BN%E3%80%81LN%E3%80%81IN%E3%80%81CIN%E3%80%81GN/</id>
    <published>2022-01-16T04:33:19.000Z</published>
    <updated>2022-01-28T13:06:52.811Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本文介绍了常见的几种Normalization方式，如BatchNorm、LayerNorm、InstanceNorm、Conditional Instance Norm、Group Norm。 各种Normalization在理论上都能够起到平滑损失函数平面的效果，加速函数的收敛效果，但是它们在机器学习的各个领域上，各有偏重与优势。</strong></p><h3 id="目录概述">目录概述：</h3><ul><li><strong>BatchNorm</strong>：batch方向做归一化，算N * H * W的均值, 常用于CNN等视觉识别领域，如果当Batch的尺寸比较小或是在一些动态网络中时不适用。</li><li><strong>LayerNorm</strong>：channel方向做归一化，算C * H * W的均值，LN不适用于CNN等视觉识别领域，但是可在BN无法使用的领域如RNN和Batch Size较小时进行使用。</li><li><strong>InstanceNorm</strong>：一个channel一个实例内做归一化，算H * W的均值，其适用于批量较小且单独考虑每个像素点的场景中，如GAN生成网络，但在MLP或RNN或Feature Map较小的时候不适用。</li><li><strong>GroupNorm</strong>：将channel方向分group，然后每个group内做归一化，算(C  G) * H * W的均值</li></ul><h3 id="batch-normbn">1、Batch Norm（BN）</h3><p>​ Ioffe 和 Szegedy 的开创性工作引入了批量归一化（BN）层，通过归一化特征统计显示简化了前馈网络的训练。 BN 层最初旨在加速判别网络的训练，但也被发现在生成图像建模中有效。</p><h5 id="输入"><strong>输入</strong>：</h5><p>​ <strong>An input batch</strong> <span class="math inline">\(x \in R^{N \times C \times H \times W}\)</span></p><h5 id="说明"><strong>说明</strong>：</h5><p>​ Batch Normalization就是对一个Batch中的数据进行标准化，就是每一个值减去batch的均值，除以batch的标准差，计算公式如下：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116184553370.png" alt="image-20220116184553370" /><figcaption aria-hidden="true">image-20220116184553370</figcaption></figure><ul><li><p><span class="math inline">\(\gamma , \beta \in R^{C}\)</span>是从数据中训练得到的参数。</p></li><li><p><span class="math inline">\(\mu(x) , \sigma(x) \in R^{C}\)</span>是均值和方差，为每个特征通道（C）独立计算批量大小（N）和空间维度（H * W）</p></li><li><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116185246851.png" alt="image-20220116185246851" /><figcaption aria-hidden="true">image-20220116185246851</figcaption></figure></li></ul><h5 id="卷积网络中的bn"><strong>卷积网络中的BN</strong></h5><p>​ BN除了可以应用在MLP上，其在CNN网络中的表现也非常好，<strong>卷积网络和MLP的不同点是卷积网络中每个样本的隐层节点的输出是三维（宽度，高度，维度）的</strong>，而MLP是一维的。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_33_31_622.png" alt="屏幕捕获_2022_01_17_09_33_31_622" /><figcaption aria-hidden="true">屏幕捕获_2022_01_17_09_33_31_622</figcaption></figure><p>​ 在上图中，假设一个批量有 <span class="math inline">\(m\)</span>个样本，Feature Map的尺寸是 <span class="math inline">\({p \times q}\)</span>，通道数是<span class="math inline">\(d\)</span>。<strong>在卷积网络中，BN的操作是以Feature Map为单位的</strong>，因此一个BN要统计的数据个数为 <span class="math inline">\({m \times p \times q}\)</span>，每个Feature Map使用一组<span class="math inline">\(\gamma\)</span>和<span class="math inline">\({\beta}\)</span>.</p><h5 id="类比"><strong>类比</strong>：</h5><p>​ 如果把输入<span class="math inline">\(x \in R^{N \times C \times H \times W}\)</span>类比为一摞书，这摞书总共有 N 本，每本有 C 页，每页有 H 行，每行 W 个字符。BN 求均值时，相当于把这些书按页码一一对应地加起来（例如第1本书第36页，第2本书第36页......），再除以每个页码下的字符总数：N<strong>×</strong>H<strong>×</strong>W，因此可以把 <strong>BN 看成求“平均书”</strong>的操作（注意这个“平均书”每页只有一个字）。</p><h5 id="总结"><strong>总结：</strong></h5><p>​ BN是深度学习调参中非常好用的策略之一（另外一个可能就是Dropout），当你的模型发生<strong>梯度消失/爆炸或者损失值震荡比较严重</strong>的时候，在BN中加入网络往往能取得非常好的效果，因为BN能够起到平滑损失平面的作用。</p><p>​ BN也有一些不是非常适用的场景，在遇见这些场景时要谨慎的使用BN：</p><ul><li>受制于硬件限制，每个Batch的尺寸比较小，这时候谨慎使用BN；</li><li>在类似于RNN的<strong>动态网络</strong>中谨慎使用BN；</li><li>训练数据集和测试数据集方差较大的时候。</li></ul><h3 id="layer-normln">2、Layer Norm（LN）</h3><p>​ Layer Normalization（LN）的提出有效的解决了BN的这两个问题（一个是不适用于动态网络，一个是batch尺寸较小的时候）。LN和BN不同点是归一化的维度是互相垂直的。如下图所示。在图1中<span class="math inline">\(N\)</span>表示样本轴， <span class="math inline">\(C\)</span>表示通道轴， <span class="math inline">\(F\)</span>是每个通道的特征数量( W*H )。BN 如右侧所示，它是取不同样本的同一个通道的特征做归一化；LN则是如左侧所示，它取的是同一个样本的不同通道做归一化。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_33_37_355.png" alt="屏幕捕获_2022_01_17_09_33_37_355" /><figcaption aria-hidden="true">屏幕捕获_2022_01_17_09_33_37_355</figcaption></figure><h5 id="mlp中的ln"><strong>MLP中的LN</strong></h5><p>​ BN的两个缺点的产生原因均是因为<strong>计算归一化统计量时计算的样本数太少</strong>。LN是一个独立于batch size的算法，所以无论样本数多少都不会影响参与LN计算的数据量，从而解决BN的两个问题。</p><p>​ 先看MLP中的LN。设<span class="math inline">\(H\)</span>是一层中隐层节点的数量， <span class="math inline">\(l\)</span>是MLP的层数，我们可以计算LN的归一化统计量<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_24_28_295.png" alt="屏幕捕获_2022_01_17_09_24_28_295" /><figcaption aria-hidden="true">屏幕捕获_2022_01_17_09_24_28_295</figcaption></figure><p>​ 注意上面<strong>统计量的计算是和样本数量没有关系</strong>的，它的<strong>数量只取决于隐层节点的数量</strong>，所以只要隐层节点的数量足够多，我们就能保证LN的归一化统计量足够具有代表性。</p><p>​ 通过<span class="math inline">\(\mu ^{l}\)</span>和<span class="math inline">\(\sigma ^{l}\)</span>可以计算得到归一化后的值：<span class="math inline">\(\hat{a}^l\)</span></p><p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_26_22_177.png" alt="屏幕捕获_2022_01_17_09_26_22_177" /></p><p>​ BN的文章中介绍过几乎所有的归一化方法都能起到平滑损失平面的作用。<strong>所以从原理上讲，LN能加速收敛速度的。</strong>但我们发现，将LN添加到CNN后，实验结果表明LN破坏了卷积层学习到的特征，使得模型无法收敛，所以在CNN之后使用BN是一个较好的选择。</p><h5 id="类比-1"><strong>类比</strong>：</h5><p>​ LN 求均值时，相当于把每一本书的所有字加起来，再除以这本书的字符总数：C<strong>×</strong>H<strong>×</strong>W，即求整本书的“平均字”，求标准差时也是同理。</p><h5 id="总结-1"><strong>总结</strong></h5><p>​ 总体而言，LN是和BN非常近似的一种归一化方法，不同的是<strong>BN取的是不同样本的同一个特征，而LN取的是同一个样本的不同特征</strong>。在BN和LN都能使用的场景中，<strong>BN的效果一般优于LN，原因是基于不同数据，同一特征得到的归一化特征更不容易损失信息。</strong></p><p>​ 但是有些场景是不能使用BN的，例如batchsize较小或者在RNN中，这时候可以选择使用LN，LN得到的模型更稳定且起到正则化的作用。RNN能应用到小批量和RNN中是因为LN的归一化统计量的计算是和batchsize没有关系的</p><h3 id="instance-normin">3、Instance Norm（IN）</h3><p>​ 对于图像风格迁移这类<strong>注重每个像素的任务</strong>来说，每个样本的每个像素点的信息都是非常重要的，于是像Batch Normlization这种每个批量的所有样本都做归一化的算法就不太适用了，因为BN计算归一化统计量时考虑了一个批量中所有图片的内容，从而<strong>造成了每个样本独特细节的丢失</strong>。同理对于LayerNormalization这类需要考虑一个样本所有通道的算法来说可能忽略了不同通道的差异，也不太适用于图像风格迁移这类应用。</p><p>​ 所以一篇论文提出了Instance Normalization（IN），一种更适合对单个像素有更高要求的场景的归一化算法（IST，GAN等）。IN的算法非常简单，<strong>计算归一化统计量时考虑单个样本，单个通道的所有元素</strong>。IN（右）和BN（中）以及LN（左）的不同从图1中可以非常明显的看出。<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_33_44_819.png" alt="屏幕捕获_2022_01_17_09_33_44_819" /></p><p>​ IN方法计算公式如下：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117093553291.png" alt="image-20220117093553291" /><figcaption aria-hidden="true">image-20220117093553291</figcaption></figure><p>​ 不同于BN(x),这边的<span class="math inline">\(\mu(x) , \sigma(x) \in R^{C}\)</span>是均值和方差，是为每个实例的每个特征通道（C）独立计算的。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117093715421.png" alt="image-20220117093715421" /><figcaption aria-hidden="true">image-20220117093715421</figcaption></figure><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117093735940.png" alt="image-20220117093735940" /><figcaption aria-hidden="true">image-20220117093735940</figcaption></figure><p>​ IN在计算归一化统计量时并没有像BN那样跨样本、单通道，也没有像LN那样单样本、跨通道。它是取的单通道，单样本上的数据进行计算。所以对比BN的公式，它只需要它只需要去掉批量维的求和即可。</p><h5 id="类比-2"><strong>类比</strong>：</h5><p>​ IN 求均值时，相当于把一页书中所有字加起来，再除以该页的总字数：H<strong>×</strong>W，即求每页书的“平均字”，求标准差时也是同理。</p><h5 id="总结-2"><strong>总结</strong>：</h5><p>​ IN本身是一个非常简单的算法，<strong>尤其适用于批量较小且单独考虑每个像素点的场景中</strong>，因为其计算归一化统计量时没有混合批量和通道之间的数据，对于这种场景下的应用，我们可以考虑使用IN。</p><p>​ 另外需要注意的一点是在图像这类应用中，每个通道上的值是比较大的，因此也能够取得比较合适的归一化统计量。但是有两个场景建议不要使用IN:</p><ol type="1"><li>MLP或者RNN中：因为在MLP或者RNN中，每个通道上只有一个数据，这时会自然不能使用IN；</li><li>Feature Map比较小时：因为此时IN的采样数据非常少，得到的归一化统计量将不再具有代表性。</li></ol><h3 id="group-normgn">4、Group Norm（GN）</h3><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/n6ck05dvpj.png" alt="n6ck05dvpj" /><figcaption aria-hidden="true">n6ck05dvpj</figcaption></figure><p>​ GN 把通道分为组，并计算每一组之内的均值和方差，以进行归一化。GN 的计算与批量大小无关，其精度也在各种批量大小下保持稳定。可以看到，GN和LN很像。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/1496926-9e0fd762d02d26c1.webp" alt="1496926-9e0fd762d02d26c1" /><figcaption aria-hidden="true">1496926-9e0fd762d02d26c1</figcaption></figure><h5 id="类比-3"><strong>类比</strong>：</h5><p>​ GN 相当于把一本 C 页的书平均分成 G 份，每份成为有 C/G 页的小册子，求每个小册子的“平均字”和字的“标准差”。</p><h5 id="总体"><strong>总体</strong>：</h5><p>​ <strong>LN 和 IN 在视觉识别上的成功率都是很有限的</strong>，对于<strong>训练序列模型（RNN/LSTM）或生成模型（GAN）</strong>很有效。所以，<strong>在视觉识别领域，BN用的比较多，GN就是为了改善BN的不足而来的。</strong></p><p>​ <strong>GN适用于占用显存比较大的任务，例如图像分割</strong>。对这类任务，可能 batchsize 只能是个位数，再大显存就不够用了。而当 batchsize 是个位数时，BN 的表现很差，因为没办法通过几个样本的数据量，来近似总体的均值和标准差。GN 是独立于 batch 的,所以可以适用。</p><h3 id="conditional-instance-normcin">5、Conditional Instance Norm（CIN）</h3><p>​ Dumoulin等人在进行风格迁移任务，使用IN的时候，用不同的<span class="math inline">\(\gamma\)</span>和 <span class="math inline">\(\beta\)</span>即可生成出风格不同的图像，于是提出了Conditional Instance Normalization(CIN)。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117102111791.png" alt="image-20220117102111791" /><figcaption aria-hidden="true">image-20220117102111791</figcaption></figure><p>​ 其中s代表风格，<span class="math inline">\(\gamma ^s\)</span>和 <span class="math inline">\(\beta ^s\)</span>是学出来的，一组 <span class="math inline">\((\gamma ^s,\beta ^s)\)</span>对应一种风格。Dumoulin等人的方法迁移有限种的风格，想迁移新的的风格则需要训练新的模型。</p><p><strong>部分内容参考链接：</strong></p><p>https://zhuanlan.zhihu.com/p/54530247</p><p>https://zhuanlan.zhihu.com/p/56542480</p><p>https://www.jianshu.com/p/f15fcdf13438</p>]]></content>
    
    
    <summary type="html">本文介绍了常见的几种Normalization方式，如BatchNorm、LayerNorm、InstanceNorm、Conditional Instance Norm、Group Norm。 各种Normalization在理论上都能够起到平滑损失函数平面的效果，加速函数的收敛效果，但是它们在机器学习的各个领域上，各有偏重与优势。</summary>
    
    
    
    <category term="机器学习基础系列笔记" scheme="https://blog.slks.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Normalization" scheme="https://blog.slks.xyz/tags/Normalization/"/>
    
  </entry>
  
  <entry>
    <title>《Deep Residual Fourier Transformation for Single Image Deblurring》论文笔记</title>
    <link href="https://blog.slks.xyz/2022/01/14/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E3%80%8ADeep%20Residual%20Fourier%20Transformation%20for%20Single%20Image%20Deblurring%E3%80%8B/"/>
    <id>https://blog.slks.xyz/2022/01/14/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E3%80%8ADeep%20Residual%20Fourier%20Transformation%20for%20Single%20Image%20Deblurring%E3%80%8B/</id>
    <published>2022-01-14T10:32:19.000Z</published>
    <updated>2022-01-28T13:08:55.589Z</updated>
    
    <content type="html"><![CDATA[<h4 id="论文名称deep-residual-fourier-transformation-for-single-image-deblurring">论文名称：《Deep Residual Fourier Transformation for Single Image Deblurring》</h4><h4 id="论文地址-httpsarxiv.orgabs2111.11745">论文地址： https://arxiv.org/abs/2111.11745</h4><h2 id="关键词">1、关键词：</h2><p>​ Image Deblurring、FFT、ResBlock</p><h2 id="领域背景">2、领域背景：</h2><p>​ Image Deblurring 图像去模糊，往往指由非常规相机或物体移动、光学虚焦等因素引起的一种现象，他们会导致看上去低质量的图像。</p><h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2><p>​ 在先前，DeepDeblur使用CNN配合ResBlock，构建了多尺度的一个结构，使用Residual Block来聚焦学习模糊的图像和清晰的图像对之间的差距。并且取得了非常好的效果。但是，它也有一定的局限性：</p><ul><li><p>ResBlock通常在CNN中进行，其感知域容易受到限制（尤其是在比较前面的层），所以ResBlock的机制往往会无法对全局的信息进行建模（这些信息往往在从一个模糊图像重建一个清晰图像的时候较为有用）</p></li><li><p>先前的方法很少从频域的角度去关注模糊的图像和清晰的图像之间的关系，而我们发现，相较于模糊图像，清晰的图像往往包含更少的低频信息以及更多的高频信息。</p></li><li><p>CNN在捕获可见的特征的时候很厉害，但是对于频域的特征较弱，并且ResBlock 可能具备良好的高频信息的学习，但是对于低频信息的学习较弱一些。</p></li><li><p>论文贡献</p><ul><li>这篇论文提出的 <em>Residual Fast Fourier Transform with Convolution Block (Res FFT-Conv Block)</em>既能够捕获长距离信息，也能捕获短距离信息，同时也有能力考虑整个高频和低频的信息。通过使用这个Block还提出了一个框架，可以应用于图像去模糊领域 <em>Residual Fourier Transformation (DeepRFT) framework</em></li></ul></li></ul><h2 id="主要设计思想">4、主要设计思想：</h2><p>​ 提出了<strong>Residual Fast Fourier Transform with Convolution Block（Res FFT-Conv Block）</strong>模块，这是一个即插即用的模块。设计思想如下所示：</p><ul><li>Res FFT-Conv模块 将图像从空间域转至频域，然后使用<span class="math inline">\(1 \times 1\)</span>的卷积层进行卷积。由于FFT的特性，就能够使得在很early的层上卷积的感知域就能包含整个图像。其能更好的捕获模糊图像和清晰图像之间全局的差异。</li></ul><p>​ 提出了<strong>Deep Residual Fourier Transformation (DeepRFT) 框架</strong>，主要操作如下：</p><ul><li>DeepRFT框架通过将Res FFT-Conv模块插入进MIMO-UNet这个网络结构中，来进行图像去模糊的任务同时，使用DepthWise Over-parameterized Conolution以加速网络训练，达到很好的效果。</li></ul><h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2><h3 id="resblocklook-back">1) ResBlock（Look Back）</h3><ul><li><p><strong>网络结构</strong>：包含两个<span class="math inline">\(3 \times 3\)</span>卷积层 以及 一个 RELU激活函数层</p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114155111786.png" alt="image-20220114155111786" style="zoom: 50%;" /></p></li><li><p><strong>意义</strong>：能够训练更深的网络结构、拥有更大的感知域、加快训练时的收敛速度。卷积操作能够很好的学习到一些图像中的高频信息，因为其经常能够从图像的edges中捕获信息。</p></li><li><p><strong>缺陷</strong>：</p><ul><li>缺少对低频信息的建模能力</li><li>虽然我们能够通过堆叠模块来加大感知域，但是堆叠会带来巨大的计算复杂度。且在前几层网络中，感知域大小还是非常局部的，缺少全局的信息。</li></ul></li></ul><h3 id="residual-fast-fourier-transform-blockcurrent-substitute">2）Residual Fast Fourier Transform Block（Current Substitute）</h3><ul><li><p><strong>输入</strong>：<span class="math inline">\(Z \in R^{H \times W \times C}\)</span></p></li><li><p><strong>网络结构</strong>：两条残差流：</p><ul><li>与ResBlock一致的空间域信息残差流</li><li>基于Channel-Wise FFT的频域信息流，用于在频域中捕获图像的全局信息。</li></ul><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114155802644.png" alt="image-20220114155802644" style="zoom: 50%;" /></p></li><li><p><strong>核心实现-公式表达</strong>（最左侧的频域信息流）</p><ol type="1"><li><span class="math inline">\(F(Z) \in C^{H \times W/2 \times C}\)</span>,对输入的特征Z，计算 2D real FFT ,由于傅立叶变换两侧对称，所以宽方向可以仅保留一半</li><li><span class="math inline">\(\widetilde{Z} = R(F(Z)) \odot_C I((F(Z))) \in R^{H \times W/2 \times 2C}\)</span>, 将傅立叶变换后的实数部分和虚数部分在Channel层面Concatenate起来</li><li>经过2个<span class="math inline">\(1 \times 1\)</span>的卷积层和1个ReLU激活函数层</li><li><span class="math inline">\(Y^{fft} = F^{-1}(f^{real} + jf^{img}) \in R^{H \times W \times C}\)</span>应用逆向2D real FFT来将 f <span class="math inline">\((f^{real} \odot_C f^{img} )\)</span>变换回空间域</li><li><span class="math inline">\(Y = Y^{fft} + Y^{res} + Z\)</span>，最终输出使用三个相加的形式得到。</li></ol></li></ul><h3 id="deep-residual-fourier-transform-framework">3） Deep Residual Fourier Transform Framework</h3><ul><li><p><strong>简介</strong>：基于MIMO-UNet进行的设计，MIMO-UNet是一个用于做多尺度图像去模糊的多输入多输出的U-Net架构。将MIMO-UNet中的所有ResBlocks用该论文提出的Res FFT-Conv Blocks进行替换。同时，我们额外将所有的<span class="math inline">\(1 \times 1\)</span>卷积层用我们提出的DO-Conv替换掉了。</p></li><li><p>MIMO-UNet 详细可以参见这篇论文：《Rethinking Coarse-to-Fine Approach in Single Image Deblurring》</p></li><li><p><strong>网络结构</strong>：</p><p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114170635133.png" alt="image-20220114170635133" style="zoom:50%;" /></p></li></ul><h3 id="depthwise-over-parameterized-convolution">4）<strong>Depthwise over-parameterized convolution</strong></h3><ul><li><p><strong>简介</strong>：DO-Conv已经在许多高级别的视觉任务中显示出了它的潜力，它加速了训练并且通过使用深度卷积增强卷积层来获得更好的性能。DO-Conv 是两个相邻的线性运算，在操作时可以组合成传统的卷积运算。</p></li><li><p>具体可以参见这篇论文《DO-Conv: Depthwise Over-parameterized Convolutional》</p></li><li><p>论文链接：https://arxiv.org/pdf/2006.12030.pdf</p></li></ul><h3 id="loss-function">5） Loss Function</h3><ul><li><p><span class="math inline">\(Let\)</span><span class="math inline">\({k \in {0,……,K-1}}\)</span>为 DeepRFT的第k个层级</p></li><li><p><span class="math inline">\({\hat{S_k}}\)</span>为 <span class="math inline">\(k_{th}\)</span>重建图像</p></li><li><p><span class="math inline">\(S_k 为\)</span> <span class="math inline">\(k_{th}\)</span>GroundTruth清晰的图像</p></li><li><p>考虑3种，不同类型的Loss Function：</p><ul><li><p>Multi-Scale Charbonnier loss：</p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114171903475.png" alt="image-20220114171903475" style="zoom:50%;" /></p></li><li><p>Multi-Scale Edge loss：</p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114171944558.png" alt="image-20220114171944558" style="zoom:50%;" /></p></li><li><p>Multi-Scale Frequency Reconstruction (MSFR) 在频域计算，FT代表FFT操作</p></li></ul></li></ul><p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114172909832.png" alt="image-20220114172909832" style="zoom: 50%;" /></p><ul><li>最终Loss函数：<span class="math inline">\(L = L_{msc} + \alpha_1L_{msed}+ \alpha_2L_{msfr}\)</span></li><li><span class="math inline">\(\alpha_1\)</span>、<span class="math inline">\(\alpha_2\)</span>为tradeoff参数，通常为0.05 和 0.01</li></ul>]]></content>
    
    
    <summary type="html">提出了Residual Fast Fourier Transform with Convolution Block（Res FFT-Conv Block）模块，是一个即插即用的模块。该模块将图像从空间域转至频域，然后使用1X1的卷积层进行卷积。由于FFT的特性，就能够使得在很浅的层上卷积的感知域就能包含整个图像，同时其也能更好的捕获模糊图像和清晰图像之间全局的差异。</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://blog.slks.xyz/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="FFT" scheme="https://blog.slks.xyz/tags/FFT/"/>
    
    <category term="Image Deblurring" scheme="https://blog.slks.xyz/tags/Image-Deblurring/"/>
    
    <category term="Residual Block" scheme="https://blog.slks.xyz/tags/Residual-Block/"/>
    
  </entry>
  
  <entry>
    <title>《SwinIR- Image Restoration Using Swin Transformer》论文笔记</title>
    <link href="https://blog.slks.xyz/2022/01/13/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E3%80%8ASwinIR-%20Image%20Restoration%20Using%20Swin%20Transformer%E3%80%8B/"/>
    <id>https://blog.slks.xyz/2022/01/13/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E3%80%8ASwinIR-%20Image%20Restoration%20Using%20Swin%20Transformer%E3%80%8B/</id>
    <published>2022-01-13T05:19:19.000Z</published>
    <updated>2022-01-28T12:13:54.435Z</updated>
    
    <content type="html"><![CDATA[<h3 id="论文名称swinir-image-restoration-using-swin-transformer">论文名称：《SwinIR: Image Restoration Using Swin Transformer》</h3><h3 id="论文链接httpsopenaccess.thecvf.comcontenticcv2021waimhtmlliang_swinir_image_restoration_using_swin_transformer_iccvw_2021_paper.html">论文链接：https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html</h3><h2 id="关键词">1、关键词：</h2><p>​ 图像修复（Image Restoration）、Transformer</p><h2 id="领域背景图像修复">2、领域背景—图像修复：</h2><p>​ 图像修复是一个经典问题，一般而言其目标为从低分辨率的图像中恢复出高分辨率的图像。通常可以用于超分辨率、图像去噪，以及JPEG压缩鬼影去除等应用。此篇文章将Swin Transformer应用于图像修复领域中。</p><h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2><p>​ 和CNN比起来，Transformer设计了Self-Attention机制来捕获全局内容之间的信息交互，也在一系列的任务重得到了比较好的结果。但是ViT用于图像恢复的话，因为Vit要划分patch，所以就会不可避免的导致恢复的图像在patch和patch之间有边界感，同时每一个patch邻近边界的像素也会因为缺少信息而没法做出更好的恢复效果。</p><p>​ <strong>先前方法的问题</strong>：使用基于CNN的方法进行图像修复：会存在两个源于卷积层本身带来的基本的问题：</p><p>1、图像和卷积核之间是内容无关的，我们用同一个卷积核去修复不同的图像区域，可能并不是一个好的选择</p><p>2、由于CNN的local processing方案，卷积层是在局部邻域（ local neighborhood ）内建立像素关系，其长距离的依赖关系（long-range dependency）主要通过深度叠加卷积层来进行建模，有的时候可能并不是很有效</p><p>​ 而Swin Transformer，结合了两者的优势。既能够像CNN那样处理大尺度的图像，也能弥补CNN在long-range dependency上的不足（使用SW-MSA机制）</p><h2 id="主要设计思想">4、主要设计思想：</h2><p>​ SwinIR由三部分组成。首先浅层特征提取部分是由卷积层组成的，输出结果将直接传输到重建模块中，为了保持图像本身的低频信息，而深层特征提取模块主要由RSTB（Residual Swin Transformer blocks）组成。同时，其在每一个block的后面还加了一层卷积层，来做特征加强，以及使用残差网络来为特征聚合提供捷径。最终，浅层和深层特征被输送到重建模块，进行高质量的图像重建。总体流程概括如下：</p><ul><li>浅层特征提取：<strong>低质量图像</strong> <span class="math inline">\(\to\)</span> <strong>浅层特征图</strong></li><li>深层特征提取：<strong>浅层特征图</strong><span class="math inline">\(\to\)</span><strong>深层特征图</strong></li><li>高质量图片生成：<strong>浅层特征图</strong>+<strong>深层特征图</strong><span class="math inline">\(\to\)</span><strong>高质量图像</strong></li></ul><h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/network_swinir.png" alt="network_swinir" /><figcaption aria-hidden="true">network_swinir</figcaption></figure><p>​ 首先，对于这一整个架构而言，分为3大块，分别是浅层特征提取、深层特征提取以及图像重建，<strong>对于不同的任务而言，我们使用相同的特征提取模块，但是会使用不同的图像重建模块</strong>。</p><h3 id="浅层特征提取shallow-feature-extraction">1）浅层特征提取(Shallow Feature Extraction)</h3><ul><li><strong>概述</strong>：使用卷积网络从<strong>低质量图像</strong>中提取<strong>浅层特征图</strong></li><li><strong>输入</strong>：<strong>低质量图像(LQ)</strong> <span class="math inline">\(I_{LQ}\in R^{H\times W\times C_{in}}\)</span></li><li><strong>输出</strong>：<strong>浅层特征图</strong> <span class="math inline">\(F_{0}\in R^{H\times W\times C_{embed}}\)</span>，<span class="math inline">\(C_{embed}\)</span>为特征通道数目</li><li><strong>网络结构</strong>：单层<span class="math inline">\(3\times3\)</span>卷积网络</li><li><strong>公式表达</strong>：<span class="math inline">\(F_{0}=H_{SF}(I_{LQ})\)</span>，其中<span class="math inline">\(H_{SF}\)</span>为卷积网络</li><li><strong>实现细节（官方代码）</strong><ul><li><span class="math inline">\(H_{SF}\)</span>为单层，输入图像通道<span class="math inline">\(C_{in}=3\)</span>，输出特征通道<span class="math inline">\(C=96\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络</li></ul></li><li><strong>意义</strong>：卷积层在前期的视觉处理中往往能起到较好的效果，能够导致一个较为稳定的结果，同时也提供了一个从图像空间映射到高维特征空间的手段。</li></ul><h3 id="深层特征提取deep-feature-extraction">2）深层特征提取(Deep Feature Extraction)：</h3><ul><li><p><strong>输入</strong>：<strong>浅层特征图</strong><span class="math inline">\(F_{0}\in R^{H\times W\times C_{embed}}\)</span></p></li><li><p><strong>输出</strong>：<strong>深层特征图</strong><span class="math inline">\(F_{DF}\in R^{H\times W\times C_{embed}}\)</span></p></li><li><p><strong>网络结构：</strong></p><ul><li><span class="math inline">\(K\)</span>个串联的Residual Swin Transformer Block（RSTB）和 <span class="math inline">\(1\)</span>个卷积层 构成</li><li>每个RSTB（Residual Swin Transformer Block）内部由<span class="math inline">\(L\)</span>个串联的 Swin Transformer Layer（STL）和 一个卷积层构成（如上图a）</li><li>STL（Swin Transformer Layer）结构为Layer Norm<span class="math inline">\(\to\)</span>MSA<span class="math inline">\(\to\)</span>Layer Norm<span class="math inline">\(\to\)</span>MLP 且 MSA和MLP后有残差连接（如上图b）</li></ul></li><li><p><strong>公式表达</strong>：</p><ul><li><strong>整体结构</strong>：<ul><li><span class="math inline">\(F_i=H_{RSTB_i}(F_{i-1}),\ i=1,2,...,K\)</span></li><li><span class="math inline">\(F_{DF}=H_{CONV}(F_{K})\)</span></li></ul></li><li><strong>RSTB</strong>：<ul><li><span class="math inline">\(F_{i,j}=H_{Swin_{i,j}}(F_{i,j-1}),\ j=1,2,...,L\)</span></li><li><span class="math inline">\(F_i=H_{CONV_i}(F_{i,L})\)</span></li></ul></li><li><strong>STL</strong>：<ul><li><span class="math inline">\(H_{Swin_{i,j}}(X)=STL_2(STL_1(X))\)</span></li><li><span class="math inline">\(STL_1(X)=MSA(LN(X))+X\)</span></li><li><span class="math inline">\(STL_2(X)=MLP(LN(X))+X\)</span></li></ul></li></ul></li><li><p><strong>实现细节（官方代码）</strong></p><ul><li><p><strong>PatchEmbed和PatchUnEmbed操作</strong></p><ul><li>代码中使用PatchEmbed操作将<span class="math inline">\(224\times224\)</span>的特征图拆分为<span class="math inline">\(16\times16\)</span>的Patch，并且有可选的LayerNorm操作</li><li>对应的代码中使用PatchUnEmbed操作将<span class="math inline">\(16\times16\)</span>的Patch还原为<span class="math inline">\(224\times224\)</span>的特征图</li><li>PatchEmbed和PatchUnEmbed操作的执行逻辑如下<ul><li>每一个RSTB的开头，执行PatchEmbed操作</li><li>每一个RSTB的卷积操作前，执行PatchUnEmbed操作</li><li>只在第一个RSTB开头的PatchEmbed操作中，使用LayerNorm</li></ul></li></ul></li><li><p><strong>RSTB的串联数量（<span class="math inline">\(K\)</span>的取值）</strong></p><ul><li>虽然论文图片中的RSTB有6个，但在代码中只有4个，即<span class="math inline">\(K=4\)</span></li></ul></li><li><p><strong>STL的串联数量（<span class="math inline">\(L\)</span>的取值）</strong></p><ul><li>代码中每个RSTB内部的STL的数目为6个，与论文图片中的一致，即<span class="math inline">\(L=6\)</span></li></ul></li><li><p><strong>配对的STL</strong></p><ul><li>如前述所说，每个RSTB中STL的数目为6个，其中每两个STL构成一组，第一个STL内的MSA为Swin中的W-MSA，第二个为Swin中的SW-MSA，这一操作与原版Swin中的一致</li></ul></li><li><p><strong>卷积操作</strong></p><ul><li>对于整个深层特征提取模块末尾和RSTB内部的卷积，代码根据任务不同分为两种<ol type="1"><li>对于小模型任务，如一般图片、轻量图片的SR、图片降噪和JPEG格式压缩图像修复，采用的是单层，输入输出维度为<span class="math inline">\(C_{embed}=96\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络</li><li>对于大模型任务，如真实世界图片SR，采用的是一个多层卷积网络<ul><li>第一层为输入维度为<span class="math inline">\(C_{embed}=96\)</span>，输出维度为<span class="math inline">\(C_{embed}/4\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对输出进行LeakyReLu</li><li>第二层为输入维度为<span class="math inline">\(C_{embed}/4\)</span>，输出维度为<span class="math inline">\(C_{embed}/4\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对输出进行LeakyReLu</li><li>第三层为输入维度为<span class="math inline">\(C_{embed}/4\)</span>，输出维度为<span class="math inline">\(C_{embed}\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络</li></ul></li></ol></li></ul></li></ul></li><li><p><strong>意义</strong>：</p><ul><li>作者认为在所有RSTB后面，再加一层Conv层，能够将卷积操作的归纳偏置(inductive bias)带入这类基于Transformer骨架的网络中，为后续浅层和深层特征的融合打好基础。</li><li><!--具体的作用需要通过实验验证，这个Conv层是否是必要的。--></li><li>对于单个RSTB内部的卷积操作，论文认为卷积核在空间上的不变性可以增强提取出特征的平移不变性</li><li>残差连接为不同的RSTB模块提供了一个到后续图像重建模块的短连接，允许不同级别的特征在最后一个重建模块中更好的进行聚合。</li><li><!--同样，具体的作用需要通过实验验证，这个Conv层是否是必要的。--></li></ul></li></ul><h3 id="高质量图像修复-hq-image-reconstruction">3）高质量图像修复 (HQ Image Reconstruction)：</h3><ul><li><strong>概述</strong>：根据<strong>浅层与深层特征图</strong>生成<strong>高质量图像</strong>，浅层特征负责包含低频信息，深层特征聚焦于恢复丢失的高频信息。</li><li><strong>输入</strong>：<strong>浅层特征图</strong> <span class="math inline">\(F_{0}\in R^{H\times W\times C_{embed}}\)</span> 和 <strong>深层特征图</strong> <span class="math inline">\(F_{DF}\in R^{H\times W\times C_{embed}}\)</span></li><li><strong>输出</strong>：<strong>高质量重建图像(RHQ)</strong> <span class="math inline">\(I_{RHQ}\in R^{H&#39;\times W&#39;\times C_{out}}\)</span></li><li><strong>网络结构</strong>：卷积网络</li><li><strong>公式表达</strong>：<span class="math inline">\(I_{RHQ}=H_{REC}(F_{0}+F_{DF})\)</span>，其中<span class="math inline">\(H_{REC}\)</span>为重建模块函数，其实在实现上就是一个卷积模块</li><li><strong>实现细节（官方代码）</strong><ul><li><span class="math inline">\(H_{REC}(F_{0}+F_{DF})\)</span>中的“+”号就是数学意义上的相加，本质上是残差连接</li><li><strong>卷积操作</strong><ul><li>根据任务不同分为四种卷积操作：<ol type="1"><li>对于图像去噪和JPEG格式压缩图像修复，采用的是单层，输入维度为<span class="math inline">\(C_{embed}=96\)</span>，输出维度为<span class="math inline">\(C_{out}=3\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，同时卷积输出与输入使用残差连接，即<span class="math inline">\(H_{REC}(X)=H_{CONV}(X)+X\)</span></li><li>对于一般图像SR，卷积分为三层<ul><li>第一层进行特征降维，为输入维度为<span class="math inline">\(C_{embed}=96\)</span>，输出维度为<span class="math inline">\(C_{feat}=64\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对卷积输出进行LeakyReLU</li><li>第二层进行上采样，先通过输入维度为<span class="math inline">\(C_{feat}=64\)</span>，输出维度为<span class="math inline">\(4\times C_{feat}\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，然后对卷积输出进行PixelShuffle</li><li>第三层为输入维度为<span class="math inline">\(C_{feat}=64\)</span>，输出维度为<span class="math inline">\(C_{out}=3\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络</li></ul></li><li>对于轻量图像SR，为了减少参数，采用的是单层，输入维度为<span class="math inline">\(C_{feat}=64\)</span>，输出维度为<span class="math inline">\(Scale^2\times C_{out}\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，然后对卷积输出进行PixelShuffle</li><li>对于真实世界图像SR，使用多层卷积<ul><li>第一层进行特征降维，为输入维度为<span class="math inline">\(C_{embed}=96\)</span>，输出维度为<span class="math inline">\(C_{feat}=64\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对卷积输出进行LeakyReLu</li><li>第二、三层进行上采样，先使用torch.nn.functional.interpolate函数进行指定<span class="math inline">\(Scale\)</span>的最近邻上采样，然后通过输入输出维度为<span class="math inline">\(C_{feat}=64\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对卷积输出进行LeakyReLU</li><li>第四层为输入输出维度为<span class="math inline">\(C_{feat}=64\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对卷积输出进行LeakyReLU</li><li>第五层为输入维度为<span class="math inline">\(C_{feat}=64\)</span>，输出维度为<span class="math inline">\(C_{out}=3\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络</li></ul></li></ol></li></ul></li></ul></li></ul><h3 id="其他细节">4） 其他细节：</h3><p>​ 上述总的结构中看到的<strong>Skip-Connection</strong>，是能够将浅层特征直接输入到重建模块中，让深层特征提取模块专注于高频信息的提取以及能够获得更稳定的训练。</p><h3 id="使用的损失函数根据任务情况略有不同">5） 使用的损失函数：（根据任务情况略有不同）</h3><ul><li><p>对于一般图像以及轻量图像SR，使用<strong>L1 Loss</strong>,<span class="math inline">\(I_{RHQ}\)</span> 为重建的高质量图像，<span class="math inline">\(I_{HQ}\)</span>为Ground_Truth高质量图像</p><ul><li><span class="math inline">\(L = \sqrt{||I_{RHQ} - I_{HQ} ||_1}\)</span></li></ul></li><li><p>对于真实世界图像SR，会结合<strong>Pixel Loss</strong>和<strong>GAN Loss</strong>以及<strong>Percepture Loss</strong>来提升生成质量</p></li><li><p>对于去噪和JPEG压缩任务而言，我们使用<strong>Charbonnier Loss</strong>进行优化</p><ul><li><span class="math inline">\(\sqrt{||I_{RHQ}-I_{HQ}||^2 + \varepsilon^2 }\)</span>, <span class="math inline">\(\varepsilon\)</span>是个常数，通常被设置为<span class="math inline">\(10^{-3}\)</span></li></ul></li></ul>]]></content>
    
    
    <summary type="html">SwinIR是一个基于Swin Transformer做图像修复的网络结构，由浅层特征提取、深层特征提取、图像高质量重建三部分组成。首先浅层特征提取部分是由卷积层组成的，输出结果将直接传输到重建模块中，为了保持图像本身的低频信息，而深层特征提取模块主要由RSTB（Residual Swin Transformer blocks）组成。同时，其在每一个block的后面还加了一层卷积层，来做特征加强，以及使用残差网络来为特征聚合提供捷径。最终，浅层和深层特征被输送到重建模块，进行高质量的图像重建。</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://blog.slks.xyz/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Swin Transformer" scheme="https://blog.slks.xyz/tags/Swin-Transformer/"/>
    
    <category term="Image Restoration" scheme="https://blog.slks.xyz/tags/Image-Restoration/"/>
    
  </entry>
  
  <entry>
    <title>《Uformer A General U-Shaped Transformer for Image Restoration》论文笔记</title>
    <link href="https://blog.slks.xyz/2022/01/13/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94%E3%80%8AUformer%20A%20General%20U-Shaped%20Transformer%20for%20Image%20Restoration%E3%80%8B/"/>
    <id>https://blog.slks.xyz/2022/01/13/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94%E3%80%8AUformer%20A%20General%20U-Shaped%20Transformer%20for%20Image%20Restoration%E3%80%8B/</id>
    <published>2022-01-13T05:19:19.000Z</published>
    <updated>2022-01-28T12:14:00.226Z</updated>
    
    <content type="html"><![CDATA[<h3 id="论文名称uformer-a-general-u-shaped-transformer-for-image-restoration">论文名称：《Uformer: A General U-Shaped Transformer for Image Restoration》</h3><h3 id="论文地址-httpsarxiv.orgabs2106.03106">论文地址： https://arxiv.org/abs/2106.03106</h3><h2 id="关键词">1、关键词：</h2><p>​ 图像修复（Image Restoration）、类UNet结构、Transformer</p><h2 id="领域背景图像修复">2、领域背景—图像修复：</h2><p>​ 图像修复是一个经典问题，一般而言其目标为从低分辨率的图像中恢复出高分辨率的图像。通常可以用于超分辨率、图像去噪，以及JPEG压缩鬼影去除等应用。此篇文章将Swin Transformer应用于图像修复领域中。</p><h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2><p>​ 近年来图像修复很好结果的都是基于卷积的网络，但是卷积网络在long-range dependencies上依旧存在局限性。同时，最近也有很多文章使用transofmer对低分辨率的特征图进行处理（限于self-attention的计算复杂度）。</p><h2 id="主要设计思想">4、主要设计思想：</h2><p>​ 论文旨在 在多尺度分辨率下，去恢复更多的图像细节。Uformer基于UNet，只不过将所有的卷积层替换为了Encoder-Decoder结构，同时保留了整体的Encoder-Decoder架构以及skip-connections。总体来说就是使用TransformerBlock 构建了一个层次化的encoder-decoder网络。</p><p>​ <strong>2个核心设计</strong></p><p>​ 1、<strong>LeWin Transformer Block</strong>( locally-enhanced window Transformer block ),使用分块的self-attetiond代替全局的self-attention，在捕获高分辨率图像的局部特征时，减少了计算量。</p><p>​ 2、提出了<strong>可学习的多尺度恢复调制器</strong>，以多尺度 <strong>spatial bias</strong> 的形式去在decoder的不同层上调节特征。来处理不同的图像退化问题（比如说虚焦、运动模糊等。这个模块本身由一个多尺度的spatial bias组成，用来在decoder的不同层级上调整特征。更具体一点的话，就是一个可学习的基于窗口的tensor张量，和特征直接相加，从而来调整特征能够恢复更多的细节。整体而言，这个调制器对于恢复图像细节而言有非常强的能力，同时只带来了一点点额外的参数和计算代价。</p><h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220112112308268.png" alt="image-20220112112308268" /><figcaption aria-hidden="true">image-20220112112308268</figcaption></figure><h3 id="整体架构overall-pipeline">1) 整体架构（Overall Pipeline）</h3><ul><li><strong>概述</strong>：U型带Skip-Connection的Encoder-Decoder网络，基于UNet结构，但是将UNet中的所有卷积层都替换成了LeWinBlocks。</li><li><strong>输入</strong>：退化图像 <span class="math inline">\(I \in R^{3\times H\times W}\)</span></li><li><strong>输出</strong>：修复图像 <span class="math inline">\(I^{&#39;} \in R^{3\times H\times W}\)</span><ul><li><span class="math inline">\(Residual \in R^{3\times H\times W }\)</span></li><li><span class="math inline">\(I^{&#39;} = I + R e sidual\)</span></li></ul></li></ul><h3 id="input-projection">2) Input Projection</h3><ul><li><strong>概述</strong>：用于从退化图像中提取低层级信息</li><li><strong>输入</strong>：退化图像 <span class="math inline">\(I \in R^{3\times H\times W}\)</span></li><li><strong>输出</strong>：low-level 特征图 <span class="math inline">\(X_0\)</span></li><li><strong>网络结构</strong>：<ul><li><span class="math inline">\(3\times 3\)</span> 卷积层 + LeakyReLU激活函数</li></ul></li></ul><h3 id="encoder---one-stage-encoder中总共k个stage">3) Encoder - One Stage （ Encoder中总共K个Stage ）</h3><ul><li><strong>概述</strong>：用于从退化图像中提取低层级信息</li><li><strong>输入</strong>：特征图 <span class="math inline">\(X_{l-1}\)</span></li><li><strong>输出</strong>：第<span class="math inline">\(l\)</span>个阶段的输出特征图 <span class="math inline">\(X_l \in R^{2^lC \times \frac{H}{2^l} \times \frac{W}{2^l}}\)</span></li><li><strong>网络结构</strong>：<ul><li><span class="math inline">\({N_l}\)</span> 个 LeWinBlocks的叠加，以及一个下采样层（$4  $ 卷积 <span class="math inline">\(stride\)</span> 2）</li></ul></li><li><strong>实现细节（官方代码）</strong><ul><li>LeWinBlocks<ul><li>将2D空间特征图先进行patch划分，然后展平，具体见后</li></ul></li><li>Down Sampling：（channel 翻倍，特征图宽高 减半）<ul><li>Reshape 展平的特征 至 2D的空间特征图，然后使用 $4  $ 卷积 <span class="math inline">\(stride\)</span> 2，对其进行卷积</li></ul></li></ul></li><li><strong>意义</strong>：在一遍遍的下采样与Lewin Transformer Blocks中提取特征信息，Lewin Transformer Block可以更好的提取远距离的相关性依赖信息，并且有效的降低计算复杂度</li></ul><h3 id="bottleneck-stage">4) BottleNeck Stage</h3><ul><li><strong>概述</strong>：用于提取更长距离的信息，甚至是全局的信息</li><li><strong>输入</strong>：特征图 <span class="math inline">\(X_{K}\)</span></li><li><strong>输出</strong>：序列向量 V</li><li><strong>网络结构</strong>：<ul><li><span class="math inline">\({N_?}\)</span> 个 LeWinBlocks的叠加</li></ul></li><li><strong>实现细节（官方代码）</strong><ul><li>LeWinBlocks<ul><li>将2D空间特征图先进行patch划分，然后展平，具体见后</li></ul></li></ul></li><li><strong>意义</strong>：由于前面已经进行了许多次的特征提取以及下采样了，所以在此处再增加一个LeWinBlocks的模块，能够捕获到更长距离的信息，甚至是能够捕获全局的信息。</li></ul><p><!-- 但是这个模块难道不可以用Encoder阶段更大的K 来弥补吗？需要实验 --></p><h3 id="decoder---one-stage-decoder中总共k个stage与encoder一致">5）Decoder - One Stage （Decoder中总共K个Stage，与Encoder一致 ）</h3><ul><li><strong>概述</strong>：用于小尺寸的特征图中逐渐重建恢复特征信息</li><li><strong>输入</strong>：1维特征序列 <span class="math inline">\(V_{in}\)</span></li><li><strong>输出</strong>：1维特征输出序列 <span class="math inline">\(V_{out}\)</span></li><li><strong>网络结构</strong>：<ul><li>一个上采样层（$2  $ 反卷积 <span class="math inline">\(stride\)</span> 2）以及 <span class="math inline">\({N_l}\)</span> 个 LeWinBlocks的叠加</li></ul></li><li><strong>实现细节（官方代码）</strong><ul><li>Up Sampling：（channel 减半，特征图宽高 翻倍）<ul><li>先将输入的特征序列 Reshape 成 2D特征图，然后进行上采样，得到新的2D特征图</li></ul></li><li>Residual Module：<ul><li>Up Sampling 完以后得到的2D特征图需要和对应Encoder-Stage中 输出的2D特征图进行Concat，然后得到一个新的2D特征图。</li></ul></li><li>LeWinBlocks<ul><li>将残差Concat得到的2D空间特征图先进行patch划分，然后展平，具体见后</li></ul></li></ul></li><li><strong>意义</strong>：在一遍遍的上采样与Lewin Transformer Blocks中重建特征信息。</li></ul><h3 id="output-projection">6) Output Projection</h3><ul><li><strong>概述</strong>：用于将输出的序列，变换成2D特征图后，再 映射至 3通道的残差图像，准备和原图像叠加。</li><li><strong>输入</strong>：1维特征输出序列 <span class="math inline">\(V_{out}\)</span></li><li><strong>输出</strong>：残差图像 <span class="math inline">\(Residual \in R^{3\times H\times W }\)</span></li><li><strong>网络结构</strong>：<ul><li><span class="math inline">\(3\times 3\)</span> 卷积层</li></ul></li><li><strong>实现细节（官方代码）</strong>：<ul><li>先将 最后一个Decoder Stage输出的 序列 <span class="math inline">\(V_{out}\)</span> Reshape成2D的特征图，然后应用<span class="math inline">\(3 \times 3\)</span>的卷积层，来获得一个残差图像</li></ul></li></ul><h3 id="最终输出-loss函数">7）最终输出 &amp; Loss函数</h3><ul><li><strong>概述</strong>：将OutPut Projection 得到的Residual Image 和 原来的输入图像相加，得到 最终的修复图像。</li><li><strong>Loss</strong>：使用 <strong>Charbonnier Loss</strong>，<span class="math inline">\(I^{&#39;}\)</span> 是 ground-truth 图像<ul><li><span class="math display">\[l(I^{&#39;},\hat I) = \sqrt{||I^{&#39;} - \hat I||^2 + \varepsilon^2}\]</span></li></ul></li></ul><h3 id="lewin-transformer-block">8）LeWin Transformer Block</h3><ul><li><p><strong>概述</strong>：由W-MSA 以及LeFF两个模块组成</p></li><li><p><strong>输入</strong>：2D特征图</p></li><li><p><strong>输出</strong>：2D特征图</p></li><li><p><strong>网络结构</strong>：公式表达：</p><ul><li><span class="math inline">\(X_{l}^{&#39;} = W-MSA(LN(X_{l-1})) + X_{l-1}\)</span> <strong>W-MSA 模块的输出</strong></li><li><span class="math inline">\(X_l = LeFF(LN(X_l^{&#39;})) + X_l^{&#39;}\)</span> **LeFF模块的输出*</li></ul></li><li><p><strong>实现细节（官方代码）</strong>：</p><ul><li><p>W-MSA 与 Vision Transformer中一致</p></li><li><p>作者论文中说尝试了移动窗口，但是带来的结果好坏增长微乎其微，说明：在图像修复领域，窗口与窗口之间的信息交互并不是很重要。图像修复领域比较注重局部的信息交互？</p></li><li><p>LeFF模块结构：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/截屏2022-01-12%20下午3.31.12.png" alt="截屏2022-01-12 下午3.31.12" /><figcaption aria-hidden="true">截屏2022-01-12 下午3.31.12</figcaption></figure><ul><li>1、对于每个Token，应用一个Linear Projection，增加特征channel</li><li>2、将tokens Reshape 成 2D 特征图，用 <span class="math inline">\(3 \times 3\)</span> depth-wise convolutional 捕获局部信息</li><li>3、再将2D特征图展平回tokens，再应用一个Linear Projection，缩减特征channel</li><li>在每一个Linear Projection / Convolution 后都会应用 GELU激活函数。</li></ul></li></ul></li><li><p><strong>意义</strong>：解决了如下两个问题 1）全局计算self-attention复杂度太高 2）在捕捉局部的依赖关系时有限制。</p></li></ul><h3 id="multi-scale-restoration-modulator">9）Multi-Scale Restoration Modulator</h3><ul><li><strong>概述</strong>：因为不同的图像有不同的混乱残差形式，比如模糊、噪音、下雨等等，为了更好的应对各种不同的修复任务，提出了这个light-weight multi-scale的恢复模块。来calibrate特征以及更好的修复细节。<br /></li><li><strong>网络结构</strong>：<ul><li>在每一个LeWin Transformer Block中，其为一个<span class="math inline">\(M \times M \times C\)</span>大小的张量，M为window_size，C为特征图通道数。</li><li>在一个LeWin Transformer Block中，对于所有其分割出来的windows，这个 调制器模块的参数都是共享的。</li></ul></li><li><strong>形式</strong>：在Self-Attention计算前，将其直接加到每一个窗口的像素值上。</li><li><strong>意义</strong>：在Image Deblurring 和 Image Denoising里面，作者证实了该模块的重要性，能够更好的修复细节。一种可能的解释是添加 在解码器的每个阶段的modulator可以对特征图进行更为灵活的调整，从而提高恢复的细节的性能表现。这个跟先前的StyleGAN的某个模块一致。</li></ul>]]></content>
    
    
    <summary type="html">论文旨在在多尺度分辨率下，去恢复更多的图像细节。Uformer基于UNet，只不过将所有的卷积层替换为了Encoder-Decoder结构，同时保留了整体的Encoder-Decoder架构以及skip-connections。总体来说就是使用TransformerBlock 构建了一个层次化的Encoder-Decoder网络。</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://blog.slks.xyz/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Swin Transformer" scheme="https://blog.slks.xyz/tags/Swin-Transformer/"/>
    
    <category term="Image Restoration" scheme="https://blog.slks.xyz/tags/Image-Restoration/"/>
    
    <category term="UNet" scheme="https://blog.slks.xyz/tags/UNet/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础系列笔记6——全卷积网络FCN &amp; U-Net结构</title>
    <link href="https://blog.slks.xyz/2022/01/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B06%E2%80%94%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9CFCN&amp;U-Net%E7%BB%93%E6%9E%84/"/>
    <id>https://blog.slks.xyz/2022/01/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B06%E2%80%94%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9CFCN&amp;U-Net%E7%BB%93%E6%9E%84/</id>
    <published>2022-01-12T02:18:19.000Z</published>
    <updated>2022-01-28T12:20:17.094Z</updated>
    
    <content type="html"><![CDATA[<p>本文讲解FCN与U-Net相关的知识，在此之前你需要了解CNN是什么</p><h3 id="一fcn全卷积网络">一、FCN全卷积网络</h3><p>​ 首先，我们还是要回顾一下CNN的整体网络架构与优势：CNN网络最后输出的是类别的概率值。CNN 的强大之处在于它的多层卷积结构能自动学习特征，并且可以学习到多个层次的特征：</p><p>​ 较浅的卷积层感知域较小，学习到一些局部区域的特征。</p><p>​ 而较深的卷积层具有较大的感知域，能够学习到更加抽象一些的特征。这些抽象特征对物体的大小、位置和方向等敏感性更低，从而有助于识别性能的提高。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114212245766.png" alt="image-20220114212245766" /><figcaption aria-hidden="true">image-20220114212245766</figcaption></figure><p>​ 而FCN相较于CNN来说，其将CNN最后几个用于输出概率的全连接层都改成了卷积层，从而使得模型网络中所有的层都是卷积层，最终输出一张已经label好的图像，故称为全卷积网络。全卷积神经网络主要使用了三种技术：</p><p>​ 1、卷积化（Convolutional）</p><p>​ 2、上采样（Upsample）</p><p>​ 3、跳跃结构（Skip Layer）</p><p>​ 整个FCN网络基本原理如图5（只是原理示意图）：</p><p>​ 1、image经过多个卷积和+一个max pooling变为pool1 feature，宽高变为1/2</p><p>​ 2、pool1 feature再经过多个conv+一个max pooling变为pool2 feature，宽高变为1/4</p><p>​ 3、pool2 feature再经过多个conv+一个max pooling变为pool3 feature，宽高变为1/8</p><p>​ 4、......</p><p>​ 5、直到pool5 feature，宽高变为1/32。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/dsawqeasd.png" alt="dsawqeasd" /><figcaption aria-hidden="true">dsawqeasd</figcaption></figure><p>那么，对于三种不同规格参数的FCN，后续还原操作也不太一样，如下所示：</p><p>​ 1、对于FCN-32s，直接对pool5 feature进行32倍上采样获得32x upsampled feature，再对32x upsampled feature每个点做softmax prediction获得32x upsampled feature prediction（即语义分割图）。</p><p>​ 2、对于FCN-16s，首先对pool5 feature进行2倍上采样获得2x upsampled feature，再把pool4 feature和2x upsampled feature逐点相加，然后对相加的feature进行16倍上采样，并softmax prediction，获得16x upsampled feature prediction。</p><p>​ 3、对于FCN-8s，首先进行pool4+2x upsampled feature逐点相加，然后又进行pool3+2x upsampled逐点相加，即进行更多次特征融合。具体过程与16s类似，不再赘述。</p><p>​ 在上述处理过程中，我们发现FCN-16s和FCN-8s都引入了skip connection，将pool3或是pool4 feature与pool5上采样后的feature逐像素相加，进行多次特征融合，这样处理的原因在于：</p><p>​ FCN模型虽然通过卷积和反卷积我们基本能定位到目标区域，但是，我们会发现模型前期是通过卷积、池化、非线性激活函数等作用输出了特征权重图像，我们经过反卷积等操作输出的<strong>图像实际是很粗糙的</strong>，毕竟丢了很多细节。因此我们需要找到一种方式填补丢失的细节数据，所以就有了跳跃结构。</p><p>​ 作者在原文种给出3种网络结果对比，明显可以看出效果：FCN-32s &lt; FCN-16s &lt; FCN-8s，即使用多层feature融合有利于提高分割准确性。</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114212421618.png" alt="image-20220114212421618" /><figcaption aria-hidden="true">image-20220114212421618</figcaption></figure><h4 id="fcn优点">FCN优点：</h4><p>​ 与传统用CNN进行图像分割的方法相比，FCN有两大明显的优点：一是可以接受任意大小的输入图像，而不用要求所有的训练图像和测试图像具有同样的尺寸。二是更加高效，因为避免了由于使用像素块而带来的重复存储和计算卷积的问题。</p><h4 id="fcn缺点">FCN缺点：</h4><p>1、分割的结果不够精细。图像过于模糊或平滑，没有分割出目标图像的细节</p><p>2、因为模型是基于CNN改进而来，即便是用卷积替换了全连接，但是依然是独立像素进行分类，没有充分考虑像素与像素之间的关系</p><h2 id="二关于卷积网络中降采样与上采样以及特征提取阶段的理解">二、关于卷积网络中降采样与上采样以及特征提取阶段的理解：</h2><p>​ 1、降采样的理论意义是，它可以增加对输入图像的一些小扰动的鲁棒性，比如图像平移，旋转等，减少过拟合的风险，降低运算量，增加感受野的大小。</p><p>​ 2、上采样的最大的作用其实就是把抽象的特征再还原解码到原图的尺寸，最终得到分割结果。但很容易得到模糊或过于平滑的结果，无法还原细节部分。</p><p>​ 3、对于特征提取阶段，浅层结构可以抓取图像的一些简单的特征，比如边界，颜色，而深层结构因为感受野大了，而且经过的卷积操作多了，能抓取到图像的一些抽象特征。</p><h3 id="三unet网络">三、UNet网络</h3><p>​ UNet网络整体结构如下：</p><figure><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114212533302.png" alt="image-20220114212533302" /><figcaption aria-hidden="true">image-20220114212533302</figcaption></figure><p>​ 蓝色箭头代表3*3的卷积层+ReLU激活函数层</p><p>​ 红色箭头代表2*2的最大池化层</p><p>​ 绿色箭头代表2*2的上采样层（通常采用反卷积）</p><p>​ 浅蓝色箭头代表1*1的卷积层，在最后用于调整输出通道数目</p><p>​ 在整一个过程中，值得注意的是灰色箭头，我们会注意到灰色箭头代表的是“copy and crop”，即复制和剪切。我们以第一层的灰色箭头举例来看：其左侧大小为64 * 568 * 568，右侧大小为128 * 392 * 392。其中，右侧有64个通道的数据来源于左侧，64个通道的数据来源于上一层的上采样。那么灰色箭头应该就是把左侧的内容复制到了右侧，并且concat在了原先上采样后得到的64 * 392 * 392的数据上，形成了128 * 392 * 392的数据。</p><p>​ 但是问题在于，568 * 568和392 * 392还有较大的尺寸差别，这就需要利用crop来完成，由于在每个卷积中都会丢失边界像素，因此裁剪crop是必要的</p><p>​ 在上述网络形式中，最重要的结构就是其中的<strong>skip-connection</strong>。UNet中<strong>Concat</strong>形式的skip-connection的好处是，<strong>对于分割这个任务，空间域信息非常重要</strong>。而网络的encoder部分，通过各个pooling层已经把特征图分辨率降得非常小了，这一点不利于精确的分割mask生成，通过skip-connection可以把较浅的卷积层特征引过来，那些特征分辨率较高，且层数浅，会含有比较丰富的low-level信息，更利于生成分割mask。</p><p>​ 总体来说，就是把对应尺度上的特征信息引入到上采样或反卷积过程，为后期图像分割提供多尺度多层次的信息，由此可以得到更精细的分割效果，如U-Net论文描述的分割结果一样。这比单纯用编解码器框架要好，纯粹的编解码器框架，在编码过程中压缩和丢失了大量细节信息，而这些信息很可能会有助于后期的图像分割。</p><p>​ 同时，需要注意的一点是：此处的skip-connection与ResNet中直接相加形式的skip-connection不同，ResNet中的跳跃连接可以有效的减少梯度消失和网络退化问题，使训练更容易。直观上理解可以认为BP的时候，深层的梯度可以更容易的传回浅层，因为这种结构的存在，对神经网络层数的设定可以更随意一些。</p>]]></content>
    
    
    <summary type="html">FCN相较于CNN来说，其将CNN最后几个用于输出概率的全连接层都改成了卷积层，从而使得模型网络中所有的层都是卷积层，最终输出一张已经label好的图像，故称为全卷积网络。</summary>
    
    
    
    <category term="机器学习基础系列笔记" scheme="https://blog.slks.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Machine Learning" scheme="https://blog.slks.xyz/tags/Machine-Learning/"/>
    
    <category term="UNet" scheme="https://blog.slks.xyz/tags/UNet/"/>
    
    <category term="FCN" scheme="https://blog.slks.xyz/tags/FCN/"/>
    
  </entry>
  
</feed>
