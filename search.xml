<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>java系列笔记13——Maven基础介绍（更新中）</title>
    <url>/2022/02/04/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B013%E2%80%94Maven%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1255945359327200</p>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记12——java多线程（更新中）</title>
    <url>/2022/02/03/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B012%E2%80%94java%E5%A4%9A%E7%BA%BF%E7%A8%8B/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1304521607217185</p>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记11——java正则表达式</title>
    <url>/2022/02/02/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B011%E2%80%94java%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1304066130968610</p>
<h3 id="一什么是正则表达式">一、什么是正则表达式：</h3>
<p>​ 正则表达式可以用字符串来描述规则，并用来匹配字符串。例如，判断手机号，我们用正则表达式<code>\d&#123;11&#125;</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">isValidMobileNumber</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> s.matches(<span class="string">&quot;\\d&#123;11&#125;&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 使用正则表达式的好处有哪些？一个正则表达式就是一个描述规则的字符串，所以，只需要编写正确的规则，我们就可以让正则表达式引擎去判断目标字符串是否符合规则。</p>
<p>​ 正则表达式是用字符串描述的一个匹配规则，使用正则表达式可以快速判断给定的字符串是否符合匹配规则。<strong>Java标准库<code>java.util.regex</code>内建了正则表达式引擎。</strong></p>
<h3 id="二匹配规则">二、匹配规则：</h3>
<p>正则表达式的匹配规则是从左到右按规则匹配。</p>
<h4 id="精确匹配">1、精确匹配：</h4>
<p>​ 对于正则表达式<code>abc</code>来说，它只能精确地匹配字符串<code>"abc"</code>，不能匹配<code>"ab"</code>，<code>"Abc"</code>，<code>"abcd"</code>等其他任何字符串。</p>
<p>​ 如果正则表达式有特殊字符，那就需要用<code>\</code>转义。例如，正则表达式<code>a\&amp;c</code>，其中<code>\&amp;</code>是用来匹配特殊字符<code>&amp;</code>的，它能精确匹配字符串<code>"a&amp;c"</code>，但不能匹配<code>"ac"</code>、<code>"a-c"</code>、<code>"a&amp;&amp;c"</code>等。</p>
<p>​ 如果想匹配非ASCII字符，例如中文，那就用<code>\u####</code>的十六进制表示，例如：<code>a\u548cc</code>匹配字符串<code>"a和c"</code>，中文字符<code>和</code>的Unicode编码是<code>548c</code>。</p>
<h4 id="模糊匹配.">2、模糊匹配：(.)</h4>
<p>​ 正则表达式<code>a.c</code>中间的<code>.</code>可以匹配一个任意字符</p>
<p>​ <code>.</code>匹配一个字符且仅限一个字符</p>
<h4 id="匹配数字">3、匹配数字：（</h4>
<p>​ 如果我们只想匹配<code>0</code>~<code>9</code>这样的数字，可以用<code>\d</code>匹配。例如，正则表达式<code>00\d</code>可以匹配：</p>
<ul>
<li><code>"007"</code>，因为<code>\d</code>可以匹配字符<code>7</code>；</li>
<li><code>"008"</code>，因为<code>\d</code>可以匹配字符<code>8</code>。</li>
</ul>
<p>​ 它不能匹配<code>"00A"</code>，<code>"0077"</code>，因为<code>\d</code>仅限单个数字字符。</p>
<h4 id="匹配常用字符">4、匹配常用字符：()</h4>
<p>用<code>\w</code>可以匹配一个字母、数字或下划线，w的意思是word。例如，<code>java\w</code>可以匹配：</p>
<ul>
<li><code>"javac"</code>，因为<code>\w</code>可以匹配英文字符<code>c</code>；</li>
<li><code>"java9"</code>，因为<code>\w</code>可以匹配数字字符<code>9</code>；。</li>
<li><code>"java_"</code>，因为<code>\w</code>可以匹配下划线<code>_</code>。</li>
</ul>
<p>因为<code>\w</code>不能匹配<code>#</code>、空格等字符。</p>
<h4 id="匹配空格字符">5、 匹配空格字符()</h4>
<p>用<code>\s</code>可以匹配一个空格字符，注意空格字符不但包括空格`<code>，还包括tab字符（在Java中用</code>表示）。例如，<code>a\sc</code>可以匹配：</p>
<ul>
<li><code>"a c"</code>，因为<code>\s</code>可以匹配空格字符``；</li>
<li><code>"a c"</code>，因为<code>\s</code>可以匹配tab字符<code>\t</code>。</li>
</ul>
<p>它不能匹配<code>"ac"</code>，<code>"abc"</code>等。</p>
<h4 id="匹配非数字">6、匹配非数字：（）</h4>
<p>​ <code>\D</code>匹配一个非数字</p>
<p>​ 类似的，<code>\W</code>可以匹配<code>\w</code>不能匹配的字符，<code>\S</code>可以匹配<code>\s</code>不能匹配的字符，这几个正好是反着来的。</p>
<h4 id="重复匹配">7、重复匹配：</h4>
<ul>
<li><p>修饰符<code>*</code>可以匹配任意个字符，包括0个字符。我们用<code>A\d*</code>可以匹配：</p>
<ul>
<li><p><code>A</code>：因为<code>\d*</code>可以匹配0个数字；</p></li>
<li><p><code>A0</code>：因为<code>\d*</code>可以匹配1个数字<code>0</code>；</p></li>
<li><p><code>A380</code>：因为<code>\d*</code>可以匹配多个数字<code>380</code>。</p></li>
</ul></li>
<li><p>修饰符<code>+</code>可以匹配至少一个字符。我们用<code>A\d+</code>可以匹配：</p>
<ul>
<li><code>A0</code>：因为<code>\d+</code>可以匹配1个数字<code>0</code>；</li>
<li><code>A380</code>：因为<code>\d+</code>可以匹配多个数字<code>380</code>。</li>
</ul></li>
<li><p>修饰符<code>?</code>可以匹配0个或一个字符。我们用<code>A\d?</code>可以匹配：</p>
<ul>
<li><code>A</code>：因为<code>\d?</code>可以匹配0个数字；</li>
<li><code>A0</code>：因为<code>\d?</code>可以匹配1个数字<code>0</code>。</li>
</ul></li>
<li><p>用修饰符<code>&#123;n&#125;</code>精确指定n个字符,<code>A\d&#123;3&#125;</code>可以精确匹配：</p>
<ul>
<li><code>A380</code>：因为<code>\d&#123;3&#125;</code>可以匹配3个数字<code>380</code>。</li>
</ul></li>
<li><p>用修饰符<code>&#123;n,m&#125;</code>指定匹配n~m个字符, <code>A\d&#123;3,5&#125;</code>可以精确匹配：</p>
<ul>
<li><code>A380</code>：因为<code>\d&#123;3,5&#125;</code>可以匹配3个数字<code>380</code>；</li>
<li><code>A3800</code>：因为<code>\d&#123;3,5&#125;</code>可以匹配4个数字<code>3800</code>；</li>
<li><code>A38000</code>：因为<code>\d&#123;3,5&#125;</code>可以匹配5个数字<code>38000</code>。</li>
</ul></li>
<li><p>如果没有上限，那么修饰符<code>&#123;n,&#125;</code>就可以匹配至少n个字符。</p></li>
</ul>
<h4 id="匹配规则快速查找">8、匹配规则快速查找：</h4>
<p>单个字符的匹配规则如下：</p>
<table>
<thead>
<tr class="header">
<th>正则表达式</th>
<th>规则</th>
<th>可以匹配</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>A</code></td>
<td>指定字符</td>
<td><code>A</code></td>
</tr>
<tr class="even">
<td><code>\u548c</code></td>
<td>指定Unicode字符</td>
<td><code>和</code></td>
</tr>
<tr class="odd">
<td><code>.</code></td>
<td>任意字符</td>
<td><code>a</code>，<code>b</code>，<code>&amp;</code>，<code>0</code></td>
</tr>
<tr class="even">
<td><code>\d</code></td>
<td>数字0~9</td>
<td><code>0</code>~<code>9</code></td>
</tr>
<tr class="odd">
<td><code>\w</code></td>
<td>大小写字母，数字和下划线</td>
<td><code>a</code><sub><code>z</code>，<code>A</code></sub><code>Z</code>，<code>0</code>~<code>9</code>，<code>_</code></td>
</tr>
<tr class="even">
<td><code>\s</code></td>
<td>空格、Tab键</td>
<td>空格，Tab</td>
</tr>
<tr class="odd">
<td><code>\D</code></td>
<td>非数字</td>
<td><code>a</code>，<code>A</code>，<code>&amp;</code>，<code>_</code>，……</td>
</tr>
<tr class="even">
<td><code>\W</code></td>
<td>非</td>
<td><code>&amp;</code>，<code>@</code>，<code>中</code>，……</td>
</tr>
<tr class="odd">
<td><code>\S</code></td>
<td>非</td>
<td><code>a</code>，<code>A</code>，<code>&amp;</code>，<code>_</code>，……</td>
</tr>
</tbody>
</table>
<p>多个字符的匹配规则如下：</p>
<table>
<thead>
<tr class="header">
<th>正则表达式</th>
<th>规则</th>
<th>可以匹配</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>A*</code></td>
<td>任意个数字符</td>
<td>空，<code>A</code>，<code>AA</code>，<code>AAA</code>，……</td>
</tr>
<tr class="even">
<td><code>A+</code></td>
<td>至少1个字符</td>
<td><code>A</code>，<code>AA</code>，<code>AAA</code>，……</td>
</tr>
<tr class="odd">
<td><code>A?</code></td>
<td>0个或1个字符</td>
<td>空，<code>A</code></td>
</tr>
<tr class="even">
<td><code>A&#123;3&#125;</code></td>
<td>指定个数字符</td>
<td><code>AAA</code></td>
</tr>
<tr class="odd">
<td><code>A&#123;2,3&#125;</code></td>
<td>指定范围个数字符</td>
<td><code>AA</code>，<code>AAA</code></td>
</tr>
<tr class="even">
<td><code>A&#123;2,&#125;</code></td>
<td>至少n个字符</td>
<td><code>AA</code>，<code>AAA</code>，<code>AAAA</code>，……</td>
</tr>
<tr class="odd">
<td><code>A&#123;0,3&#125;</code></td>
<td>最多n个字符</td>
<td>空，<code>A</code>，<code>AA</code>，<code>AAA</code></td>
</tr>
</tbody>
</table>
<h3 id="三复杂匹配规则">三、复杂匹配规则：</h3>
<h4 id="匹配开头结尾">1、匹配开头结尾：</h4>
<p>​ 进行多行匹配时，用<code>^</code>表示开头，<code>$</code>表示结尾。例如，<code>^A\d&#123;3&#125;$</code>，可以匹配<code>"A001"</code>、<code>"A380"</code>。</p>
<h4 id="匹配指定范围">2、匹配指定范围：</h4>
<p>​ <strong>使用<code>[...]</code>可以匹配范围内的字符</strong>，例如，<code>[123456789]</code>可以匹配<code>1</code>~<code>9</code>，这样就可以写出上述电话号码的规则：<code>[123456789]\d&#123;6,7&#125;</code>。</p>
<p>​ 把所有字符全列出来太麻烦，<code>[...]</code>还有一种写法，直接写<code>[1-9]</code>就可以。</p>
<p>​ <strong>要匹配大小写不限的十六进制数，比如<code>1A2b3c</code>，我们可以这样写：<code>[0-9a-fA-F]</code>，它表示一共可以匹配以下任意范围的字符：</strong></p>
<ul>
<li><code>0-9</code>：字符<code>0</code>~<code>9</code>；</li>
<li><code>a-f</code>：字符<code>a</code>~<code>f</code>；</li>
<li><code>A-F</code>：字符<code>A</code>~<code>F</code>。</li>
</ul>
<p>​ <strong><code>[...]</code>还有一种排除法，即不包含指定范围的字符。假设我们要匹配任意字符，但不包括数字，可以写<code>[^1-9]&#123;3&#125;</code>：</strong></p>
<ul>
<li>可以匹配<code>"ABC"</code>，因为不包含字符<code>1</code>~<code>9</code>；</li>
<li>可以匹配<code>"A00"</code>，因为不包含字符<code>1</code>~<code>9</code>；</li>
<li>不能匹配<code>"A01"</code>，因为包含字符<code>1</code>；</li>
<li>不能匹配<code>"A05"</code>，因为包含字符<code>5</code>。</li>
</ul>
<h4 id="或规则匹配">3、或规则匹配：</h4>
<p>​ 用<code>|</code>连接的两个正则规则是<em>或</em>规则，例如，<code>AB|CD</code>表示可以匹配<code>AB</code>或<code>CD</code>。</p>
<h4 id="使用括号">4、使用括号:</h4>
<p>​ 现在我们想要匹配字符串<code>learn java</code>、<code>learn php</code>和<code>learn go</code>怎么办？一个最简单的规则是<code>learn\sjava|learn\sphp|learn\sgo</code>，但是这个规则太复杂了，可以把公共部分提出来，然后用<code>(...)</code>把子规则括起来表示成<code>learn\\s(java|php|go)</code>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String re = <span class="string">&quot;learn\\s(java|php|go)&quot;</span>;</span><br></pre></td></tr></table></figure>
<h4 id="规则总结">5、规则总结：</h4>
<p>复杂匹配规则主要有：</p>
<table>
<thead>
<tr class="header">
<th>正则表达式</th>
<th>规则</th>
<th>可以匹配</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>^</td>
<td>开头</td>
<td>字符串开头</td>
</tr>
<tr class="even">
<td>$</td>
<td>结尾</td>
<td>字符串结束</td>
</tr>
<tr class="odd">
<td>[ABC]</td>
<td>[…]内任意字符</td>
<td>A，B，C</td>
</tr>
<tr class="even">
<td>[A-F0-9xy]</td>
<td>指定范围的字符</td>
<td><code>A</code>，……，<code>F</code>，<code>0</code>，……，<code>9</code>，<code>x</code>，<code>y</code></td>
</tr>
<tr class="odd">
<td>[^A-F]</td>
<td>指定范围外的任意字符</td>
<td>非<code>A</code>~<code>F</code></td>
</tr>
<tr class="even">
<td>AB|CD|EF</td>
<td>AB或CD或EF</td>
<td><code>AB</code>，<code>CD</code>，<code>EF</code></td>
</tr>
</tbody>
</table>
<h3 id="四分组匹配-提取子串">四、分组匹配 + 提取子串：</h3>
<p>​ <code>(...)</code>还有一个重要作用，就是分组匹配。</p>
<p>​ 我们来看一下如何用正则匹配<code>区号-电话号</code>码这个规则。利用前面讲到的匹配规则，写出来很容易：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\d&#123;3,4&#125;\-\d&#123;6,8&#125;</span><br></pre></td></tr></table></figure>
<p>​ 虽然这个正则匹配规则很简单，但是往往匹配成功后，下一步是提取区号和电话号码，分别存入数据库。于是问题来了：<strong>如何提取匹配的子串</strong>？</p>
<p>​ 当然可以用<code>String</code>提供的<code>indexOf()</code>和<code>substring()</code>这些方法，但它们从正则匹配的字符串中提取子串没有通用性，下一次要提取<code>learn\s(java|php)</code>还得改代码。</p>
<p>​ <strong>正确的方法是用<code>(...)</code>先把要提取的规则分组，把上述正则表达式变为<code>(\d&#123;3,4&#125;)\-(\d&#123;6,8&#125;)</code></strong>。然后引入<code>java.util.regex</code>包，用<code>Pattern</code>对象匹配，匹配后获得一个<code>Matcher</code>对象，如果匹配成功，就可以直接从<code>Matcher.group(index)</code>返回子串：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Pattern p = Pattern.compile(<span class="string">&quot;(\\d&#123;3,4&#125;)\\-(\\d&#123;7,8&#125;)&quot;</span>);</span><br><span class="line">        Matcher m = p.matcher(<span class="string">&quot;010-12345678&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (m.matches()) &#123;</span><br><span class="line">            String g1 = m.group(<span class="number">1</span>);</span><br><span class="line">            String g2 = m.group(<span class="number">2</span>);</span><br><span class="line">            System.out.println(g1);</span><br><span class="line">            System.out.println(g2);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;匹配失败!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行上述代码，会得到两个匹配上的子串<code>010</code>和<code>12345678</code>。</p>
<p>​ 要特别注意，<code>Matcher.group(index)</code>方法的参数用1表示第一个子串，2表示第二个子串。如果我们传入0会得到什么呢？答案是<code>010-12345678</code>，即整个正则匹配到的字符串。</p>
<p>​ 我们在前面的代码中用到的正则表达式代码是<code>String.matches()</code>方法，而我们在分组提取的代码中用的是<code>java.util.regex</code>包里面的<code>Pattern</code>类和<code>Matcher</code>类。实际上这两种代码本质上是一样的，因为<code>String.matches()</code>方法内部调用的就是<code>Pattern</code>和<code>Matcher</code>类的方法。</p>
<p>​ 但是反复使用<code>String.matches()</code>对同一个正则表达式进行多次匹配效率较低，因为每次都会创建出一样的<code>Pattern</code>对象。完全可以先创建出一个<code>Pattern</code>对象，然后反复使用，就可以实现编译一次，多次匹配：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Pattern pattern = Pattern.compile(<span class="string">&quot;(\\d&#123;3,4&#125;)\\-(\\d&#123;7,8&#125;)&quot;</span>);</span><br><span class="line">        pattern.matcher(<span class="string">&quot;010-12345678&quot;</span>).matches(); <span class="comment">// true</span></span><br><span class="line">        pattern.matcher(<span class="string">&quot;021-123456&quot;</span>).matches(); <span class="comment">// false</span></span><br><span class="line">        pattern.matcher(<span class="string">&quot;022#1234567&quot;</span>).matches(); <span class="comment">// false</span></span><br><span class="line">        <span class="comment">// 获得Matcher对象:</span></span><br><span class="line">        Matcher matcher = pattern.matcher(<span class="string">&quot;010-12345678&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (matcher.matches()) &#123;</span><br><span class="line">            String whole = matcher.group(<span class="number">0</span>); <span class="comment">// &quot;010-12345678&quot;, 0表示匹配的整个字符串</span></span><br><span class="line">            String area = matcher.group(<span class="number">1</span>); <span class="comment">// &quot;010&quot;, 1表示匹配的第1个子串</span></span><br><span class="line">            String tel = matcher.group(<span class="number">2</span>); <span class="comment">// &quot;12345678&quot;, 2表示匹配的第2个子串</span></span><br><span class="line">            System.out.println(area);</span><br><span class="line">            System.out.println(tel);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 使用<code>Matcher</code>时，必须首先调用<code>matches()</code>判断是否匹配成功，匹配成功后，才能调用<code>group()</code>提取子串。</p>
<p>​ 利用提取子串的功能，我们轻松获得了区号和号码两部分。</p>
<h3 id="五非贪婪匹配">五、非贪婪匹配：</h3>
<p>先看一个简单的问题：</p>
<p>给定一个字符串表示的数字，判断该数字末尾<code>0</code>的个数。例如：</p>
<ul>
<li><code>"123000"</code>：3个<code>0</code></li>
<li><code>"10100"</code>：2个<code>0</code></li>
<li><code>"1001"</code>：0个<code>0</code></li>
</ul>
<p>可以很容易地写出该正则表达式：<code>(\d+)(0*)</code>，Java代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Pattern pattern = Pattern.compile(<span class="string">&quot;(\\d+)(0*)&quot;</span>);</span><br><span class="line">        Matcher matcher = pattern.matcher(<span class="string">&quot;1230000&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (matcher.matches()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;group1=&quot;</span> + matcher.group(<span class="number">1</span>)); <span class="comment">// &quot;1230000&quot;</span></span><br><span class="line">            System.out.println(<span class="string">&quot;group2=&quot;</span> + matcher.group(<span class="number">2</span>)); <span class="comment">// &quot;&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>然而打印的第二个子串是空字符串<code>""</code>。</p>
<p>实际上，我们期望分组匹配结果是：</p>
<table>
<thead>
<tr class="header">
<th>input</th>
<th><code>\d+</code></th>
<th><code>0*</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>123000</td>
<td>"123"</td>
<td>"000"</td>
</tr>
<tr class="even">
<td>10100</td>
<td>"101"</td>
<td>"00"</td>
</tr>
<tr class="odd">
<td>1001</td>
<td>"1001"</td>
<td>""</td>
</tr>
</tbody>
</table>
<p>但实际的分组匹配结果是这样的：</p>
<table>
<thead>
<tr class="header">
<th>input</th>
<th><code>\d+</code></th>
<th><code>0*</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>123000</td>
<td>"123000"</td>
<td>""</td>
</tr>
<tr class="even">
<td>10100</td>
<td>"10100"</td>
<td>""</td>
</tr>
<tr class="odd">
<td>1001</td>
<td>"1001"</td>
<td>""</td>
</tr>
</tbody>
</table>
<p>​ 仔细观察上述实际匹配结果，实际上它是完全合理的，因为<code>\d+</code>确实可以匹配后面任意个<code>0</code>。</p>
<p>​ <strong>这是因为正则表达式默认使用贪婪匹配：任何一个规则，它总是尽可能多地向后匹配，因此，<code>\d+</code>总是会把后面的<code>0</code>包含进来。</strong></p>
<p>​ <strong>要让<code>\d+</code>尽量少匹配，让<code>0*</code>尽量多匹配，我们就必须让<code>\d+</code>使用非贪婪匹配。在规则<code>\d+</code>后面加个<code>?</code>即可表示非贪婪匹配。我们改写正则表达式如下：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Pattern pattern = Pattern.compile(<span class="string">&quot;(\\d+?)(0*)&quot;</span>);</span><br><span class="line">        Matcher matcher = pattern.matcher(<span class="string">&quot;1230000&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (matcher.matches()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;group1=&quot;</span> + matcher.group(<span class="number">1</span>)); <span class="comment">// &quot;123&quot;</span></span><br><span class="line">            System.out.println(<span class="string">&quot;group2=&quot;</span> + matcher.group(<span class="number">2</span>)); <span class="comment">// &quot;0000&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ <strong>因此，给定一个匹配规则，加上<code>?</code>后就变成了非贪婪匹配。</strong></p>
<p>​ <strong>我们再来看这个正则表达式<code>(\d??)(9*)</code>，注意<code>\d?</code>表示匹配0个或1个数字，后面第二个<code>?</code>表示非贪婪匹配，因此，给定字符串<code>"9999"</code>，匹配到的两个子串分别是<code>""</code>和<code>"9999"</code>，因为对于<code>\d?</code>来说，可以匹配1个<code>9</code>，也可以匹配0个<code>9</code>，但是因为后面的<code>?</code>表示非贪婪匹配，它就会尽可能少的匹配，结果是匹配了0个<code>9</code>。</strong></p>
<h3 id="六搜索和替换">六、搜索和替换：</h3>
<h4 id="分割字符串">1、分割字符串：</h4>
<p>​ 使用正则表达式分割字符串可以实现更加灵活的功能。<code>String.split()</code>方法传入的正是正则表达式。我们来看下面的代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;a b c&quot;.split(&quot;\\s&quot;); // &#123; &quot;a&quot;, &quot;b&quot;, &quot;c&quot; &#125;</span><br><span class="line">&quot;a b  c&quot;.split(&quot;\\s&quot;); // &#123; &quot;a&quot;, &quot;b&quot;, &quot;&quot;, &quot;c&quot; &#125;</span><br><span class="line">&quot;a, b ;; c&quot;.split(&quot;[\\,\\;\\s]+&quot;); // &#123; &quot;a&quot;, &quot;b&quot;, &quot;c&quot; &#125;</span><br></pre></td></tr></table></figure>
<p>​ 如果我们想让用户输入一组标签，然后把标签提取出来，因为用户的输入往往是不规范的，这时，使用合适的正则表达式，就可以消除多个空格、混合<code>,</code>和<code>;</code>这些不规范的输入，直接提取出规范的字符串。</p>
<h4 id="搜索字符串">2、搜索字符串：</h4>
<p>​ 使用正则表达式还可以搜索字符串，我们来看例子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String s = <span class="string">&quot;the quick brown fox jumps over the lazy dog.&quot;</span>;</span><br><span class="line">        Pattern p = Pattern.compile(<span class="string">&quot;\\wo\\w&quot;</span>);</span><br><span class="line">        Matcher m = p.matcher(s);</span><br><span class="line">        <span class="keyword">while</span> (m.find()) &#123;</span><br><span class="line">            String sub = s.substring(m.start(), m.end());</span><br><span class="line">            System.out.println(sub);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 我们获取到<code>Matcher</code>对象后，不需要调用<code>matches()</code>方法（因为匹配整个串肯定返回false），而是反复调用<code>find()</code>方法，在整个串中搜索能匹配上<code>\\wo\\w</code>规则的子串，并打印出来。<strong>这种方式比<code>String.indexOf()</code>要灵活得多，因为我们搜索的规则是3个字符：中间必须是<code>o</code>，前后两个必须是字符<code>[A-Za-z0-9_]</code>。</strong></p>
<h4 id="替换字符串">3、替换字符串：</h4>
<p>​ 使用正则表达式替换字符串可以直接调用<code>String.replaceAll()</code>，它的第一个参数是正则表达式，第二个参数是待替换的字符串。我们还是来看例子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String s = <span class="string">&quot;The     quick\t\t brown   fox  jumps   over the  lazy dog.&quot;</span>;</span><br><span class="line">        String r = s.replaceAll(<span class="string">&quot;\\s+&quot;</span>, <span class="string">&quot; &quot;</span>);</span><br><span class="line">        System.out.println(r); <span class="comment">// &quot;The quick brown fox jumps over the lazy dog.&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记10——java单元测试JUnit</title>
    <url>/2022/02/02/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B010%E2%80%94java%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1304048154181666</p>
<h3 id="一编写junit测试">一、编写JUnit测试：</h3>
<h4 id="什么是测试驱动开发">1、什么是测试驱动开发？</h4>
<p>​ 所谓测试驱动开发，是指先编写接口，紧接着编写测试。编写完测试后，我们才开始真正编写实现代码。在编写实现代码的过程中，一边写，一边测，什么时候测试全部通过了，那就表示编写的实现完成了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    编写接口</span><br><span class="line">     │</span><br><span class="line">     ▼</span><br><span class="line">    编写测试</span><br><span class="line">     │</span><br><span class="line">     ▼</span><br><span class="line">┌─&gt; 编写实现</span><br><span class="line">│    │</span><br><span class="line">│ N  ▼</span><br><span class="line">└── 运行测试</span><br><span class="line">     │ Y</span><br><span class="line">     ▼</span><br><span class="line">    任务完成</span><br></pre></td></tr></table></figure>
<h4 id="junit测试框架">2、JUnit测试框架：</h4>
<p>​ JUnit是一个开源的Java语言的单元测试框架，专门针对Java设计，使用最广泛。JUnit是事实上的单元测试的标准框架，任何Java开发者都应当学习并使用JUnit编写单元测试</p>
<p>​ 使用JUnit编写单元测试的好处在于，我们可以非常简单地组织测试代码，并随时运行它们，JUnit就会给出成功的测试和失败的测试，还可以生成测试报告，不仅包含测试的成功率，还可以统计测试的代码覆盖率，即被测试的代码本身有多少经过了测试。对于高质量的代码来说，测试覆盖率应该在80%以上。</p>
<h4 id="intellij中如何开启单元测试">3、IntelliJ中如何开启单元测试？</h4>
<p>​ 教程：https://blog.csdn.net/qq754772661/article/details/107790362</p>
<h4 id="单元测试文件怎么编写">4、单元测试文件怎么编写？</h4>
<p>​ 核心测试方法<code>testFact()</code>加上了<code>@Test</code>注解，这是JUnit要求的，它会把带有<code>@Test</code>的方法识别为测试方法。在测试方法内部，我们用<code>assertEquals(1, Factorial.fact(1))</code>表示，期望<code>Factorial.fact(1)</code>返回<code>1</code>。<code>assertEquals(expected, actual)</code>是最常用的测试方法，它在<code>Assertion</code>类中定义。<code>Assertion</code>还定义了其他断言方法，例如：</p>
<ul>
<li><code>assertTrue()</code>: 期待结果为<code>true</code></li>
<li><code>assertFalse()</code>: 期待结果为<code>false</code></li>
<li><code>assertNotNull()</code>: 期待结果为非<code>null</code></li>
<li><code>assertArrayEquals()</code>: 期待结果为数组并与期望数组每个元素的值均相等</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.itranswarp.learnjava;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.junit.jupiter.api.Assertions.*;</span><br><span class="line"><span class="keyword">import</span> org.junit.jupiter.api.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FactorialTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">testFact</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        assertEquals(<span class="number">1</span>, Factorial.fact(<span class="number">1</span>));</span><br><span class="line">        assertEquals(<span class="number">2</span>, Factorial.fact(<span class="number">2</span>));</span><br><span class="line">        assertEquals(<span class="number">6</span>, Factorial.fact(<span class="number">3</span>));</span><br><span class="line">        assertEquals(<span class="number">3628800</span>, Factorial.fact(<span class="number">10</span>));</span><br><span class="line">        assertEquals(<span class="number">2432902008176640000L</span>, Factorial.fact(<span class="number">20</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编写完成后，在编译器内直接选择运行该文件即可。</p>
<h4 id="单元测试的结果">5、单元测试的结果：</h4>
<ul>
<li>如果全部通过测试：</li>
</ul>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_02_02_10_13_50_410.png" alt="屏幕捕获_2022_02_02_10_13_50_410" /><figcaption aria-hidden="true">屏幕捕获_2022_02_02_10_13_50_410</figcaption>
</figure>
<ul>
<li>如果有案例通不过测试：</li>
</ul>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_02_02_10_14_11_224.png" alt="屏幕捕获_2022_02_02_10_14_11_224" /><figcaption aria-hidden="true">屏幕捕获_2022_02_02_10_14_11_224</figcaption>
</figure>
<h4 id="单元测试的好处">6、单元测试的好处：</h4>
<p>​ 单元测试可以确保单个方法按照正确预期运行，如果修改了某个方法的代码，只需确保其对应的单元测试通过，即可认为改动正确。此外，测试代码本身就可以作为示例代码，用来演示如何调用该方法。</p>
<p>​ 使用JUnit进行单元测试，我们可以使用断言（<code>Assertion</code>）来测试期望结果，可以方便地组织和运行测试，并方便地查看测试结果。此外，<strong>JUnit既可以直接在IDE中运行，也可以方便地集成到Maven这些自动化工具中运行。</strong></p>
<p>在编写单元测试的时候，我们要遵循一定的规范：</p>
<p>​ 一是单元测试代码本身必须非常简单，能一下看明白，决不能再为测试代码编写测试；</p>
<p>​ 二是每个单元测试应当互相独立，不依赖运行的顺序；</p>
<p>​ <strong>三是测试时不但要覆盖常用测试用例，还要特别注意测试边界条件，例如输入为<code>0</code>，<code>null</code>，空字符串<code>""</code>等情况。</strong></p>
<h3 id="二使用fixture">二、使用Fixture</h3>
<p>​ 在一个单元测试中，我们经常编写多个<code>@Test</code>方法，来分组、分类对目标代码进行测试。在测试的时候，我们经常遇到一个对象需要初始化，测试完可能还需要清理的情况。如果每个<code>@Test</code>方法都写一遍这样的重复代码，显然比较麻烦。</p>
<p>​ <strong>JUnit提供了编写测试前准备、测试后清理的固定代码，我们称之为Fixture。</strong></p>
<ul>
<li>以下为要测试的代码：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Calculator</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> n = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">add</span><span class="params">(<span class="keyword">long</span> x)</span> </span>&#123;</span><br><span class="line">        n = n + x;</span><br><span class="line">        <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">sub</span><span class="params">(<span class="keyword">long</span> x)</span> </span>&#123;</span><br><span class="line">        n = n - x;</span><br><span class="line">        <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 这个类的功能很简单，但是测试的时候，我们要先初始化对象，我们不必在每个测试方法中都写上初始化代码，而是<strong>通过<code>@BeforeEach</code>来初始化，通过<code>@AfterEach</code>来清理资源：</strong>如下所示</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CalculatorTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    Calculator calculator;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@BeforeEach</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.calculator = <span class="keyword">new</span> Calculator();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@AfterEach</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">tearDown</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.calculator = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">testAdd</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        assertEquals(<span class="number">100</span>, <span class="keyword">this</span>.calculator.add(<span class="number">100</span>));</span><br><span class="line">        assertEquals(<span class="number">150</span>, <span class="keyword">this</span>.calculator.add(<span class="number">50</span>));</span><br><span class="line">        assertEquals(<span class="number">130</span>, <span class="keyword">this</span>.calculator.add(-<span class="number">20</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">testSub</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        assertEquals(-<span class="number">100</span>, <span class="keyword">this</span>.calculator.sub(<span class="number">100</span>));</span><br><span class="line">        assertEquals(-<span class="number">150</span>, <span class="keyword">this</span>.calculator.sub(<span class="number">50</span>));</span><br><span class="line">        assertEquals(-<span class="number">130</span>, <span class="keyword">this</span>.calculator.sub(-<span class="number">20</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 还有一些资源初始化和清理可能更加繁琐，而且会耗费较长的时间，例如初始化数据库。**JUnit还提供了<code>@BeforeAll</code>和<code>@AfterAll</code>，它们在运行所有@Test前后运行。</p>
<p>​ 因为<code>@BeforeAll</code>和<code>@AfterAll</code>在所有<code>@Test</code>方法运行前后仅运行一次，因此，它们只能初始化静态变量。</p>
<p><strong>事实上，<code>@BeforeAll</code>和<code>@AfterAll</code>也只能标注在静态方法上。</strong></p>
<p><strong>因此，我们总结出编写Fixture的套路如下：</strong></p>
<ol type="1">
<li>对于实例变量，在<code>@BeforeEach</code>中初始化，在<code>@AfterEach</code>中清理，它们在各个<code>@Test</code>方法中互不影响，因为是不同的实例；</li>
<li>对于静态变量，在<code>@BeforeAll</code>中初始化，在<code>@AfterAll</code>中清理，它们在各个<code>@Test</code>方法中均是唯一实例，会影响各个<code>@Test</code>方法。</li>
</ol>
<p>​ 大多数情况下，使用<code>@BeforeEach</code>和<code>@AfterEach</code>就足够了。只有某些测试资源初始化耗费时间太长，以至于我们不得不尽量“复用”时才会用到<code>@BeforeAll</code>和<code>@AfterAll</code>。</p>
<p>​ <strong>最后，注意到每次运行一个<code>@Test</code>方法前，JUnit首先创建一个<code>XxxTest</code>实例，因此，每个<code>@Test</code>方法内部的成员变量都是独立的，不能也无法把成员变量的状态从一个<code>@Test</code>方法带到另一个<code>@Test</code>方法。</strong></p>
<h3 id="三异常测试">三、异常测试：</h3>
<p>​ 对于可能抛出的异常进行测试，本身就是测试的重要环节。在编写JUnit测试的时候，除了正常的输入输出，我们还要特别针对可能导致异常的情况进行测试。示例如下：</p>
<ul>
<li>被测试代码如下：在方法入口，我们增加了对参数<code>n</code>的检查，如果为负数，则直接抛出<code>IllegalArgumentException</code>。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Factorial</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">fact</span><span class="params">(<span class="keyword">long</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (n &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">long</span> r = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">long</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            r = r * i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> r;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>测试代码：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testNegative</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    assertThrows(IllegalArgumentException.class, () -&gt; &#123;</span><br><span class="line">        Factorial.fact(-<span class="number">1</span>);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="四条件测试">四、条件测试：</h3>
<h4 id="disabled">1、<span class="citation" data-cites="Disabled">@Disabled</span></h4>
<p>​ 在运行测试的时候，有些时候，我们需要排出某些<code>@Test</code>方法，不要让它运行，这时，我们就可以给它标记一个<code>@Disabled</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Disabled</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testBug101</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 这个测试不会运行</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 为什么我们不直接注释掉<code>@Test</code>，而是要加一个<code>@Disabled</code>？这是因为注释掉<code>@Test</code>，JUnit就不知道这是个测试方法，而加上<code>@Disabled</code>，JUnit仍然识别出这是个测试方法，只是暂时不运行。它会在测试结果中显示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Tests run: 68, Failures: 2, Errors: 0, Skipped: 5</span><br></pre></td></tr></table></figure>
<p>​ 类似<code>@Disabled</code>这种注解就称为条件测试，JUnit根据不同的条件注解，决定是否运行当前的<code>@Test</code>方法。</p>
<h4 id="enabledonos">2、<span class="citation" data-cites="EnabledOnOs">@EnabledOnOs</span></h4>
<p>给两个测试方法分别加上条件如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="meta">@EnabledOnOs(OS.WINDOWS)</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testWindows</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    assertEquals(<span class="string">&quot;C:\\test.ini&quot;</span>, config.getConfigFile(<span class="string">&quot;test.ini&quot;</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="meta">@EnabledOnOs(&#123; OS.LINUX, OS.MAC &#125;)</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testLinuxAndMac</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    assertEquals(<span class="string">&quot;/usr/local/test.cfg&quot;</span>, config.getConfigFile(<span class="string">&quot;test.cfg&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>@EnableOnOs</code>就是一个条件测试判断。</p>
<p>在Windows平台执行的测试: <span class="citation" data-cites="EnabledOnOs">@EnabledOnOs</span>(OS.WINDOWS)</p>
<h4 id="其他条件测试">3、其他条件测试：</h4>
<p>只能在Java 9或更高版本执行的测试，可以加上<code>@DisabledOnJre(JRE.JAVA_8)</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="meta">@DisabledOnJre(JRE.JAVA_8)</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testOnJava9OrAbove</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> this test is disabled on java 8</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>只能在64位操作系统上执行的测试，可以用<code>@EnabledIfSystemProperty</code>判断：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="meta">@EnabledIfSystemProperty(named = &quot;os.arch&quot;, matches = &quot;.*64.*&quot;)</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testOnlyOn64bitSystem</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> this test is only run on 64 bit system</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要传入环境变量<code>DEBUG=true</code>才能执行的测试，可以用<code>@EnabledIfEnvironmentVariable</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="meta">@EnabledIfEnvironmentVariable(named = &quot;DEBUG&quot;, matches = &quot;true&quot;)</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testOnlyOnDebugMode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> this test is only run on DEBUG=true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="五参数化测试">五、参数化测试：</h3>
<p>如果待测试的输入和输出是一组数据： 可以把测试数据组织起来 用不同的测试数据调用相同的测试方法</p>
<p>参数化测试和普通测试稍微不同的地方在于，一个测试方法需要接收至少一个参数，然后，传入一组参数反复运行。</p>
<p>JUnit提供了一个<code>@ParameterizedTest</code>注解，用来进行参数化测试。</p>
<p>假设我们想对<code>Math.abs()</code>进行测试，先用一组正数进行测试：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@ParameterizedTest</span></span><br><span class="line"><span class="meta">@ValueSource(ints = &#123; 0, 1, 5, 100 &#125;)</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testAbs</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    assertEquals(x, Math.abs(x));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>再用一组负数进行测试：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@ParameterizedTest</span></span><br><span class="line"><span class="meta">@ValueSource(ints = &#123; -1, -5, -100 &#125;)</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testAbsNegative</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    assertEquals(-x, Math.abs(x));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意到参数化测试的注解是<code>@ParameterizedTest</code>，而不是普通的<code>@Test</code>。</p>
<p>实际的测试场景往往没有这么简单。假设我们自己编写了一个<code>StringUtils.capitalize()</code>方法，它会把字符串的第一个字母变为大写，后续字母变为小写：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringUtils</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">capitalize</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s.length() == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> s;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> Character.toUpperCase(s.charAt(<span class="number">0</span>)) + s.substring(<span class="number">1</span>).toLowerCase();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>要用参数化测试的方法来测试，我们不但要给出输入，还要给出预期输出。因此，测试方法至少需要接收两个参数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@ParameterizedTest</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testCapitalize</span><span class="params">(String input, String result)</span> </span>&#123;</span><br><span class="line">    assertEquals(result, StringUtils.capitalize(input));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在问题来了：参数如何传入？</p>
<p>最简单的方法是通过<code>@MethodSource</code>注解，它允许我们编写一个同名的静态方法来提供测试参数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@ParameterizedTest</span></span><br><span class="line"><span class="meta">@MethodSource</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testCapitalize</span><span class="params">(String input, String result)</span> </span>&#123;</span><br><span class="line">    assertEquals(result, StringUtils.capitalize(input));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> List&lt;Arguments&gt; <span class="title">testCapitalize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> List.of( <span class="comment">// arguments:</span></span><br><span class="line">            Arguments.arguments(<span class="string">&quot;abc&quot;</span>, <span class="string">&quot;Abc&quot;</span>), <span class="comment">//</span></span><br><span class="line">            Arguments.arguments(<span class="string">&quot;APPLE&quot;</span>, <span class="string">&quot;Apple&quot;</span>), <span class="comment">//</span></span><br><span class="line">            Arguments.arguments(<span class="string">&quot;gooD&quot;</span>, <span class="string">&quot;Good&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的代码很容易理解：静态方法<code>testCapitalize()</code>返回了一组测试参数，每个参数都包含两个<code>String</code>，正好作为测试方法的两个参数传入。</p>
<p>如果静态方法和测试方法的名称不同，<span class="citation" data-cites="MethodSource也允许指定方法名">@MethodSource也允许指定方法名</span>。但使用默认同名方法最方便。</p>
<p>另一种传入测试参数的方法是使用<code>@CsvSource</code>，它的每一个字符串表示一行，一行包含的若干参数用<code>,</code>分隔，因此，上述测试又可以改写如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@ParameterizedTest</span></span><br><span class="line"><span class="meta">@CsvSource(&#123; &quot;abc, Abc&quot;, &quot;APPLE, Apple&quot;, &quot;gooD, Good&quot; &#125;)</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testCapitalize</span><span class="params">(String input, String result)</span> </span>&#123;</span><br><span class="line">    assertEquals(result, StringUtils.capitalize(input));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果有成百上千的测试输入，那么，直接写<code>@CsvSource</code>就很不方便。这个时候，我们可以把测试数据提到一个独立的CSV文件中，然后标注上<code>@CsvFileSource</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@ParameterizedTest</span></span><br><span class="line"><span class="meta">@CsvFileSource(resources = &#123; &quot;/test-capitalize.csv&quot; &#125;)</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">testCapitalizeUsingCsvFile</span><span class="params">(String input, String result)</span> </span>&#123;</span><br><span class="line">    assertEquals(result, StringUtils.capitalize(input));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>JUnit只在classpath中查找指定的CSV文件，因此，<code>test-capitalize.csv</code>这个文件要放到<code>test</code>目录下，内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apple, Apple</span><br><span class="line">HELLO, Hello</span><br><span class="line">JUnit, Junit</span><br><span class="line">reSource, Resource</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记8——java 日期与时间（更新中）</title>
    <url>/2022/02/01/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B09%E2%80%94java%E6%97%A5%E6%9C%9F%E4%B8%8E%E6%97%B6%E9%97%B4/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1298613246361634</p>
<h3 id="一基本概念">一、基本概念：</h3>
<h4 id="本地时间">1、本地时间</h4>
<p>​ 不同的时区，在同一时刻，本地时间是不同的。全球一共分为24个时区，<strong>伦敦所在的时区称为标准时区，其他时区按东／西偏移的小时区分，北京所在的时区是东八区。</strong></p>
<h4 id="时区">2、时区</h4>
<p>​ 因为光靠本地时间还无法唯一确定一个准确的时刻，所以我们还需要给本地时间加上一个时区。时区有好几种表示方式。</p>
<p>​ 一种是以<code>GMT</code>或者<code>UTC</code>加时区偏移表示，例如：<code>GMT+08:00</code>或者<code>UTC+08:00</code>表示东八区。</p>
<p>​ <code>GMT</code>和<code>UTC</code>可以认为基本是等价的，只是<code>UTC</code>使用更精确的原子钟计时，每隔几年会有一个闰秒，我们在开发程序的时候可以忽略两者的误差，因为计算机的时钟在联网的时候会自动与时间服务器同步时间。</p>
<p>​ 另一种是缩写，例如，<code>CST</code>表示<code>China Standard Time</code>，也就是中国标准时间。但是<code>CST</code>也可以表示美国中部时间<code>Central Standard Time USA</code>，因此，缩写容易产生混淆，我们尽量不要使用缩写。</p>
<p>​ 最后一种是以洲／城市表示，例如，<code>Asia/Shanghai</code>，表示上海所在地的时区。特别注意城市名称不是任意的城市，而是由国际标准组织规定的城市。</p>
<p>​ 因为时区的存在，东八区的2019年11月20日早上8:15，和西五区的2019年11月19日晚上19:15，他们的时刻是相同的</p>
<h4 id="夏令时">3、夏令时</h4>
<p>​ 时区还不是最复杂的，更复杂的是夏令时。所谓夏令时，就是夏天开始的时候，把时间往后拨1小时，夏天结束的时候，再把时间往前拨1小时。</p>
<p>​ 因为涉及到夏令时，相同的时区，如果表示的方式不同，转换出的时间是不同的。我们举个栗子：</p>
<p>对于2019-11-20和2019-6-20两个日期来说，假设北京人在纽约：</p>
<ul>
<li>如果以<code>GMT</code>或者<code>UTC</code>作为时区，无论日期是多少，时间都是<code>19:00</code>；</li>
<li>如果以国家／城市表示，例如<code>America／NewYork</code>，虽然纽约也在西五区，但是，因为夏令时的存在，在不同的日期，<code>GMT</code>时间和纽约时间可能是不一样的：</li>
</ul>
<table>
<thead>
<tr class="header">
<th>时区</th>
<th>2019-11-20</th>
<th>2019-6-20</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GMT-05:00</td>
<td>19:00</td>
<td>19:00</td>
</tr>
<tr class="even">
<td>UTC-05:00</td>
<td>19:00</td>
<td>19:00</td>
</tr>
<tr class="odd">
<td>America/New_York</td>
<td>19:00</td>
<td>20:00</td>
</tr>
</tbody>
</table>
<p>​ 实行夏令时的不同地区，进入和退出夏令时的时间很可能是不同的。同一个地区，根据历史上是否实行过夏令时，标准时间在不同年份换算成当地时间也是不同的。因此，计算夏令时，没有统一的公式，必须按照一组给定的规则来算，并且，该规则要定期更新。</p>
<h4 id="本地化">4、本地化</h4>
<p>​ 在计算机中，通常使用<code>Locale</code>表示一个国家或地区的日期、时间、数字、货币等格式。<code>Locale</code>由<code>语言_国家</code>的字母缩写构成，例如，<code>zh_CN</code>表示中文+中国，<code>en_US</code>表示英文+美国。语言使用小写，国家使用大写。</p>
<p>对于日期来说，不同的Locale，例如，中国和美国的表示方式如下：</p>
<ul>
<li>zh_CN：2016-11-30</li>
<li>en_US：11/30/2016</li>
</ul>
<p>​ 计算机用<code>Locale</code>在日期、时间、货币和字符串之间进行转换。一个电商网站会根据用户所在的<code>Locale</code>对用户显示如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>中国用户</th>
<th>美国用户</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>购买价格</td>
<td>12000.00</td>
<td>12,000.00</td>
</tr>
<tr class="even">
<td>购买日期</td>
<td>2016-11-30</td>
<td>11/30/2016</td>
</tr>
</tbody>
</table>
<h3 id="二date和calendar">二、Date和Calendar</h3>
<p>​ 这个“同一个时刻”在计算机中存储的本质上只是一个整数，我们称它为<code>Epoch Time</code></p>
<p>​ <code>Epoch Time</code>是计算从1970年1月1日零点（格林威治时区／GMT+00:00）到现在所经历的秒数，例如：</p>
<p>​ <code>1574208900</code>表示从从1970年1月1日零点GMT时区到该时刻一共经历了1574208900秒，换算成伦敦、北京和纽约时间分别是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1574208900 = 北京时间2019-11-20 8:15:00</span><br><span class="line">           = 伦敦时间2019-11-20 0:15:00</span><br><span class="line">           = 纽约时间2019-11-19 19:15:00</span><br></pre></td></tr></table></figure>
<p><code>Epoch Time</code>又称为时间戳，在不同的编程语言中，会有几种存储方式：</p>
<ul>
<li>以秒为单位的整数：1574208900，缺点是精度只能到秒；</li>
<li>以毫秒为单位的整数：1574208900123，最后3位表示毫秒数；</li>
<li>以秒为单位的浮点数：1574208900.123，小数点后面表示零点几秒。</li>
</ul>
<p><strong>在Java程序中，时间戳通常是用<code>long</code>表示的毫秒数，即：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">long t = 1574208900123L;</span><br></pre></td></tr></table></figure>
<p><strong>转换成北京时间就是<code>2019-11-20T8:15:00.123</code>。要获取当前时间戳，可以使用<code>System.currentTimeMillis()</code>，这是Java程序获取时间戳最常用的方法。</strong></p>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记8——java IO模块（更新中）</title>
    <url>/2022/02/01/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B08%E2%80%94java%20IO%E6%A8%A1%E5%9D%97/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1255945227202752</p>
<h3 id="一简介">一、简介：</h3>
<p>​ 在Java中，<code>InputStream</code>代表输入字节流，<code>OuputStream</code>代表输出字节流，这是最基本的两种IO流。</p>
<p>​ 如果我们需要读写的是字符，并且字符不全是单字节表示的ASCII字符，那么，按照<code>char</code>来读写显然更方便，这种流称为<em>字符流</em>。</p>
<p>​ Java提供了<code>Reader</code>和<code>Writer</code>表示字符流，字符流传输的最小数据单位是<code>char</code>。</p>
<p>​ <code>Reader</code>和<code>Writer</code><strong>本质上是一个能自动编解码的<code>InputStream</code>和<code>OutputStream</code></strong>。</p>
<p>​ <strong>使用<code>Reader</code>，数据源虽然是字节，但我们读入的数据都是<code>char</code>类型的字符，原因是<code>Reader</code>内部把读入的<code>byte</code>做了解码，转换成了<code>char</code>。</strong></p>
<p>​ <strong>使用<code>InputStream</code>，我们读入的数据和原始二进制数据一模一样，是<code>byte[]</code>数组</strong>，但是我们可以自己把二进制<code>byte[]</code>数组按照某种编码转换为字符串。</p>
<h3 id="二file对象">二、File对象：</h3>
<h4 id="构造file对象">1、构造File对象</h4>
<p>​ Java的标准库<code>java.io</code>提供了<code>File</code>对象来操作文件和目录。</p>
<p>​ 要构造一个<code>File</code>对象，需要传入文件路径：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        File f = <span class="keyword">new</span> File(<span class="string">&quot;C:\\Windows\\notepad.exe&quot;</span>);</span><br><span class="line">        System.out.println(f);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="file对象有3种形式表示的路径">2、File对象有3种形式表示的路径</h4>
<ul>
<li><code>getPath()</code>，返回构造方法传入的路径</li>
<li><code>getAbsolutePath()</code>，返回绝对路径</li>
<li><code>getCanonicalPath</code>，它和绝对路径类似，但是返回的是规范路径。</li>
</ul>
<p><strong>什么是规范路径？</strong></p>
<p>​ 绝对路径可以表示成<code>C:\Windows\System32\..\notepad.exe</code>，而<strong>规范路径就是把<code>.</code>和<code>..</code>转换成标准的绝对路径后的路径</strong>：<code>C:\Windows\notepad.exe</code>。</p>
<h4 id="文件和目录">3、文件和目录：</h4>
<p>​ <code>File</code>对象既可以表示文件，也可以表示目录。特别要注意的是，<strong>构造一个<code>File</code>对象，即使传入的文件或目录不存在，代码也不会出错</strong>，因为构造一个<code>File</code>对象，并不会导致任何磁盘操作。<strong>只有当我们调用<code>File</code>对象的某些方法的时候，才真正进行磁盘操作</strong>。</p>
<ul>
<li><code>isFile()</code>，判断该<code>File</code>对象是否是一个已存在的文件</li>
<li><code>isDirectory()</code>，判断该<code>File</code>对象是否是一个已存在的目录：</li>
</ul>
<p><strong>用<code>File</code>对象获取到一个文件时，还可以进一步判断文件的权限和大小：</strong></p>
<ul>
<li><code>boolean canRead()</code>：是否可读；</li>
<li><code>boolean canWrite()</code>：是否可写；</li>
<li><code>boolean canExecute()</code>：是否可执行；</li>
<li><code>long length()</code>：文件字节大小。</li>
</ul>
<p>对目录而言，是否可执行表示能否列出它包含的文件和子目录。</p>
<h4 id="创建和删除文件">4、创建和删除文件：</h4>
<p>​ 当File对象表示一个文件时，可以通过<code>createNewFile()</code>创建一个新文件，用<code>delete()</code>删除该文件：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">File file = <span class="keyword">new</span> File(<span class="string">&quot;/path/to/file&quot;</span>);</span><br><span class="line"><span class="keyword">if</span> (file.createNewFile()) &#123;</span><br><span class="line">    <span class="comment">// 文件创建成功:</span></span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span></span></span><br><span class="line">    <span class="keyword">if</span> (file.delete()) &#123;</span><br><span class="line">        <span class="comment">// 删除文件成功:</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 有些时候，程序需要读写一些临时文件，<strong>File对象提供了<code>createTempFile()</code>来创建一个临时文件</strong>，以及<code>deleteOnExit()</code>在JVM退出时自动删除该文件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        File f = File.createTempFile(<span class="string">&quot;tmp-&quot;</span>, <span class="string">&quot;.txt&quot;</span>); <span class="comment">// 提供临时文件的前缀和后缀</span></span><br><span class="line">        f.deleteOnExit(); <span class="comment">// JVM退出时自动删除</span></span><br><span class="line">        System.out.println(f.isFile());</span><br><span class="line">        System.out.println(f.getAbsolutePath());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="遍历文件和目录">5、遍历文件和目录：</h4>
<p>​ 当File对象表示一个目录时，可以使用<code>list()</code>和<code>listFiles()</code>列出目录下的文件和子目录名。<code>listFiles()</code>提供了一系列重载方法，可以过滤不想要的文件和目录：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        File f = <span class="keyword">new</span> File(<span class="string">&quot;C:\\Windows&quot;</span>);</span><br><span class="line">        File[] fs1 = f.listFiles(); <span class="comment">// 列出所有文件和子目录</span></span><br><span class="line">        printFiles(fs1);</span><br><span class="line">        File[] fs2 = f.listFiles(<span class="keyword">new</span> FilenameFilter() &#123; <span class="comment">// 仅列出.exe文件</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">accept</span><span class="params">(File dir, String name)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> name.endsWith(<span class="string">&quot;.exe&quot;</span>); <span class="comment">// 返回true表示接受该文件</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        printFiles(fs2);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">printFiles</span><span class="params">(File[] files)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;==========&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (files != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (File f : files) &#123;</span><br><span class="line">                System.out.println(f);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">&quot;==========&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和文件操作类似，File对象如果表示一个目录，可以通过以下方法创建和删除目录：</p>
<ul>
<li><code>boolean mkdir()</code>：创建当前File对象表示的目录；</li>
<li><code>boolean mkdirs()</code>：创建当前File对象表示的目录，并在必要时将不存在的父目录也创建出来；</li>
<li><code>boolean delete()</code>：删除当前File对象表示的目录，当前目录必须为空才能删除成功。</li>
</ul>
<h4 id="path对象">6、path对象：</h4>
<p>​ Java标准库还提供了一个<code>Path</code>对象，它位于<code>java.nio.file</code>包。<code>Path</code>对象和<code>File</code>对象类似，但操作更加简单：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Path p1 = Paths.get(<span class="string">&quot;.&quot;</span>, <span class="string">&quot;project&quot;</span>, <span class="string">&quot;study&quot;</span>); <span class="comment">// 构造一个Path对象</span></span><br><span class="line">        System.out.println(p1);</span><br><span class="line">        Path p2 = p1.toAbsolutePath(); <span class="comment">// 转换为绝对路径</span></span><br><span class="line">        System.out.println(p2);</span><br><span class="line">        Path p3 = p2.normalize(); <span class="comment">// 转换为规范路径</span></span><br><span class="line">        System.out.println(p3);</span><br><span class="line">        File f = p3.toFile(); <span class="comment">// 转换为File对象</span></span><br><span class="line">        System.out.println(f);</span><br><span class="line">        <span class="keyword">for</span> (Path p : Paths.get(<span class="string">&quot;..&quot;</span>).toAbsolutePath()) &#123; <span class="comment">// 可以直接遍历Path</span></span><br><span class="line">            System.out.println(<span class="string">&quot;  &quot;</span> + p);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="三inputstream-和-outputstream">三、InputStream 和 OutputStream</h3>
<h4 id="inputstream">1、InputStream</h4>
<p>​ <code>InputStream</code>就是Java标准库提供的最基本的输入流。它位于<code>java.io</code>这个包里。<code>java.io</code>包提供了所有同步IO的功能。</p>
<p>​ <code>InputStream</code>并不是一个接口，而是一个抽象类，它是所有输入流的超类。这个抽象类定义的一个最重要的方法就是<code>int read()</code>，签名如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public abstract int read() throws IOException;</span><br></pre></td></tr></table></figure>
<p>​ 这个方法会读取输入流的下一个字节，并返回字节表示的<code>int</code>值（0~255）。如果已读到末尾，返回<code>-1</code>表示不能继续读取了。</p>
<h4 id="fileinputstream">2、FileInputStream</h4>
<p>​ <code>FileInputStream</code>是<code>InputStream</code>的一个子类。顾名思义，<code>FileInputStream</code>就是从文件流中读取数据。下面的代码演示了如何完整地读取一个<code>FileInputStream</code>的所有字节</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 创建一个FileInputStream对象:</span></span><br><span class="line">    InputStream input = <span class="keyword">new</span> FileInputStream(<span class="string">&quot;src/readme.txt&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">        <span class="keyword">int</span> n = input.read(); <span class="comment">// 反复调用read()方法，直到返回-1</span></span><br><span class="line">        <span class="keyword">if</span> (n == -<span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(n); <span class="comment">// 打印byte的值</span></span><br><span class="line">    &#125;</span><br><span class="line">    input.close(); <span class="comment">// 关闭流, InputStream和OutputStream都是通过close()方法来关闭流。关闭流就会释放对应的底层资源。</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 我们还要注意到在读取或写入IO流的过程中，可能会发生错误，例如，文件不存在导致无法读取，没有写权限导致写入失败，等等，这些底层错误由Java虚拟机自动封装成<code>IOException</code>异常并抛出。因此，所有与IO操作相关的代码都必须正确处理<code>IOException</code>。</p>
<p>​ <strong>我们可以利用Java 7引入的新的<code>try(resource)</code>的语法</strong>，只需要编写<code>try</code>语句，让编译器自动为我们关闭资源。推荐的写法如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> (InputStream input = <span class="keyword">new</span> FileInputStream(<span class="string">&quot;src/readme.txt&quot;</span>)) &#123;</span><br><span class="line">        <span class="keyword">int</span> n;</span><br><span class="line">        <span class="keyword">while</span> ((n = input.read()) != -<span class="number">1</span>) &#123;</span><br><span class="line">            System.out.println(n);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="comment">// 编译器在此自动为我们写入finally并调用close()</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 实际上，编译器并不会特别地为<code>InputStream</code>加上自动关闭。<strong>编译器只看<code>try(resource = ...)</code>中的对象是否实现了<code>java.lang.AutoCloseable</code>接口</strong>，如果实现了，就自动加上<code>finally</code>语句并调用<code>close()</code>方法。<code>InputStream</code>和<code>OutputStream</code>都实现了这个接口，因此，都可以用在<code>try(resource)</code>中。</p>
<h4 id="缓冲区读取数据">3、缓冲区读取数据</h4>
<p>​ 在读取流的时候，一次读取一个字节并不是最高效的方法。很多流支持一次性读取多个字节到缓冲区，对于文件和网络流来说，利用缓冲区一次性读取多个字节效率往往要高很多。<code>InputStream</code>提供了两个重载方法来支持读取多个字节：</p>
<ul>
<li><code>int read(byte[] b)</code>：读取若干字节并填充到<code>byte[]</code>数组，返回读取的字节数</li>
<li><code>int read(byte[] b, int off, int len)</code>：指定<code>byte[]</code>数组的偏移量和最大填充数</li>
</ul>
<p>​ 利用上述方法一次读取多个字节时，需要先定义一个<code>byte[]</code>数组作为缓冲区，<code>read()</code>方法会尽可能多地读取字节到缓冲区， 但不会超过缓冲区的大小。<code>read()</code>方法的返回值不再是字节的<code>int</code>值，而是返回实际读取了多少个字节。如果返回<code>-1</code>，表示没有更多的数据了。</p>
<p>利用缓冲区一次读取多个字节的代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> (InputStream input = <span class="keyword">new</span> FileInputStream(<span class="string">&quot;src/readme.txt&quot;</span>)) &#123;</span><br><span class="line">        <span class="comment">// 定义1000个字节大小的缓冲区:</span></span><br><span class="line">        <span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1000</span>];</span><br><span class="line">        <span class="keyword">int</span> n;</span><br><span class="line">        <span class="keyword">while</span> ((n = input.read(buffer)) != -<span class="number">1</span>) &#123; <span class="comment">// 读取到缓冲区</span></span><br><span class="line">            System.out.println(<span class="string">&quot;read &quot;</span> + n + <span class="string">&quot; bytes.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="阻塞">4、阻塞</h4>
<p>​ 在调用<code>InputStream</code>的<code>read()</code>方法读取数据时，我们说<code>read()</code>方法是阻塞（Blocking）的。它的意思是，对于下面的代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> n;</span><br><span class="line">n = input.read(); <span class="comment">// 必须等待read()方法返回才能执行下一行代码</span></span><br><span class="line"><span class="keyword">int</span> m = n;</span><br></pre></td></tr></table></figure>
<p>​ 执行到第二行代码时，必须等<code>read()</code>方法返回后才能继续。因为读取IO流相比执行普通代码，速度会慢很多，因此，无法确定<code>read()</code>方法调用到底要花费多长时间。</p>
<h3 id="四outputstream">四、OutputStream</h3>
<h4 id="介绍">1、介绍</h4>
<p>​ 和<code>InputStream</code>类似，<code>OutputStream</code>也是抽象类，它是所有输出流的超类。这个抽象类定义的一个最重要的方法就是<code>void write(int b)</code>，签名如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(<span class="keyword">int</span> b)</span> <span class="keyword">throws</span> IOException</span>;</span><br></pre></td></tr></table></figure>
<p>​ 这个方法会<strong>写入一个字节到输出流</strong>。要注意的是，虽然传入的是<code>int</code>参数，但只会写入一个字节，即只写入<code>int</code>最低8位表示字节的部分（相当于<code>b &amp; 0xff</code>）。</p>
<h4 id="flush方法">2、Flush方法</h4>
<p>​ 要特别注意：<code>OutputStream</code>还提供了一个<code>flush()</code>方法，它的目的是将缓冲区的内容真正输出到目的地。</p>
<p>​ 为什么要有<code>flush()</code>？因为向磁盘、网络写入数据的时候，出于效率的考虑，操作系统并不是输出一个字节就立刻写入到文件或者发送到网络，而是把输出的字节先放到内存的一个缓冲区里（本质上就是一个<code>byte[]</code>数组），等到缓冲区写满了，再一次性写入文件或者网络。对于很多IO设备来说，一次写一个字节和一次写1000个字节，花费的时间几乎是完全一样的，所以<code>OutputStream</code>有个<code>flush()</code>方法，能强制把缓冲区内容输出。</p>
<p>​ 通常情况下，我们不需要调用这个<code>flush()</code>方法，因为缓冲区写满了<code>OutputStream</code>会自动调用它，并且，在调用<code>close()</code>方法关闭<code>OutputStream</code>之前，也会自动调用<code>flush()</code>方法。</p>
<p>​ <strong>但是，在某些情况下，我们必须手动调用<code>flush()</code>方法。举个栗子：</strong></p>
<p>​ 小明正在开发一款在线聊天软件，当用户输入一句话后，就通过<code>OutputStream</code>的<code>write()</code>方法写入网络流。小明测试的时候发现，发送方输入后，接收方根本收不到任何信息，怎么肥四？</p>
<p>​ 原因就在于写入网络流是先写入内存缓冲区，等缓冲区满了才会一次性发送到网络。如果缓冲区大小是4K，则发送方要敲几千个字符后，操作系统才会把缓冲区的内容发送出去，这个时候，接收方会一次性收到大量消息。</p>
<p>​ 解决办法就是每输入一句话后，立刻调用<code>flush()</code>，不管当前缓冲区是否已满，强迫操作系统把缓冲区的内容立刻发送出去。</p>
<h4 id="将若干个字节写入文件流-示例">3、将若干个字节写入文件流 示例：</h4>
<p>​ 我们以<code>FileOutputStream</code>为例，演示如何将若干个字节写入文件流：</p>
<p>​ 每次写入一个字节非常麻烦，更常见的方法是一次性写入若干字节。这时，可以用<code>OutputStream</code>提供的重载方法<code>void write(byte[])</code>来实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    OutputStream output = <span class="keyword">new</span> FileOutputStream(<span class="string">&quot;out/readme.txt&quot;</span>);</span><br><span class="line">    output.write(<span class="string">&quot;Hello&quot;</span>.getBytes(<span class="string">&quot;UTF-8&quot;</span>)); <span class="comment">// Hello</span></span><br><span class="line">    output.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 和<code>InputStream</code>一样，上述代码没有考虑到在发生异常的情况下如何正确地关闭资源。写入过程也会经常发生IO错误，例如，磁盘已满，无权限写入等等。我们需要用<code>try(resource)</code>来保证<code>OutputStream</code>在无论是否发生IO错误的时候都能够正确地关闭：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> (OutputStream output = <span class="keyword">new</span> FileOutputStream(<span class="string">&quot;out/readme.txt&quot;</span>)) &#123;</span><br><span class="line">        output.write(<span class="string">&quot;Hello&quot;</span>.getBytes(<span class="string">&quot;UTF-8&quot;</span>)); <span class="comment">// Hello</span></span><br><span class="line">    &#125; <span class="comment">// 编译器在此自动为我们写入finally并调用close()</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="四阻塞">四、阻塞</h4>
<p>和<code>InputStream</code>一样，<code>OutputStream</code>的<code>write()</code>方法也是阻塞的。</p>
<h3 id="五filter模式">五、Filter模式</h3>
<h3 id="六操作zip">六、操作Zip</h3>
<p><code>ZipInputStream</code>是一种<code>FilterInputStream</code>，它可以直接读取zip包的内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">┌───────────────────┐</span><br><span class="line">│    InputStream    │</span><br><span class="line">└───────────────────┘</span><br><span class="line">          ▲</span><br><span class="line">          │</span><br><span class="line">┌───────────────────┐</span><br><span class="line">│ FilterInputStream │</span><br><span class="line">└───────────────────┘</span><br><span class="line">          ▲</span><br><span class="line">          │</span><br><span class="line">┌───────────────────┐</span><br><span class="line">│InflaterInputStream│</span><br><span class="line">└───────────────────┘</span><br><span class="line">          ▲</span><br><span class="line">          │</span><br><span class="line">┌───────────────────┐</span><br><span class="line">│  ZipInputStream   │</span><br><span class="line">└───────────────────┘</span><br><span class="line">          ▲</span><br><span class="line">          │</span><br><span class="line">┌───────────────────┐</span><br><span class="line">│  JarInputStream   │</span><br><span class="line">└───────────────────┘</span><br></pre></td></tr></table></figure>
<h4 id="读取zip包">1、读取Zip包：</h4>
<p>​ 我们要创建一个<code>ZipInputStream</code>，通常是传入一个<code>FileInputStream</code>作为数据源，然后，循环调用<code>getNextEntry()</code>，直到返回<code>null</code>，表示zip流结束。</p>
<p>​ 一个<code>ZipEntry</code>表示一个压缩文件或目录，如果是压缩文件，我们就用<code>read()</code>方法不断读取，直到返回<code>-1</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> (ZipInputStream zip = <span class="keyword">new</span> ZipInputStream(<span class="keyword">new</span> FileInputStream(...))) &#123;</span><br><span class="line">    ZipEntry entry = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">while</span> ((entry = zip.getNextEntry()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        String name = entry.getName();</span><br><span class="line">        <span class="keyword">if</span> (!entry.isDirectory()) &#123;</span><br><span class="line">            <span class="keyword">int</span> n;</span><br><span class="line">            <span class="keyword">while</span> ((n = zip.read()) != -<span class="number">1</span>) &#123;</span><br><span class="line">                ...</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="写入zip包">2、写入Zip包：</h4>
<p><code>ZipOutputStream</code>是一种<code>FilterOutputStream</code>，它可以直接写入内容到zip包。我们要先创建一个<code>ZipOutputStream</code>，通常是包装一个<code>FileOutputStream</code>，然后，每写入一个文件前，先调用<code>putNextEntry()</code>，然后用<code>write()</code>写入<code>byte[]</code>数据，写入完毕后调用<code>closeEntry()</code>结束这个文件的打包。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> (ZipOutputStream zip = <span class="keyword">new</span> ZipOutputStream(<span class="keyword">new</span> FileOutputStream(...))) &#123;</span><br><span class="line">    File[] files = ...</span><br><span class="line">    <span class="keyword">for</span> (File file : files) &#123;</span><br><span class="line">        zip.putNextEntry(<span class="keyword">new</span> ZipEntry(file.getName()));</span><br><span class="line">        zip.write(getFileDataAsBytes(file));</span><br><span class="line">        zip.closeEntry();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的代码没有考虑文件的目录结构。如果要实现目录层次结构，<code>new ZipEntry(name)</code>传入的<code>name</code>要用相对路径。</p>
<h3 id="七读取classpath资源">七、读取classpath资源：</h3>
<h3 id="八序列化">八、序列化</h3>
<h3 id="九reader">九、Reader</h3>
<h3 id="十writer">十、Writer</h3>
<h3 id="十一printstream和printwriter">十一、PrintStream和PrintWriter</h3>
<h3 id="十二使用files">十二、使用Files</h3>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记7——java泛型（更新中）</title>
    <url>/2022/02/01/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B07%E2%80%94java%E6%B3%9B%E5%9E%8B/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1264799402020448</p>
<h3 id="一泛型定义简介">一、泛型定义简介</h3>
<p>​ 泛型是一种“代码模板”，可以用一套代码套用各种类型。</p>
<p>​ 我们可以把<code>ArrayList</code>变成一种模板：<code>ArrayList&lt;T&gt;</code>，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ArrayList</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> T[] array;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> size;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(T e)</span> </span>&#123;...&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;...&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> T <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;...&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>T</code>可以是任何class。这样一来，我们就实现了：<strong>编写一次模版，可以创建任意类型的<code>ArrayList</code>：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建可以存储String的ArrayList:</span></span><br><span class="line">ArrayList&lt;String&gt; strList = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"><span class="comment">// 创建可以存储Float的ArrayList:</span></span><br><span class="line">ArrayList&lt;Float&gt; floatList = <span class="keyword">new</span> ArrayList&lt;Float&gt;();</span><br><span class="line"><span class="comment">// 创建可以存储Person的ArrayList:</span></span><br><span class="line">ArrayList&lt;Person&gt; personList = <span class="keyword">new</span> ArrayList&lt;Person&gt;();</span><br></pre></td></tr></table></figure>
<p>​ 因此，<strong>泛型就是定义一种模板，例如<code>ArrayList&lt;T&gt;</code>，然后在代码中为用到的类创建对应的<code>ArrayList&lt;类型&gt;</code>：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ArrayList&lt;String&gt; strList = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：注意泛型的继承关系：可以把<code>ArrayList&lt;Integer&gt;</code>向上转型为<code>List&lt;Integer&gt;</code>（<code>T</code>不能变！），但不能把<code>ArrayList&lt;Integer&gt;</code>向上转型为<code>ArrayList&lt;Number&gt;</code>（<code>T</code>不能变成父类）。</p>
<h3 id="二使用泛型">二、使用泛型</h3>
<h4 id="简写泛型">1、简写泛型：</h4>
<p>当我们定义泛型类型<code>&lt;Number&gt;</code>后，<code>List&lt;T&gt;</code>的泛型接口变为强类型<code>List&lt;Number&gt;</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;Number&gt; list = <span class="keyword">new</span> ArrayList&lt;Number&gt;();</span><br><span class="line">list.add(<span class="keyword">new</span> Integer(<span class="number">123</span>));</span><br><span class="line">list.add(<span class="keyword">new</span> Double(<span class="number">12.34</span>));</span><br><span class="line">Number first = list.get(<span class="number">0</span>);</span><br><span class="line">Number second = list.get(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p><strong>编译器如果能自动推断出泛型类型，就可以省略后面的泛型类型</strong>。例如，对于下面的代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;Number&gt; list = <span class="keyword">new</span> ArrayList&lt;Number&gt;();</span><br></pre></td></tr></table></figure>
<p>编译器看到泛型类型<code>List&lt;Number&gt;</code>就可以自动推断出后面的<code>ArrayList&lt;T&gt;</code>的泛型类型必须是<code>ArrayList&lt;Number&gt;</code>，因此，可以把代码简写为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 可以省略后面的Number，编译器可以自动推断泛型类型：</span></span><br><span class="line">List&lt;Number&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure>
<h4 id="使用泛型接口">2、使用泛型接口：</h4>
<p>​ 除了<code>ArrayList&lt;T&gt;</code>使用了泛型，还可以在接口中使用泛型。例如， <code>Arrays.sort(Object[])</code>可以对任意数组进行排序，但待排序的元素必须实现<code>Comparable&lt;T&gt;</code>这个泛型接口：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public interface Comparable&lt;T&gt; &#123;</span><br><span class="line">    /**</span><br><span class="line">     * 返回负数: 当前实例比参数o小</span><br><span class="line">     * 返回0: 当前实例与参数o相等</span><br><span class="line">     * 返回正数: 当前实例比参数o大</span><br><span class="line">     */</span><br><span class="line">    int compareTo(T o);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 如下示例所示：实现对我们的自定义类Person的元素排序：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Person[] ps = <span class="keyword">new</span> Person[] &#123;</span><br><span class="line">            <span class="keyword">new</span> Person(<span class="string">&quot;Bob&quot;</span>, <span class="number">61</span>),</span><br><span class="line">            <span class="keyword">new</span> Person(<span class="string">&quot;Alice&quot;</span>, <span class="number">88</span>),</span><br><span class="line">            <span class="keyword">new</span> Person(<span class="string">&quot;Lily&quot;</span>, <span class="number">75</span>),</span><br><span class="line">        &#125;;</span><br><span class="line">        Arrays.sort(ps);</span><br><span class="line">        System.out.println(Arrays.toString(ps));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Comparable</span>&lt;<span class="title">Person</span>&gt; </span>&#123;</span><br><span class="line">    String name;</span><br><span class="line">    <span class="keyword">int</span> score;</span><br><span class="line">    Person(String name, <span class="keyword">int</span> score) &#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.score = score;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Person other)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.name.compareTo(other.name);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.name + <span class="string">&quot;,&quot;</span> + <span class="keyword">this</span>.score;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 运行上述代码，可以正确实现按name进行排序。</span></span><br></pre></td></tr></table></figure>
<h3 id="三编写泛型">三、编写泛型</h3>
<h4 id="一般方法">1、一般方法：</h4>
<ul>
<li><p>首先，按照某种类型，例如：<code>String</code>，来编写类：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String first;</span><br><span class="line">    <span class="keyword">private</span> String last;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Pair</span><span class="params">(String first, String last)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.first = first;</span><br><span class="line">        <span class="keyword">this</span>.last = last;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getFirst</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> first;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getLast</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>然后，标记所有的特定类型，这里是<code>String</code>,并替换为T，并申明<code>&lt;T&gt;</code>：</p></li>
<li><p>``` public class Pair<T> { private T first; private T last; public Pair(T first, T last) { this.first = first; this.last = last; } public T getFirst() { return first; } public T getLast() { return last; } } <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 熟练后即可直接从`T`开始编写。</span><br><span class="line"></span><br><span class="line">#### 2、静态方法：</span><br><span class="line"></span><br><span class="line">​	泛型类型`&lt;T&gt;`不能用于静态方法。</span><br><span class="line"></span><br><span class="line">​	对于静态方法，我们可以单独改写为“泛型”方法，只需要使用另一个类型即可。对于上面的`create()`静态方法，我们应该把它改为另一种泛型类型，例如，`&lt;K&gt;`：</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first = first;</span><br><span class="line">        this.last = last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public T getLast() &#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    // 静态泛型方法应该使用其他类型区分:</span><br><span class="line">    public static &lt;K&gt; Pair&lt;K&gt; create(K first, K last) &#123;</span><br><span class="line">        return new Pair&lt;K&gt;(first, last);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></li>
</ul>
<p>这样才能清楚地将静态方法的泛型类型和实例类型的泛型类型区分开。</p>
<h4 id="多个泛型的类型">3、多个泛型的类型：</h4>
<p>​ 泛型还可以定义多种类型。例如，我们希望<code>Pair</code>不总是存储两个类型一样的对象，就可以使用类型<code>&lt;T, K&gt;</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>&lt;<span class="title">T</span>, <span class="title">K</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> T first;</span><br><span class="line">    <span class="keyword">private</span> K last;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Pair</span><span class="params">(T first, K last)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.first = first;</span><br><span class="line">        <span class="keyword">this</span>.last = last;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> T <span class="title">getFirst</span><span class="params">()</span> </span>&#123; ... &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> K <span class="title">getLast</span><span class="params">()</span> </span>&#123; ... &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用的时候，需要指出两种类型：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pair&lt;String, Integer&gt; p = <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;test&quot;</span>, <span class="number">123</span>);</span><br></pre></td></tr></table></figure>
<p>​ <strong>Java标准库的<code>Map&lt;K, V&gt;</code>就是使用两种泛型类型的例子。它对Key使用一种类型，对Value使用另一种类型。</strong></p>
<h3 id="四擦拭法java语言的泛型实现方法">四、擦拭法（java语言的泛型实现方法）：</h3>
<h4 id="擦拭法的基本含义">1、擦拭法的基本含义</h4>
<p>​ 擦拭法是指，<strong>虚拟机对泛型其实一无所知，所有的工作都是编译器做的。</strong></p>
<p>​ 例如，我们编写了一个泛型类<code>Pair&lt;T&gt;</code>，这是编译器看到的代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> T first;</span><br><span class="line">    <span class="keyword">private</span> T last;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Pair</span><span class="params">(T first, T last)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.first = first;</span><br><span class="line">        <span class="keyword">this</span>.last = last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而虚拟机根本不知道泛型。这是虚拟机执行的代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Object first;</span><br><span class="line">    <span class="keyword">private</span> Object last;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Pair</span><span class="params">(Object first, Object last)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.first = first;</span><br><span class="line">        <span class="keyword">this</span>.last = last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因此，java使用擦拭法实现泛型，导致了：</p>
<ul>
<li><strong>编译器把类型<code>&lt;T&gt;</code>视为<code>Object</code>；</strong></li>
<li><strong>编译器根据<code>&lt;T&gt;</code>实现安全的强制转型。</strong></li>
</ul>
<p>使用泛型的时候，我们编写的代码也是编译器看到的代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pair&lt;String&gt; p = <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;world&quot;</span>);</span><br><span class="line">String first = p.getFirst();</span><br><span class="line">String last = p.getLast();</span><br></pre></td></tr></table></figure>
<p>而虚拟机执行的代码并没有泛型：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pair p = <span class="keyword">new</span> Pair(<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;world&quot;</span>);</span><br><span class="line">String first = (String) p.getFirst();</span><br><span class="line">String last = (String) p.getLast();</span><br></pre></td></tr></table></figure>
<p><strong>Java的泛型是由编译器在编译时实行的，编译器内部永远把所有类型<code>T</code>视为<code>Object</code>处理，但是，在需要转型的时候，编译器会根据<code>T</code>的类型自动为我们实行安全地强制转型。</strong></p>
<h4 id="java泛型的局限">2、java泛型的局限：</h4>
<ul>
<li><p>1） <code>&lt;T&gt;</code>不能是基本类型，例如<code>int</code>，因为实际类型是<code>Object</code>，<code>Object</code>类型无法持有基本类型：</p></li>
<li><p>2） 所有泛型实例，无论<code>T</code>的类型是什么，<code>getClass()</code>返回同一个<code>Class</code>实例，因为编译后它们全部都是<code>Pair&lt;Object&gt;</code>。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pair&lt;String&gt; p1 = <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;world&quot;</span>);</span><br><span class="line">Pair&lt;Integer&gt; p2 = <span class="keyword">new</span> Pair&lt;&gt;(<span class="number">123</span>, <span class="number">456</span>);</span><br><span class="line">Class c1 = p1.getClass();</span><br><span class="line">Class c2 = p2.getClass();</span><br><span class="line">System.out.println(c1==c2); <span class="comment">// true</span></span><br><span class="line">System.out.println(c1==Pair.class); <span class="comment">// true</span></span><br></pre></td></tr></table></figure></li>
<li><p>3）无法判断带泛型的类型，原因和前面一样，并不存在<code>Pair&lt;String&gt;.class</code>，而是只有唯一的<code>Pair.class</code>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pair&lt;Integer&gt; p = <span class="keyword">new</span> Pair&lt;&gt;(<span class="number">123</span>, <span class="number">456</span>);</span><br><span class="line"><span class="comment">// Compile error:</span></span><br><span class="line"><span class="keyword">if</span> (p <span class="keyword">instanceof</span> Pair&lt;String&gt;) &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>4）不能实例化<code>T</code>类型</p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> T first;</span><br><span class="line">    <span class="keyword">private</span> T last;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Pair</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// Compile error:</span></span><br><span class="line">        first = <span class="keyword">new</span> T();</span><br><span class="line">        last = <span class="keyword">new</span> T();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码无法通过编译，因为构造方法的两行语句：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">first = <span class="keyword">new</span> T();</span><br><span class="line">last = <span class="keyword">new</span> T();</span><br></pre></td></tr></table></figure>
<p>擦拭后实际上变成了：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">first = <span class="keyword">new</span> Object();</span><br><span class="line">last = <span class="keyword">new</span> Object();</span><br></pre></td></tr></table></figure>
<p>这样一来，创建<code>new Pair&lt;String&gt;()</code>和创建<code>new Pair&lt;Integer&gt;()</code>就全部成了<code>Object</code>，显然编译器要阻止这种类型不对的代码。</p>
<p>要实例化<code>T</code>类型，我们必须借助额外的<code>Class&lt;T&gt;</code>参数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> T first;</span><br><span class="line">    <span class="keyword">private</span> T last;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Pair</span><span class="params">(Class&lt;T&gt; clazz)</span> </span>&#123;</span><br><span class="line">        first = clazz.newInstance();</span><br><span class="line">        last = clazz.newInstance();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码借助<code>Class&lt;T&gt;</code>参数并通过反射来实例化<code>T</code>类型，使用的时候，也必须传入<code>Class&lt;T&gt;</code>。例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pair&lt;String&gt; pair = <span class="keyword">new</span> Pair&lt;&gt;(String.class);</span><br></pre></td></tr></table></figure>
<p>因为传入了<code>Class&lt;String&gt;</code>的实例，所以我们借助<code>String.class</code>就可以实例化<code>String</code>类型。</p>
<h4 id="不恰当的覆写方法">3、不恰当的覆写方法</h4>
<p>有些时候，一个看似正确定义的方法会无法通过编译。例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(T t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span> == t;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这是因为，定义的<code>equals(T t)</code>方法实际上会被擦拭成<code>equals(Object t)</code>，而这个方法是继承自<code>Object</code>的，编译器会阻止一个实际上会变成覆写的泛型方法定义。</p>
<p>换个方法名，避开与<code>Object.equals(Object)</code>的冲突就可以成功编译：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">same</span><span class="params">(T t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span> == t;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="泛型继承待补充">4、泛型继承（待补充</h4>
<h3 id="五extends通配符">五、Extends通配符</h3>
<h3 id="六super通配符">六、Super通配符</h3>
<h3 id="七泛型和反射">七、泛型和反射</h3>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记6——java注解</title>
    <url>/2022/01/31/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B06%E2%80%94java%E6%B3%A8%E8%A7%A3/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1264799402020448</p>
<h3 id="一注解是什么">一、注解是什么？</h3>
<p>​ 注解是放在Java源码的类、方法、字段、参数前的一种特殊“注释”：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// this is a component:</span></span><br><span class="line"><span class="meta">@Resource(&quot;hello&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Inject</span></span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@PostConstruct</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">hello</span><span class="params">(<span class="meta">@Param</span> String name)</span> </span>&#123;</span><br><span class="line">        System.out.println(name);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Hello&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ <strong>注释会被编译器直接忽略，注解则可以被编译器打包进入class文件</strong>，因此，注解是一种用作标注的“元数据”。</p>
<h3 id="二注解的作用">二、注解的作用：</h3>
<p>​ 从JVM的角度看，注解本身对代码逻辑没有任何影响，如何使用注解完全由工具决定。</p>
<p>​ Java的注解可以分为三类：</p>
<ul>
<li>1、<strong>由编译器使用的注解:</strong>( 这类注解不会被编译进入<code>.class</code>文件，它们在编译后就被编译器扔掉了 )
<ul>
<li><code>@Override</code>：让编译器检查该方法是否正确地实现了覆写；</li>
<li><code>@SuppressWarnings</code>：告诉编译器忽略此处代码产生的警告。</li>
</ul></li>
<li>2、<strong>由工具处理<code>.class</code>文件使用的注解</strong>:
<ul>
<li>比如有些工具会在加载class的时候，对class做动态修改，实现一些特殊的功能。<strong>这类注解会被编译进入<code>.class</code>文件，但加载结束后并不会存在于内存中</strong>。这类注解只被一些底层库使用，一般我们不必自己处理。</li>
</ul></li>
<li>3、<strong>在程序运行期能够读取的注解</strong>：
<ul>
<li>它们<strong>在加载后一直存在于JVM中</strong>，这也是最常用的注解。例如，一个配置了<code>@PostConstruct</code>的方法会在调用构造方法后自动被调用（这是Java代码读取该注解实现的功能，JVM并不会识别该注解）</li>
</ul></li>
</ul>
<h3 id="三注解的配置参数">三、注解的配置参数：</h3>
<p>定义一个注解时，还可以定义<strong>配置参数</strong>。配置参数可以包括：</p>
<ul>
<li>所有基本类型；</li>
<li>String；</li>
<li>枚举类型；</li>
<li>基本类型、String、Class以及枚举的数组。</li>
</ul>
<p>因为<strong>配置参数必须是常量</strong>，所以，上述限制保证了注解在定义时就已经确定了每个参数的值。</p>
<p><strong>注解的配置参数可以有默认值，缺少某个配置参数时将使用默认值。</strong></p>
<p>此外，大部分注解会有一个名为<code>value</code>的配置参数，对此参数赋值，可以只写常量，相当于省略了value参数。如果只写注解，相当于全部使用默认值。</p>
<p>举个栗子，对以下代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Check(min=0, max=100, value=55)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> n;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Check(value=99)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> p;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Check(99)</span> <span class="comment">// @Check(value=99)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> x;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Check</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> y;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>@Check</code>就是一个注解。第一个<code>@Check(min=0, max=100, value=55)</code>明确定义了三个参数，第二个<code>@Check(value=99)</code>只定义了一个<code>value</code>参数，它实际上和<code>@Check(99)</code>是完全一样的。最后一个<code>@Check</code>表示所有参数都使用默认值。</p>
<h3 id="四定义注解">四、定义注解：</h3>
<h4 id="如何定义一个注解">1、如何定义一个注解？</h4>
<p>java语言使用<code>@interface</code>语法来定义注解（<code>Annotation</code>），它的格式如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> Report &#123;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">type</span><span class="params">()</span> <span class="keyword">default</span> 0</span>;</span><br><span class="line">    <span class="function">String <span class="title">level</span><span class="params">()</span> <span class="keyword">default</span> &quot;info&quot;</span>;</span><br><span class="line">    <span class="function">String <span class="title">value</span><span class="params">()</span> <span class="keyword">default</span> &quot;&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 注解的参数类似无参数方法，可以用<code>default</code>设定一个默认值（强烈推荐）。最常用的参数应当命名为<code>value</code>。</p>
<h4 id="元注解">2、元注解：</h4>
<p>​ 有一些注解可以修饰其他注解，这些注解就称为元注解（meta annotation）。Java标准库已经定义了一些元注解，我们只需要使用元注解，通常不需要自己去编写元注解。</p>
<ul>
<li><p><span class="citation" data-cites="Target">@Target</span>：</p>
<ul>
<li><p>使用<code>@Target</code>可以定义<code>Annotation</code>能够被应用于源码的哪些位置</p>
<ul>
<li>类或接口：<code>ElementType.TYPE</code>；</li>
<li>字段：<code>ElementType.FIELD</code>；</li>
<li>方法：<code>ElementType.METHOD</code>；</li>
<li>构造方法：<code>ElementType.CONSTRUCTOR</code>；</li>
<li>方法参数：<code>ElementType.PARAMETER</code>。</li>
</ul></li>
<li><p>例如，定义注解<code>@Report</code>可用在方法上，我们必须添加一个<code>@Target(ElementType.METHOD)</code></p>
<ul>
<li>```java <span class="citation" data-cites="Target">@Target</span>(ElementType.METHOD) public <span class="citation" data-cites="interface">@interface</span> Report { int type() default 0; String level() default "info"; String value() default ""; } <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 定义注解`@Report`可用在方法或字段上，可以把`@Target`注解参数变为数组`&#123; ElementType.METHOD, ElementType.FIELD &#125;`：</span><br><span class="line"></span><br><span class="line">  - ```java</span><br><span class="line">    @Target(&#123;</span><br><span class="line">        ElementType.METHOD,</span><br><span class="line">        ElementType.FIELD</span><br><span class="line">    &#125;)</span><br><span class="line">    public @interface Report &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
<li><p><span class="citation" data-cites="Retention">@Retention</span>:</p>
<ul>
<li><p><code>@Retention</code>定义了<code>Annotation</code>的生命周期</p>
<ul>
<li>仅编译期：<code>RetentionPolicy.SOURCE</code>；</li>
<li>仅class文件：<code>RetentionPolicy.CLASS</code>；</li>
<li>运行期：<code>RetentionPolicy.RUNTIME</code>。</li>
</ul></li>
<li><p>如果<code>@Retention</code>不存在，则该<code>Annotation</code>默认为<code>CLASS</code>。因为通常我们自定义的<code>Annotation</code>都是<code>RUNTIME</code>，所以，务必要加上<code>@Retention(RetentionPolicy.RUNTIME)</code>这个元注解：</p>
<ul>
<li></li>
<li>```java <span class="citation" data-cites="Retention">@Retention</span>(RetentionPolicy.RUNTIME) public <span class="citation" data-cites="interface">@interface</span> Report { int type() default 0; String level() default "info"; String value() default ""; } <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- @Repeatable</span><br><span class="line"></span><br><span class="line">  - `@Repeatable`这个元注解可以定义`Annotation`是否可重复。</span><br><span class="line"></span><br><span class="line">    - ```</span><br><span class="line">      @Repeatable(Reports.class)</span><br><span class="line">      @Target(ElementType.TYPE)</span><br><span class="line">      public @interface Report &#123;</span><br><span class="line">          int type() default 0;</span><br><span class="line">          String level() default &quot;info&quot;;</span><br><span class="line">          String value() default &quot;&quot;;</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      @Target(ElementType.TYPE)</span><br><span class="line">      public @interface Reports &#123;</span><br><span class="line">          Report[] value();</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>经过<code>@Repeatable</code>修饰后，在某个类型声明处，就可以添加多个<code>@Report</code>注解：</p>
<ul>
<li>```java <span class="citation" data-cites="Report">@Report</span>(type=1, level="debug") <span class="citation" data-cites="Report">@Report</span>(type=2, level="warning") public class Hello { } <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- @Inherited</span><br><span class="line"></span><br><span class="line">  - 使用`@Inherited`定义子类是否可继承父类定义的`Annotation`</span><br><span class="line"></span><br><span class="line">  - `@Inherited`仅针对`@Target(ElementType.TYPE)`类型的`annotation`有效，**并且仅针对`class`的继承，对`interface`的继承无效**</span><br><span class="line"></span><br><span class="line">    - ```java</span><br><span class="line">      @Inherited</span><br><span class="line">      @Target(ElementType.TYPE)</span><br><span class="line">      public @interface Report &#123;</span><br><span class="line">          int type() default 0;</span><br><span class="line">          String level() default &quot;info&quot;;</span><br><span class="line">          String value() default &quot;&quot;;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>在使用的时候，如果一个类用到了<code>@Report</code>,则它的子类默认也定义了该注解：</p>
<ul>
<li>```java <span class="citation" data-cites="Report">@Report</span>(type=1) public class Person { } <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- ```java</span><br><span class="line">  public class Student extends Person &#123;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
</ul>
<h4 id="总结定义annotation的步骤">3、总结定义Annotation的步骤：</h4>
<h5 id="用interface定义注解">1）用<code>@interface</code>定义注解：</h5>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> Report &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="添加参数默认值把最常用的参数定义为value推荐所有参数都尽量设置默认值">2) 添加参数、默认值：(把最常用的参数定义为<code>value()</code>，推荐所有参数都尽量设置默认值。)</h5>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> Report &#123;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">type</span><span class="params">()</span> <span class="keyword">default</span> 0</span>;</span><br><span class="line">    <span class="function">String <span class="title">level</span><span class="params">()</span> <span class="keyword">default</span> &quot;info&quot;</span>;</span><br><span class="line">    <span class="function">String <span class="title">value</span><span class="params">()</span> <span class="keyword">default</span> &quot;&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="第三步用元注解配置注解">3) 第三步，用元注解配置注解：</h5>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Target(ElementType.TYPE)</span></span><br><span class="line"><span class="meta">@Retention(RetentionPolicy.RUNTIME)</span></span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> Report &#123;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">type</span><span class="params">()</span> <span class="keyword">default</span> 0</span>;</span><br><span class="line">    <span class="function">String <span class="title">level</span><span class="params">()</span> <span class="keyword">default</span> &quot;info&quot;</span>;</span><br><span class="line">    <span class="function">String <span class="title">value</span><span class="params">()</span> <span class="keyword">default</span> &quot;&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 必须设置<code>@Target</code>和<code>@Retention</code>，<code>@Retention</code>一般设置为<code>RUNTIME</code>，因为我们自定义的注解通常要求在运行期读取。一般情况下，不必写<code>@Inherited</code>和<code>@Repeatable</code>。</p>
<h3 id="五处理注解">五、处理注解：</h3>
<p>​ Java的注解本身对代码逻辑没有任何影响。根据<code>@Retention</code>的配置：</p>
<ul>
<li><code>SOURCE</code>类型的注解在编译期就被丢掉了；</li>
<li><code>CLASS</code>类型的注解仅保存在class文件中，它们不会被加载进JVM；</li>
<li><code>RUNTIME</code>类型的注解会被加载进JVM，并且在运行期可以被程序读取。</li>
</ul>
<p>​ 如何使用注解完全由工具决定。<code>SOURCE</code>类型的注解主要由编译器使用，因此我们一般只使用，不编写。<code>CLASS</code>类型的注解主要由底层工具库使用，涉及到class的加载，一般我们很少用到。<strong>只有<code>RUNTIME</code>类型的注解不但要使用，还经常需要编写。</strong></p>
<p>​ 因为注解定义后也是一种<code>class</code>，所有的注解都继承自<code>java.lang.annotation.Annotation</code>，因此，读取注解，需要使用反射API。</p>
<p>​ Java提供的使用反射API读取<code>Annotation</code>的方法包括：</p>
<p>判断某个注解是否存在于<code>Class</code>、<code>Field</code>、<code>Method</code>或<code>Constructor</code>：</p>
<ul>
<li><code>Class.isAnnotationPresent(Class)</code></li>
<li><code>Field.isAnnotationPresent(Class)</code></li>
<li><code>Method.isAnnotationPresent(Class)</code></li>
<li><code>Constructor.isAnnotationPresent(Class)</code></li>
</ul>
<p>例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 判断@Report是否存在于Person类:</span></span><br><span class="line">Person.class.isAnnotationPresent(Report.class);</span><br></pre></td></tr></table></figure>
<p>使用反射API读取Annotation：</p>
<ul>
<li><code>Class.getAnnotation(Class)</code></li>
<li><code>Field.getAnnotation(Class)</code></li>
<li><code>Method.getAnnotation(Class)</code></li>
<li><code>Constructor.getAnnotation(Class)</code></li>
</ul>
<p>例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取Person定义的@Report注解:</span></span><br><span class="line">Report report = Person.class.getAnnotation(Report.class);</span><br><span class="line"><span class="keyword">int</span> type = report.type();</span><br><span class="line">String level = report.level();</span><br></pre></td></tr></table></figure>
<p>使用反射API读取<code>Annotation</code>有两种方法。方法一是先判断<code>Annotation</code>是否存在，如果存在，就直接读取：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Class cls = Person.class;</span><br><span class="line"><span class="keyword">if</span> (cls.isAnnotationPresent(Report.class)) &#123;</span><br><span class="line">    Report report = cls.getAnnotation(Report.class);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第二种方法是直接读取<code>Annotation</code>，如果<code>Annotation</code>不存在，将返回<code>null</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Class cls = Person.class;</span><br><span class="line">Report report = cls.getAnnotation(Report.class);</span><br><span class="line"><span class="keyword">if</span> (report != <span class="keyword">null</span>) &#123;</span><br><span class="line">   ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>读取方法、字段和构造方法的<code>Annotation</code>和Class类似。但要读取方法参数的<code>Annotation</code>就比较麻烦一点，因为方法参数本身可以看成一个数组，而每个参数又可以定义多个注解，所以，一次获取方法参数的所有注解就必须用一个二维数组来表示。例如，对于以下方法定义的注解：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">hello</span><span class="params">(<span class="meta">@NotNull</span> <span class="meta">@Range(max=5)</span> String name, <span class="meta">@NotNull</span> String prefix)</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>要读取方法参数的注解，我们先用反射获取<code>Method</code>实例，然后读取方法参数的所有注解：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取Method实例:</span></span><br><span class="line">Method m = ...</span><br><span class="line"><span class="comment">// 获取所有参数的Annotation:</span></span><br><span class="line">Annotation[][] annos = m.getParameterAnnotations();</span><br><span class="line"><span class="comment">// 第一个参数（索引为0）的所有Annotation:</span></span><br><span class="line">Annotation[] annosOfName = annos[<span class="number">0</span>];</span><br><span class="line"><span class="keyword">for</span> (Annotation anno : annosOfName) &#123;</span><br><span class="line">    <span class="keyword">if</span> (anno <span class="keyword">instanceof</span> Range) &#123; <span class="comment">// @Range注解</span></span><br><span class="line">        Range r = (Range) anno;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (anno <span class="keyword">instanceof</span> NotNull) &#123; <span class="comment">// @NotNull注解</span></span><br><span class="line">        NotNull n = (NotNull) anno;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="六使用注解">六、使用注解</h3>
<p>​ 注解如何使用，完全由程序自己决定。例如，JUnit是一个测试框架，它会自动运行所有标记为<code>@Test</code>的方法。</p>
<p>​ 我们来看一个<code>@Range</code>注解，我们希望用它来定义一个<code>String</code>字段的规则：字段长度满足<code>@Range</code>的参数定义：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Retention(RetentionPolicy.RUNTIME)</span></span><br><span class="line"><span class="meta">@Target(ElementType.FIELD)</span></span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> Range &#123;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">min</span><span class="params">()</span> <span class="keyword">default</span> 0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">max</span><span class="params">()</span> <span class="keyword">default</span> 255</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 在某个JavaBean中，我们可以使用该注解：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Range(min=1, max=20)</span></span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Range(max=10)</span></span><br><span class="line">    <span class="keyword">public</span> String city;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 但是，定义了注解，本身对程序逻辑没有任何影响。我们必须自己编写代码来使用注解。这里，我们编写一个<code>Person</code>实例的检查方法，它可以检查<code>Person</code>实例的<code>String</code>字段长度是否满足<code>@Range</code>的定义：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">check</span><span class="params">(Person person)</span> <span class="keyword">throws</span> IllegalArgumentException, ReflectiveOperationException </span>&#123;</span><br><span class="line">    <span class="comment">// 遍历所有Field:</span></span><br><span class="line">    <span class="keyword">for</span> (Field field : person.getClass().getFields()) &#123;</span><br><span class="line">        <span class="comment">// 获取Field定义的@Range:</span></span><br><span class="line">        Range range = field.getAnnotation(Range.class);</span><br><span class="line">        <span class="comment">// 如果@Range存在:</span></span><br><span class="line">        <span class="keyword">if</span> (range != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 获取Field的值:</span></span><br><span class="line">            Object value = field.get(person);</span><br><span class="line">            <span class="comment">// 如果值是String:</span></span><br><span class="line">            <span class="keyword">if</span> (value <span class="keyword">instanceof</span> String) &#123;</span><br><span class="line">                String s = (String) value;</span><br><span class="line">                <span class="comment">// 判断值是否满足@Range的min/max:</span></span><br><span class="line">                <span class="keyword">if</span> (s.length() &lt; range.min() || s.length() &gt; range.max()) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Invalid field: &quot;</span> + field.getName());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 这样一来，我们通过<code>@Range</code>注解，配合<code>check()</code>方法，就可以完成<code>Person</code>实例的检查。注意检查逻辑完全是我们自己编写的，JVM不会自动给注解添加任何额外的逻辑。</p>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>annotation</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记5——java反射机制</title>
    <url>/2022/01/30/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B05%E2%80%94java%E5%8F%8D%E5%B0%84/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1255945147512512</p>
<h3 id="一反射简介">一、反射简介</h3>
<p>Java的反射是指程序在运行期可以拿到一个对象的所有信息。</p>
<p>​ 反射是为了解决在运行期，对某个实例一无所知的情况下，如何调用其方法。正常情况下，如果我们要调用一个对象的方法，或者访问一个对象的字段，通常会传入对象实例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Main.java</span></span><br><span class="line"><span class="keyword">import</span> com.itranswarp.learnjava.Person;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function">String <span class="title">getFullName</span><span class="params">(Person p)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> p.getFirstName() + <span class="string">&quot; &quot;</span> + p.getLastName();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但是，如果不能获得<code>Person</code>类，只有一个<code>Object</code>实例，比如这样：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">String <span class="title">getFullName</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>怎么办？可能有人说：强制转型啊！</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">String <span class="title">getFullName</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    Person p = (Person) obj;</span><br><span class="line">    <span class="keyword">return</span> p.getFirstName() + <span class="string">&quot; &quot;</span> + p.getLastName();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 强制转型的时候，你会发现一个问题：编译上面的代码，仍然需要引用<code>Person</code>类。不然，去掉<code>import</code>语句，你看能不能编译通过？</p>
<p>​ <strong>所以，反射是为了解决在运行期，对某个实例一无所知的情况下，如何调用其方法。</strong></p>
<h3 id="二class类">二、Class类</h3>
<p>​ 查看此节前，需要注意：我们平时所说的类都是指“class”(小写)，本节要介绍的是一个叫做<strong>"Class"</strong>的类。</p>
<h4 id="简介">1、简介</h4>
<p>​ 我们平常定义的各个类都是<strong>由JVM在执行过程中动态加载的</strong>。JVM在第一次读取到一个不同的类时，就会将其加载入内存。每加载一种类，JVM就会为其创建一个<strong>"Class"</strong>类型的类的实例，并进行关联。这个叫做<strong>“Class”</strong>的类定义如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Class</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Class</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 以<code>String</code>类为例，当JVM加载<code>String</code>类时，它首先读取<code>String.class</code>文件到内存，然后，为<code>String</code>类创建一个<strong>“Class”</strong>类的实例并关联起来：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Class cls = <span class="keyword">new</span> Class(String);</span><br></pre></td></tr></table></figure>
<p>​ 这个<strong>“Class”</strong>类的实例实例是JVM内部创建的，如果我们查看JDK源码，可以发现<strong>"Class"</strong>类的构造方法是<code>private</code>，只有JVM能创建<strong>"Class"</strong>类的实例，我们自己的Java程序是无法创建<strong>"Class"</strong>实例的。</p>
<p>​ 所以，JVM持有的每个<strong>"Class"</strong>实例都指向一个数据类型（<code>class</code>或<code>interface</code>），如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">┌───────────────────────────┐</span><br><span class="line">│      Class Instance       │──────&gt; String</span><br><span class="line">├───────────────────────────┤</span><br><span class="line">│name = &quot;java.lang.String&quot;  │</span><br><span class="line">└───────────────────────────┘</span><br><span class="line">┌───────────────────────────┐</span><br><span class="line">│      Class Instance       │──────&gt; Random</span><br><span class="line">├───────────────────────────┤</span><br><span class="line">│name = &quot;java.util.Random&quot;  │</span><br><span class="line">└───────────────────────────┘</span><br><span class="line">┌───────────────────────────┐</span><br><span class="line">│      Class Instance       │──────&gt; Runnable</span><br><span class="line">├───────────────────────────┤</span><br><span class="line">│name = &quot;java.lang.Runnable&quot;│</span><br><span class="line">└───────────────────────────┘</span><br></pre></td></tr></table></figure>
<p>一个<strong>"Class"</strong>实例包含了其对应的类的所有完整信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">┌───────────────────────────┐</span><br><span class="line">│      Class Instance       │──────&gt; String</span><br><span class="line">├───────────────────────────┤</span><br><span class="line">│name = &quot;java.lang.String&quot;  │</span><br><span class="line">├───────────────────────────┤</span><br><span class="line">│package = &quot;java.lang&quot;      │</span><br><span class="line">├───────────────────────────┤</span><br><span class="line">│super = &quot;java.lang.Object&quot; │</span><br><span class="line">├───────────────────────────┤</span><br><span class="line">│interface = CharSequence...│</span><br><span class="line">├───────────────────────────┤</span><br><span class="line">│field = value[],hash,...   │</span><br><span class="line">├───────────────────────────┤</span><br><span class="line">│method = indexOf()...      │</span><br><span class="line">└───────────────────────────┘</span><br></pre></td></tr></table></figure>
<p>​ 由于JVM为每个加载的类创建了对应的<strong>"Class"</strong>实例，并在实例中保存了该类的所有信息，包括类名、包名、父类、实现的接口、所有方法、字段等，因此，如果获取了某个<strong>"Class"</strong>实例，我们就可以通过这个<strong>"Class"</strong>实例获取到该实例对应的类的所有信息。</p>
<p>​ 这种通过<strong>"Class"</strong>的实例获取类的信息的方法称为<strong>反射（Reflection）</strong>。</p>
<h4 id="如何获取一个类的class实例">2、如何获取一个类的<strong>"Class"</strong>实例</h4>
<ul>
<li>方法1：直接通过一个类的静态变量<code>class</code>获取：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Class cls = String.class;</span><br></pre></td></tr></table></figure>
<ul>
<li>方法二：如果我们有一个实例变量，可以通过该实例变量提供的<code>getClass()</code>方法获取：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s = <span class="string">&quot;Hello&quot;</span>;</span><br><span class="line">Class cls = s.getClass();</span><br></pre></td></tr></table></figure>
<ul>
<li>方法三：如果知道一个类的完整类名，可以通过静态方法<code>Class.forName()</code>获取：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Class cls = Class.forName(<span class="string">&quot;java.lang.String&quot;</span>);</span><br></pre></td></tr></table></figure>
<h4 id="比较class实例">3、比较”Class“实例</h4>
<p>​ 因为<strong>"Class"</strong>实例在JVM中是唯一的，所以，上述方法获取的<strong>"Class"</strong>实例是同一个实例。可以用<code>==</code>比较两个<strong>"Class"</strong>实例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Class cls1 = String.class;</span><br><span class="line"></span><br><span class="line">String s = <span class="string">&quot;Hello&quot;</span>;</span><br><span class="line">Class cls2 = s.getClass();</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> sameClass = cls1 == cls2; <span class="comment">// true</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：<strong>"Class"</strong>实例比较和<strong>instanceof</strong>的差别：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer n = <span class="keyword">new</span> Integer(<span class="number">123</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> b1 = n <span class="keyword">instanceof</span> Integer; <span class="comment">// true，因为n是Integer类型</span></span><br><span class="line"><span class="keyword">boolean</span> b2 = n <span class="keyword">instanceof</span> Number; <span class="comment">// true，因为n是Number类型的子类</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> b3 = n.getClass() == Integer.class; <span class="comment">// true，因为n.getClass()返回Integer.class</span></span><br><span class="line"><span class="keyword">boolean</span> b4 = n.getClass() == Number.class; <span class="comment">// false，因为Integer.class!=Number.class</span></span><br></pre></td></tr></table></figure>
<p>​ 用<code>instanceof</code>不但匹配指定类型，还匹配指定类型的子类。而用<code>==</code>判断<code>class</code>实例可以精确地判断数据类型，但不能作子类型比较。</p>
<p>​ 通常情况下，我们应该用<code>instanceof</code>判断数据类型，因为面向抽象编程的时候，我们不关心具体的子类型。只有在需要精确判断一个类型是不是某个<code>class</code>的时候，我们才使用<code>==</code>判断<code>class</code>实例。</p>
<h4 id="如何从class实例获取基本信息">4、如何从Class实例获取基本信息</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">printObjectInfo</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    Class cls = obj.getClass();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">printClassInfo</span><span class="params">(Class cls)</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;Class name: &quot;</span> + cls.getName());</span><br><span class="line">    System.out.println(<span class="string">&quot;Simple name: &quot;</span> + cls.getSimpleName());</span><br><span class="line">    <span class="keyword">if</span> (cls.getPackage() != <span class="keyword">null</span>) &#123;</span><br><span class="line">    	System.out.println(<span class="string">&quot;Package name: &quot;</span> + cls.getPackage().getName());</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">&quot;is interface: &quot;</span> + cls.isInterface());</span><br><span class="line">    System.out.println(<span class="string">&quot;is enum: &quot;</span> + cls.isEnum());</span><br><span class="line">    System.out.println(<span class="string">&quot;is array: &quot;</span> + cls.isArray());</span><br><span class="line">    System.out.println(<span class="string">&quot;is primitive: &quot;</span> + cls.isPrimitive());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="动态加载">5、动态加载：</h4>
<p>​ JVM在执行Java程序的时候，<strong>并不是一次性把所有用到的class全部加载到内存，而是第一次需要用到class时才加载</strong>。例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Main.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            create(args[<span class="number">0</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        Person p = <span class="keyword">new</span> Person(name);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 当执行<code>Main.java</code>时，由于用到了<code>Main</code>，因此，JVM首先会把<code>Main.class</code>加载到内存。然而，并不会加载<code>Person.class</code>，除非程序执行到<code>create()</code>方法，JVM发现需要加载<code>Person</code>类时，才会首次加载<code>Person.class</code>。如果没有执行<code>create()</code>方法，那么<code>Person.class</code>根本就不会被加载。</p>
<p>​ 这就是<strong>JVM动态加载<code>class</code>的特性</strong>。</p>
<p>​ <strong>备注</strong>：动态加载类的特性对于Java程序非常重要。利用JVM动态加载类的特性，我们才能在运行期根据条件加载不同的实现类。例如，Commons Logging总是优先使用Log4j，只有当Log4j不存在时，才使用JDK的logging。利用JVM动态加载特性，大致的实现代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Commons Logging优先使用Log4j:</span></span><br><span class="line">LogFactory factory = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">if</span> (isClassPresent(<span class="string">&quot;org.apache.logging.log4j.Logger&quot;</span>)) &#123;</span><br><span class="line">    factory = createLog4j();</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    factory = createJdkLog();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">isClassPresent</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        Class.forName(name);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 这就是为什么我们只需要把Log4j的jar包放到classpath中，Commons Logging就会自动使用Log4j的原因。</p>
<h3 id="三通过class实例访问字段">三、通过Class实例访问字段：</h3>
<h4 id="获取字段信息">1、获取字段信息</h4>
<p>​ 对任意的一个<code>Object</code>实例，只要我们获取了它的<strong>"Class"</strong>，就可以获取它的一切信息。</p>
<p>​ <strong>"Class"</strong>类提供了以下几个方法来获取字段Field：</p>
<ul>
<li>Field getField(name)：根据字段名获取某个public的field（包括父类）</li>
<li>Field getDeclaredField(name)：根据字段名获取当前类的某个field（不包括父类）</li>
<li>Field[] getFields()：获取所有public的field（包括父类）</li>
<li>Field[] getDeclaredFields()：获取当前类的所有field（不包括父类）</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Class stdClass = Student.class;</span><br><span class="line">        <span class="comment">// 获取public字段&quot;score&quot;:</span></span><br><span class="line">        System.out.println(stdClass.getField(<span class="string">&quot;score&quot;</span>));</span><br><span class="line">        <span class="comment">// 获取继承的public字段&quot;name&quot;:</span></span><br><span class="line">        System.out.println(stdClass.getField(<span class="string">&quot;name&quot;</span>));</span><br><span class="line">        <span class="comment">// 获取private字段&quot;grade&quot;:</span></span><br><span class="line">        System.out.println(stdClass.getDeclaredField(<span class="string">&quot;grade&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> score;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> grade;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/** 上述输出 </span></span><br><span class="line"><span class="comment">public int Student.score</span></span><br><span class="line"><span class="comment">public java.lang.String Person.name</span></span><br><span class="line"><span class="comment">private int Student.grade</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<p>一个<code>Field</code>对象包含了一个字段的所有信息：</p>
<ul>
<li><code>getName()</code>：返回字段名称，例如，<code>"name"</code>；</li>
<li><code>getType()</code>：返回字段类型，也是一个<code>Class</code>实例，例如，<code>String.class</code>；</li>
<li><code>getModifiers()</code>：返回字段的修饰符，它是一个<code>int</code>，不同的bit表示不同的含义。</li>
</ul>
<h4 id="获取字段值">2、获取字段值：</h4>
<p>​ 利用反射拿到字段的一个<code>Field</code>实例只是第一步，我们还可以拿到一个实例对应的该字段的值。</p>
<p>​ 例如，对于一个<code>Person</code>实例，我们可以先拿到<code>name</code>字段对应的<code>Field</code>，再获取这个实例的<code>name</code>字段的值：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Object p = <span class="keyword">new</span> Person(<span class="string">&quot;Xiao Ming&quot;</span>);</span><br><span class="line">        Class c = p.getClass();</span><br><span class="line">        Field f = c.getDeclaredField(<span class="string">&quot;name&quot;</span>);</span><br><span class="line">        Object value = f.get(p);</span><br><span class="line">        System.out.println(value); <span class="comment">// &quot;Xiao Ming&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 上述代码先获取<code>Class</code>实例，再获取<code>Field</code>实例，然后，用<code>Field.get(Object)</code>获取指定实例的指定字段的值。</p>
<p>​ 运行代码，如果不出意外，会得到一个<code>IllegalAccessException</code>，这是因为<code>name</code>被定义为一个<code>private</code>字段，正常情况下，<code>Main</code>类无法访问<code>Person</code>类的<code>private</code>字段。要修复错误，可以将<code>private</code>改为<code>public</code>，或者，在调用<code>Object value = f.get(p);</code>前，先写一句：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">f.setAccessible(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>
<p>调用<code>Field.setAccessible(true)</code>的意思是，别管这个字段是不是<code>public</code>，一律允许访问。</p>
<h4 id="设置字段值">3、设置字段值：</h4>
<p>​ 通过Field实例既然可以获取到指定实例的字段值，自然也可以设置字段的值。</p>
<p>​ 设置字段值是通过<code>Field.set(Object, Object)</code>实现的，其中第一个<code>Object</code>参数是指定的实例，第二个<code>Object</code>参数是待修改的值。示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Person p = <span class="keyword">new</span> Person(<span class="string">&quot;Xiao Ming&quot;</span>);</span><br><span class="line">        System.out.println(p.getName()); <span class="comment">// &quot;Xiao Ming&quot;</span></span><br><span class="line">        Class c = p.getClass();</span><br><span class="line">        Field f = c.getDeclaredField(<span class="string">&quot;name&quot;</span>);</span><br><span class="line">        f.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">        f.set(p, <span class="string">&quot;Xiao Hong&quot;</span>);</span><br><span class="line">        System.out.println(p.getName()); <span class="comment">// &quot;Xiao Hong&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样的，修改非<code>public</code>字段，需要首先调用<code>setAccessible(true)</code>。</p>
<h3 id="四调用方法">四、调用方法：</h3>
<h4 id="简介-1">1、简介</h4>
<p>可以通过<code>Class</code>实例获取所有<code>Method</code>信息。<code>Class</code>类提供了以下几个方法来获取<code>Method</code>：</p>
<ul>
<li><code>Method getMethod(name, Class...)</code>：获取某个<code>public</code>的<code>Method</code>（包括父类）</li>
<li><code>Method getDeclaredMethod(name, Class...)</code>：获取当前类的某个<code>Method</code>（不包括父类）</li>
<li><code>Method[] getMethods()</code>：获取所有<code>public</code>的<code>Method</code>（包括父类）</li>
<li><code>Method[] getDeclaredMethods()</code>：获取当前类的所有<code>Method</code>（不包括父类）</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Class stdClass = Student.class;</span><br><span class="line">        <span class="comment">// 获取public方法getScore，参数为String:</span></span><br><span class="line">        System.out.println(stdClass.getMethod(<span class="string">&quot;getScore&quot;</span>, String.class));</span><br><span class="line">        <span class="comment">// 获取继承的public方法getName，无参数:</span></span><br><span class="line">        System.out.println(stdClass.getMethod(<span class="string">&quot;getName&quot;</span>));</span><br><span class="line">        <span class="comment">// 获取private方法getGrade，参数为int:</span></span><br><span class="line">        System.out.println(stdClass.getDeclaredMethod(<span class="string">&quot;getGrade&quot;</span>, <span class="keyword">int</span>.class));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getScore</span><span class="params">(String type)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">99</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">getGrade</span><span class="params">(<span class="keyword">int</span> year)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Person&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">public int Student.getScore(java.lang.String)</span></span><br><span class="line"><span class="comment">public java.lang.String Person.getName()</span></span><br><span class="line"><span class="comment">private int Student.getGrade(int)</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<p>一个<code>Method</code>对象包含一个方法的所有信息：</p>
<ul>
<li><code>getName()</code>：返回方法名称，例如：<code>"getScore"</code>；</li>
<li><code>getReturnType()</code>：返回方法返回值类型，也是一个Class实例，例如：<code>String.class</code>；</li>
<li><code>getParameterTypes()</code>：返回方法的参数类型，是一个Class数组，例如：<code>&#123;String.class, int.class&#125;</code>；</li>
<li><code>getModifiers()</code>：返回方法的修饰符，它是一个<code>int</code>，不同的bit表示不同的含义。</li>
</ul>
<h4 id="调用方法">2、调用方法：</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">String s = &quot;Hello world&quot;;</span><br><span class="line">String r = s.substring(6); // &quot;world&quot;</span><br></pre></td></tr></table></figure>
<p>​ 如果用反射来用<code>substring</code>方法，需要以下代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// String对象:</span></span><br><span class="line">        String s = <span class="string">&quot;Hello world&quot;</span>;</span><br><span class="line">        <span class="comment">// 获取String substring(int)方法，参数为int:</span></span><br><span class="line">        Method m = String.class.getMethod(<span class="string">&quot;substring&quot;</span>, <span class="keyword">int</span>.class);</span><br><span class="line">        <span class="comment">// 在s对象上调用该方法并获取结果:</span></span><br><span class="line">        String r = (String) m.invoke(s, <span class="number">6</span>);</span><br><span class="line">        <span class="comment">// 打印调用结果:</span></span><br><span class="line">        System.out.println(r);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 对<code>Method</code>实例调用<code>invoke</code>就相当于调用该方法，<code>invoke</code>的第一个参数是对象实例，即在哪个实例上调用该方法，后面的可变参数要与方法参数一致，否则将报错。</p>
<h4 id="调用静态方法">3、调用静态方法：</h4>
<p>​ 如果获取到的Method表示一个静态方法，调用静态方法时，由于无需指定实例对象，所以<code>invoke</code>方法传入的第一个参数永远为<code>null</code>。我们以<code>Integer.parseInt(String)</code>为例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 获取Integer.parseInt(String)方法，参数为String:</span></span><br><span class="line">        Method m = Integer.class.getMethod(<span class="string">&quot;parseInt&quot;</span>, String.class);</span><br><span class="line">        <span class="comment">// 调用该静态方法并获取结果:</span></span><br><span class="line">        Integer n = (Integer) m.invoke(<span class="keyword">null</span>, <span class="string">&quot;12345&quot;</span>);</span><br><span class="line">        <span class="comment">// 打印调用结果:</span></span><br><span class="line">        System.out.println(n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="调用非public方法">4、调用非public方法：</h4>
<p>为了调用非public方法，我们通过<code>Method.setAccessible(true)</code>允许其调用：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Person p = <span class="keyword">new</span> Person();</span><br><span class="line">        Method m = p.getClass().getDeclaredMethod(<span class="string">&quot;setName&quot;</span>, String.class);</span><br><span class="line">        m.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">        m.invoke(p, <span class="string">&quot;Bob&quot;</span>);</span><br><span class="line">        System.out.println(p.name);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    String name;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="五调用构造方法">五、调用构造方法：</h3>
<p>我们通常使用<code>new</code>操作符创建新的实例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Person p = new Person();</span><br></pre></td></tr></table></figure>
<p>如果通过反射来创建新的实例，可以调用Class提供的newInstance()方法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Person p = Person.class.newInstance();</span><br></pre></td></tr></table></figure>
<p>​ 调用Class.newInstance()的局限是，<strong>它只能调用该类的public无参数构造方法</strong>。如果构造方法带有参数，或者不是public，就无法直接通过Class.newInstance()来调用。</p>
<p>​ 为了调用任意的构造方法，Java的<strong>反射API提供了Constructor对象</strong>，它包含一个构造方法的所有信息，可以创建一个实例。Constructor对象和Method非常类似，不同之处仅在于它是一个构造方法，并且，调用结果总是返回实例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 获取构造方法Integer(int):</span></span><br><span class="line">        Constructor cons1 = Integer.class.getConstructor(<span class="keyword">int</span>.class);</span><br><span class="line">        <span class="comment">// 调用构造方法:</span></span><br><span class="line">        Integer n1 = (Integer) cons1.newInstance(<span class="number">123</span>);</span><br><span class="line">        System.out.println(n1);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取构造方法Integer(String)</span></span><br><span class="line">        Constructor cons2 = Integer.class.getConstructor(String.class);</span><br><span class="line">        Integer n2 = (Integer) cons2.newInstance(<span class="string">&quot;456&quot;</span>);</span><br><span class="line">        System.out.println(n2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过Class实例获取Constructor的方法如下：</p>
<ul>
<li><code>getConstructor(Class...)</code>：获取某个<code>public</code>的<code>Constructor</code>；</li>
<li><code>getDeclaredConstructor(Class...)</code>：获取某个<code>Constructor</code>；</li>
<li><code>getConstructors()</code>：获取所有<code>public</code>的<code>Constructor</code>；</li>
<li><code>getDeclaredConstructors()</code>：获取所有<code>Constructor</code>。</li>
</ul>
<p>​ 注意<code>Constructor</code>总是当前类定义的构造方法，和父类无关，因此不存在多态的问题。</p>
<p>​ 调用非<code>public</code>的<code>Constructor</code>时，必须首先通过<code>setAccessible(true)</code>设置允许访问。<code>setAccessible(true)</code>可能会失败。</p>
<h3 id="六获取继承关系">六、获取继承关系：</h3>
<h4 id="获取父类的class">1、获取父类的Class</h4>
<p>​ 有了某个类的<code>Class</code>实例，我们还可以获取它的父类的<code>Class</code>：如下，<code>Integer</code>的父类类型是<code>Number</code>，<code>Number</code>的父类是<code>Object</code>，<code>Object</code>的父类是<code>null</code>。除<code>Object</code>外，其他任何非<code>interface</code>的<code>Class</code>都必定存在一个父类类型。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Class i = Integer.class;</span><br><span class="line">        Class n = i.getSuperclass();</span><br><span class="line">        System.out.println(n);</span><br><span class="line">        Class o = n.getSuperclass();</span><br><span class="line">        System.out.println(o);</span><br><span class="line">        System.out.println(o.getSuperclass());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">class java.lang.Number</span></span><br><span class="line"><span class="comment">class java.lang.Object</span></span><br><span class="line"><span class="comment">null </span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<h4 id="获取interface">2、获取Interface</h4>
<p>​ 由于一个类可能实现一个或多个接口，通过<code>Class</code>我们就可以查询到实现的接口类型。例如，查询<code>Integer</code>实现的接口：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Class s = Integer.class;</span><br><span class="line">        Class[] is = s.getInterfaces();</span><br><span class="line">        <span class="keyword">for</span> (Class i : is) &#123;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">interface java.lang.Comparable</span></span><br><span class="line"><span class="comment">interface java.lang.constant.Constable</span></span><br><span class="line"><span class="comment">interface java.lang.constant.ConstantDesc </span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<p>运行上述代码可知，<code>Integer</code>实现的接口有：</p>
<ul>
<li>java.lang.Comparable</li>
<li>java.lang.constant.Constable</li>
<li>java.lang.constant.ConstantDesc</li>
</ul>
<p>要特别注意：</p>
<ul>
<li><p><code>getInterfaces()</code>只返回当前类直接实现的接口类型，并不包括其父类实现的接口类型：</p></li>
<li><p>对所有<code>interface</code>的<code>Class</code>调用<code>getSuperclass()</code>返回的是<code>null</code></p></li>
<li><p>如果一个类没有实现任何<code>interface</code>，那么<code>getInterfaces()</code>返回空数组。</p></li>
</ul>
<h4 id="继承关系">3、继承关系</h4>
<ul>
<li>当我们判断一个实例是否是某个类型时，正常情况下，使用<code>instanceof</code>操作符：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Object n = Integer.valueOf(<span class="number">123</span>);</span><br><span class="line"><span class="keyword">boolean</span> isDouble = n <span class="keyword">instanceof</span> Double; <span class="comment">// false</span></span><br><span class="line"><span class="keyword">boolean</span> isInteger = n <span class="keyword">instanceof</span> Integer; <span class="comment">// true</span></span><br><span class="line"><span class="keyword">boolean</span> isNumber = n <span class="keyword">instanceof</span> Number; <span class="comment">// true</span></span><br><span class="line"><span class="keyword">boolean</span> isSerializable = n <span class="keyword">instanceof</span> java.io.Serializable; <span class="comment">// true</span></span><br></pre></td></tr></table></figure>
<ul>
<li>如果是两个<code>Class</code>实例，要判断一个向上转型是否成立，可以调用<code>isAssignableFrom()</code>：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Integer i = ?</span></span><br><span class="line">Integer.class.isAssignableFrom(Integer.class); <span class="comment">// true，因为Integer可以赋值给Integer</span></span><br><span class="line"><span class="comment">// Number n = ?</span></span><br><span class="line">Number.class.isAssignableFrom(Integer.class); <span class="comment">// true，因为Integer可以赋值给Number</span></span><br><span class="line"><span class="comment">// Object o = ?</span></span><br><span class="line">Object.class.isAssignableFrom(Integer.class); <span class="comment">// true，因为Integer可以赋值给Object</span></span><br><span class="line"><span class="comment">// Integer i = ?</span></span><br><span class="line">Integer.class.isAssignableFrom(Number.class); <span class="comment">// false，因为Number不能赋值给Integer</span></span><br></pre></td></tr></table></figure>
<h3 id="七动态代理">七、动态代理：</h3>
<p>​ ? 未完待续</p>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>reflection</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记4——java异常处理</title>
    <url>/2022/01/30/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B04%E2%80%94java%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1264734349295520</p>
<h3 id="一java异常简介">一、Java异常简介：</h3>
<h4 id="异常介绍与分类">1、异常介绍与分类</h4>
<p>所以，一个健壮的程序必须处理各种各样的错误。</p>
<p>调用方如何获知调用失败的信息？有两种方法：</p>
<ul>
<li>方法1：约定返回错误码，常见于C</li>
<li>方法2：在语言层面上提供异常处理机制：（较为常用）</li>
</ul>
<p><strong>异常是一种<code>class</code>，因此它本身带有类型信息。异常可以在任何地方抛出，但只需要在上层捕获，这样就和方法调用分离了</strong></p>
<p>Java的异常是<code>class</code>，它的继承关系如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                     ┌───────────┐</span><br><span class="line">                     │  Object   │</span><br><span class="line">                     └───────────┘</span><br><span class="line">                           ▲</span><br><span class="line">                           │</span><br><span class="line">                     ┌───────────┐</span><br><span class="line">                     │ Throwable │</span><br><span class="line">                     └───────────┘</span><br><span class="line">                           ▲</span><br><span class="line">                 ┌─────────┴─────────┐</span><br><span class="line">                 │                   │</span><br><span class="line">           ┌───────────┐       ┌───────────┐</span><br><span class="line">           │   Error   │       │ Exception │</span><br><span class="line">           └───────────┘       └───────────┘</span><br><span class="line">                 ▲                   ▲</span><br><span class="line">         ┌───────┘              ┌────┴──────────┐</span><br><span class="line">         │                      │               │</span><br><span class="line">┌─────────────────┐    ┌─────────────────┐┌───────────┐</span><br><span class="line">│OutOfMemoryError │... │RuntimeException ││IOException│...</span><br><span class="line">└─────────────────┘    └─────────────────┘└───────────┘</span><br><span class="line">                                ▲</span><br><span class="line">                    ┌───────────┴─────────────┐</span><br><span class="line">                    │                         │</span><br><span class="line">         ┌─────────────────────┐ ┌─────────────────────────┐</span><br><span class="line">         │NullPointerException │ │IllegalArgumentException │...</span><br><span class="line">         └─────────────────────┘ └─────────────────────────┘</span><br></pre></td></tr></table></figure>
<p>从上图继承关系可知：<code>Throwable</code>是异常体系的根，它继承自<code>Object</code>。<code>Throwable</code>有两个体系：<code>Error</code>和<code>Exception</code>，<code>Error</code>表示严重的错误，程序对此一般无能为力，例如：</p>
<ul>
<li><code>OutOfMemoryError</code>：内存耗尽</li>
<li><code>NoClassDefFoundError</code>：无法加载某个Class</li>
<li><code>StackOverflowError</code>：栈溢出</li>
</ul>
<p>而<code>Exception</code>则是运行时的错误，它可以被捕获并处理。</p>
<ul>
<li><p>某些异常是应用程序逻辑处理的一部分，应该捕获并处理。例如：</p>
<ul>
<li><p><code>NumberFormatException</code>：数值类型的格式错误</p></li>
<li><p><code>FileNotFoundException</code>：未找到文件</p></li>
<li><p><code>SocketException</code>：读取网络失败</p></li>
</ul></li>
<li><p>还有一些异常是程序逻辑编写不对造成的，应该修复程序本身。例如：</p>
<ul>
<li><p><code>NullPointerException</code>：对某个<code>null</code>的对象调用方法或字段</p></li>
<li><p><code>IndexOutOfBoundsException</code>：数组索引越界</p></li>
</ul></li>
</ul>
<p><code>Exception</code>又分为两大类：</p>
<ol type="1">
<li><code>RuntimeException</code>以及它的子类；</li>
<li>非<code>RuntimeException</code>（包括<code>IOException</code>、<code>ReflectiveOperationException</code>等等）</li>
</ol>
<p><strong>Java规定</strong>：</p>
<ul>
<li>必须捕获的异常，包括<code>Exception</code>及其子类，但不包括<code>RuntimeException</code>及其子类，这种类型的异常称为<strong>Checked Exception</strong>。</li>
<li>不需要捕获的异常，包括<code>Error</code>及其子类，<code>RuntimeException</code>及其子类。</li>
</ul>
<h3 id="二捕获异常">二、捕获异常：</h3>
<h4 id="普通的捕获异常">1、普通的捕获异常：</h4>
<p>​ 捕获异常使用<code>try...catch</code>语句，把可能发生异常的代码放到<code>try &#123;...&#125;</code>中，然后使用<code>catch</code>捕获对应的<code>Exception</code>及其子类：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.UnsupportedEncodingException;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">byte</span>[] bs = toGBK(<span class="string">&quot;中文&quot;</span>);</span><br><span class="line">        System.out.println(Arrays.toString(bs));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">byte</span>[] toGBK(String s) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 用指定编码转换String为byte[]:</span></span><br><span class="line">            <span class="keyword">return</span> s.getBytes(<span class="string">&quot;GBK&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</span><br><span class="line">            <span class="comment">// 如果系统不支持GBK编码，会捕获到UnsupportedEncodingException:</span></span><br><span class="line">            System.out.println(e); <span class="comment">// 打印异常信息</span></span><br><span class="line">            <span class="keyword">return</span> s.getBytes(); <span class="comment">// 尝试使用用默认编码</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>​ 如果我们不捕获<code>UnsupportedEncodingException</code>，会出现编译失败的问题：编译器会报错，错误信息类似：unreported exception UnsupportedEncodingException; must be caught or declared to be thrown，并且准确地指出需要捕获的语句是<code>return s.getBytes("GBK");</code>。意思是说，像<code>UnsupportedEncodingException</code>这样的Checked Exception，必须被捕获。</p>
<p>​ <strong>这是因为<code>String.getBytes(String)</code>方法定义是：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">byte</span>[] getBytes(String charsetName) <span class="keyword">throws</span> UnsupportedEncodingException &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ <strong>在方法定义的时候，使用<code>throws Xxx</code>表示该方法可能抛出的异常类型</strong>。调用方在调用的时候，必须强制捕获这些异常，否则编译器会报错。</p>
<p>​ <strong>注意</strong>：只要是方法声明的Checked Exception，不在调用层捕获，也必须在更高的调用层捕获。所有未捕获的异常，最终也必须在<code>main()</code>方法中捕获，不会出现漏写<code>try</code>的情况。这是由编译器保证的。<code>main()</code>方法也是最后捕获<code>Exception</code>的机会。</p>
<p>​ <strong>注意</strong>：所有异常都可以调用<code>printStackTrace()</code>方法打印异常栈，这是一个简单有用的快速打印异常的方法。</p>
<h4 id="多个catch语句">2、多个catch语句：</h4>
<p>​ 可以使用多个<code>catch</code>语句，每个<code>catch</code>分别捕获对应的<code>Exception</code>及其子类。JVM在捕获到异常后，会从上到下匹配<code>catch</code>语句，匹配到某个<code>catch</code>后，执行<code>catch</code>代码块，然后不再继续匹配。</p>
<p>​ <strong>存在多个<code>catch</code>的时候，<code>catch</code>的顺序非常重要：</strong>子类必须写在前面。例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        process1();</span><br><span class="line">        process2();</span><br><span class="line">        process3();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;IO error&quot;</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123; <span class="comment">// 永远捕获不到</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Bad encoding&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ <strong>对于上面的代码，<code>UnsupportedEncodingException</code>异常是永远捕获不到的</strong>，因为它是<code>IOException</code>的子类。当抛出<code>UnsupportedEncodingException</code>异常时，会被<code>catch (IOException e) &#123; ... &#125;</code>捕获并执行。</p>
<h4 id="finally语句">3、finally语句：</h4>
<p>无论是否有异常发生，如果我们都希望执行一些语句</p>
<p>Java的<code>try ... catch</code>机制还提供了<code>finally</code>语句，<code>finally</code>语句块保证有无错误都会执行。上述代码可以改写如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        process1();</span><br><span class="line">        process2();</span><br><span class="line">        process3();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Bad encoding&quot;</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;IO error&quot;</span>);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;END&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意<code>finally</code>有几个特点：</p>
<ol type="1">
<li><code>finally</code>语句不是必须的，可写可不写；</li>
<li><code>finally</code>总是最后执行。</li>
</ol>
<p>​ 如果没有发生异常，就正常执行<code>try &#123; ... &#125;</code>语句块，然后执行<code>finally</code>。如果发生了异常，就中断执行<code>try &#123; ... &#125;</code>语句块，然后跳转执行匹配的<code>catch</code>语句块，最后执行<code>finally</code>。</p>
<h4 id="捕获多种异常">4、捕获多种异常：</h4>
<p>可以把两个处理逻辑一样的异常用<code>|</code>合并到一起</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        process1();</span><br><span class="line">        process2();</span><br><span class="line">        process3();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException | NumberFormatException e) &#123; <span class="comment">// IOException或NumberFormatException</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Bad input&quot;</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Unknown error&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="三抛出异常">三、抛出异常：</h3>
<h4 id="抛出异常链条">1、抛出异常链条：</h4>
<p>​ 当某个方法抛出了异常时，如果当前方法没有捕获异常，异常就会被抛到上层调用方法，直到遇到某个<code>try ... catch</code>被捕获为止</p>
<p>​ 通过<code>printStackTrace()</code>可以打印出方法的调用栈，类似：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.lang.NumberFormatException: <span class="keyword">null</span></span><br><span class="line">    at java.base/java.lang.Integer.parseInt(Integer.java:<span class="number">614</span>)</span><br><span class="line">    at java.base/java.lang.Integer.parseInt(Integer.java:<span class="number">770</span>)</span><br><span class="line">    at Main.process2(Main.java:<span class="number">16</span>)</span><br><span class="line">    at Main.process1(Main.java:<span class="number">12</span>)</span><br><span class="line">    at Main.main(Main.java:<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p><code>printStackTrace()</code>对于调试错误非常有用，上述信息表示：<code>NumberFormatException</code>是在<code>java.lang.Integer.parseInt</code>方法中被抛出的，从下往上看，调用层次依次是：</p>
<ol type="1">
<li><code>main()</code>调用<code>process1()</code>；</li>
<li><code>process1()</code>调用<code>process2()</code>；</li>
<li><code>process2()</code>调用<code>Integer.parseInt(String)</code>；</li>
<li><code>Integer.parseInt(String)</code>调用<code>Integer.parseInt(String, int)</code>。</li>
</ol>
<h4 id="如何抛出异常">2、如何抛出异常：</h4>
<p>抛出异常分两步：</p>
<ol type="1">
<li>创建某个<code>Exception</code>的实例；</li>
<li>用<code>throw</code>语句抛出。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">process2</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (s==<span class="keyword">null</span>) &#123;</span><br><span class="line">        NullPointerException e = <span class="keyword">new</span> NullPointerException();</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>或写成一行：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">process2</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (s==<span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="异常捕获后再次抛出">3、异常捕获后再次抛出：</h4>
<p>​ 如果一个方法捕获了某个异常后，又在<code>catch</code>子句中抛出新的异常，就相当于把抛出的异常类型“转换”了：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">process1</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        process2();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (NullPointerException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">process2</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (s==<span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>打印出的异常栈类似：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">java.lang.IllegalArgumentException</span><br><span class="line">    at Main.process1(Main.java:15)</span><br><span class="line">    at Main.main(Main.java:5)</span><br></pre></td></tr></table></figure>
<p>​ 这说明<strong>新的异常丢失了原始异常信息</strong>，我们已经看不到原始异常<code>NullPointerException</code>的信息了。</p>
<p>​ <strong>为了能追踪到完整的异常栈，在构造异常的时候，把原始的<code>Exception</code>实例传进去，新的<code>Exception</code>就可以持有原始<code>Exception</code>信息。</strong>对上述代码改进如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            process1();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">process1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            process2();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (NullPointerException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">process2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行上述代码，打印出的异常栈类似：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">java.lang.IllegalArgumentException: java.lang.NullPointerException</span><br><span class="line">    at Main.process1(Main.java:15)</span><br><span class="line">    at Main.main(Main.java:5)</span><br><span class="line">Caused by: java.lang.NullPointerException</span><br><span class="line">    at Main.process2(Main.java:20)</span><br><span class="line">    at Main.process1(Main.java:13)</span><br></pre></td></tr></table></figure>
<p>​ 注意到<code>Caused by: Xxx</code>，说明捕获的<code>IllegalArgumentException</code>并不是造成问题的根源，根源在于<code>NullPointerException</code>，是在<code>Main.process2()</code>方法抛出的。</p>
<p>​ 在代码中<strong>获取原始异常可以使用<code>Throwable.getCause()</code>方法。如果返回<code>null</code>，说明已经是“根异常”了。</strong></p>
<h4 id="在try或catch种抛出异常finally语句会先执行然后再抛出异常">4、在try或catch种抛出异常，finally语句会先执行，然后再抛出异常</h4>
<h4 id="异常屏蔽">5、异常屏蔽：</h4>
<p>​ 如果在执行<code>finally</code>语句时抛出异常，那么，<code>catch</code>语句的异常还能否继续抛出？例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Integer.parseInt(<span class="string">&quot;abc&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;catched&quot;</span>);</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;finally&quot;</span>);</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行上述代码，发现异常信息如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">catched</span><br><span class="line">finally</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException</span><br><span class="line">    at Main.main(Main.java:11)</span><br></pre></td></tr></table></figure>
<p>​ 这说明<strong><code>finally</code>抛出异常后，原来在<code>catch</code>中准备抛出的异常就“消失”了，因为只能抛出一个异常。没有被抛出的异常称为“被屏蔽”的异常</strong>（Suppressed Exception）。</p>
<p>​ <strong>绝大多数情况下，在<code>finally</code>中不要抛出异常</strong>。因此，我们通常不需要关心<code>Suppressed Exception</code>。</p>
<h3 id="四自定义异常">四、自定义异常</h3>
<p>Java标准库定义的常用异常包括：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Exception</span><br><span class="line">│</span><br><span class="line">├─ RuntimeException</span><br><span class="line">│  │</span><br><span class="line">│  ├─ NullPointerException</span><br><span class="line">│  │</span><br><span class="line">│  ├─ IndexOutOfBoundsException</span><br><span class="line">│  │</span><br><span class="line">│  ├─ SecurityException</span><br><span class="line">│  │</span><br><span class="line">│  └─ IllegalArgumentException 参数检查不合法</span><br><span class="line">│     │</span><br><span class="line">│     └─ NumberFormatException</span><br><span class="line">│</span><br><span class="line">├─ IOException</span><br><span class="line">│  │</span><br><span class="line">│  ├─ UnsupportedCharsetException</span><br><span class="line">│  │</span><br><span class="line">│  ├─ FileNotFoundException</span><br><span class="line">│  │</span><br><span class="line">│  └─ SocketException</span><br><span class="line">│</span><br><span class="line">├─ ParseException</span><br><span class="line">│</span><br><span class="line">├─ GeneralSecurityException</span><br><span class="line">│</span><br><span class="line">├─ SQLException</span><br><span class="line">│</span><br><span class="line">└─ TimeoutException</span><br></pre></td></tr></table></figure>
<p><strong>当我们在代码中需要抛出异常时，尽量使用JDK已定义的异常类型</strong></p>
<p>在一个大型项目中，可以自定义新的异常类型，但是，保持一个合理的异常继承体系是非常重要的。</p>
<p><strong>一个常见的做法是自定义一个<code>BaseException</code>作为“根异常”，然后，派生出各种业务类型的异常。</strong></p>
<p><code>BaseException</code>需要从一个适合的<code>Exception</code>派生，通常建议从<code>RuntimeException</code>派生：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseException</span> <span class="keyword">extends</span> <span class="title">RuntimeException</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其他业务类型的异常就可以从<code>BaseException</code>派生：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserNotFoundException</span> <span class="keyword">extends</span> <span class="title">BaseException</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginFailedException</span> <span class="keyword">extends</span> <span class="title">BaseException</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>自定义的<code>BaseException</code>应该提供多个构造方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseException</span> <span class="keyword">extends</span> <span class="title">RuntimeException</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BaseException</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BaseException</span><span class="params">(String message, Throwable cause)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(message, cause);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BaseException</span><span class="params">(String message)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(message);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BaseException</span><span class="params">(Throwable cause)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(cause);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 上述构造方法实际上都是原样照抄<code>RuntimeException</code>。这样，抛出异常的时候，就可以选择合适的构造方法。通过IDE可以根据父类快速生成子类的构造方法。</p>
<h3 id="五nullpointerexception">五、NullPointerException</h3>
<p>​ <code>NullPointerException</code>即空指针异常，俗称NPE。如果一个对象为<code>null</code>，调用其方法或访问其字段就会产生`NullPointerException</p>
<h4 id="如何处理nullpointerexception">1、如何处理NullPointerException</h4>
<p>​ 遇到<code>NullPointerException</code>，遵循原则是早暴露，早修复，严禁使用<code>catch</code>来隐藏这种编码错误，一些好的编码习惯：如下</p>
<ul>
<li>成员变量在定义时初始化：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name = <span class="string">&quot;&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>使用空字符串<code>""</code>而不是默认的<code>null</code>可避免很多<code>NullPointerException</code>，编写业务逻辑时，用空字符串<code>""</code>表示未填写比<code>null</code>安全得多。</p></li>
<li><p>返回空字符串<code>""</code>、空数组而不是<code>null</code>,这样可以使得调用方无需检查结果是否为<code>null</code>。</p></li>
<li><p>如果调用方一定要根据<code>null</code>判断，比如返回<code>null</code>表示文件不存在，那么考虑返回<code>Optional&lt;T&gt;</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Optional&lt;String&gt; <span class="title">readFromFile</span><span class="params">(String file)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!fileExist(file)) &#123;</span><br><span class="line">        <span class="keyword">return</span> Optional.empty();</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样调用方必须通过<code>Optional.isPresent()</code>判断是否有结果。</p></li>
</ul>
<h4 id="如何定位nullpointerexception">2、如何定位NullPointerException</h4>
<p>​ 如果产生了<code>NullPointerException</code>，例如，调用<code>a.b.c.x()</code>时产生了<code>NullPointerException</code>，原因可能是：</p>
<ul>
<li><code>a</code>是<code>null</code>；</li>
<li><code>a.b</code>是<code>null</code>；</li>
<li><code>a.b.c</code>是<code>null</code>；</li>
</ul>
<p>​ 从Java 14开始，如果产生了<code>NullPointerException</code>，JVM可以给出详细的信息告诉我们<code>null</code>对象到底是谁。我们来看例子：</p>
<p>​ 可以在<code>NullPointerException</code>的详细信息中看到类似<code>... because "&lt;local1&gt;.address.city" is null</code>，意思是<code>city</code>字段为<code>null</code>，这样我们就能快速定位问题所在。</p>
<p>​ 这种增强的<code>NullPointerException</code>详细信息是Java 14新增的功能，但默认是关闭的，我们可以给JVM添加一个<code>-XX:+ShowCodeDetailsInExceptionMessages</code>参数启用它：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">java -XX:+ShowCodeDetailsInExceptionMessages Main.java</span><br></pre></td></tr></table></figure>
<h3 id="六使用断言">六、使用断言</h3>
<p>​ 断言（Assertion）是一种<strong>调试程序的方式</strong>。在Java中，使用<code>assert</code>关键字来实现断言。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">double</span> x = Math.abs(-<span class="number">123.45</span>);</span><br><span class="line">    <span class="keyword">assert</span> x &gt;= <span class="number">0</span> : <span class="string">&quot;x must &gt;= 0&quot;</span>;</span><br><span class="line">    System.out.println(x);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 语句<code>assert x &gt;= 0;</code>即为断言，断言条件<code>x &gt;= 0</code>预期为<code>true</code>。如果计算结果为<code>false</code>，则断言失败，抛出<code>AssertionError</code>。": "x must &gt;= 0" " 为可选的断言消息：</p>
<p>​ 这样，断言失败的时候，<code>AssertionError</code>会带上消息<code>x must &gt;= 0</code>，更加便于调试。</p>
<p>​ <strong>Java断言的特点是：断言失败时会抛出<code>AssertionError</code>，导致程序结束退出。因此，断言不能用于可恢复的程序错误，只应该用于开发和测试阶段。</strong></p>
<p><strong>注意</strong>：这JVM默认关闭断言指令，即遇到<code>assert</code>语句就自动忽略了，不执行。要执行<code>assert</code>语句，必须给Java虚拟机传递<code>-enableassertions</code>（可简写为<code>-ea</code>）参数启用断言。所以，上述程序必须在命令行下运行才有效果：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ java -ea Main.java</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">&quot;main&quot;</span> java.lang.AssertionError</span><br><span class="line">	at Main.main(Main.java:5)</span><br></pre></td></tr></table></figure>
<p>​ 断言很少被使用，更好的方法是编写单元测试。</p>
<h3 id="七使用jdk-logging">七、使用JDK Logging</h3>
<h4 id="使用日志的目的">1、使用日志的目的？</h4>
<p>日志就是Logging，它的目的是为了取代<code>System.out.println()</code>。</p>
<p>输出日志，而不是用<code>System.out.println()</code>，有以下几个好处：</p>
<ol type="1">
<li>可以设置输出样式，避免自己每次都写<code>"ERROR: " + var</code>；</li>
<li>可以设置输出级别，禁止某些级别输出。例如，只输出错误日志；</li>
<li>可以被重定向到文件，这样可以在程序运行结束后查看日志；</li>
<li>可以按包名控制日志级别，只输出某些包打的日志；</li>
</ol>
<h4 id="如何使用日志">2、如何使用日志？</h4>
<p>Java标准库内置了日志包<code>java.util.logging</code>，我们可以直接用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.logging.Level;</span><br><span class="line"><span class="keyword">import</span> java.util.logging.Logger;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Logger logger = Logger.getGlobal();</span><br><span class="line">        logger.info(<span class="string">&quot;start process...&quot;</span>);</span><br><span class="line">        logger.warning(<span class="string">&quot;memory is running out...&quot;</span>);</span><br><span class="line">        logger.fine(<span class="string">&quot;ignored.&quot;</span>);</span><br><span class="line">        logger.severe(<span class="string">&quot;process will be terminated...&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行上述代码，得到类似如下的输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Mar 02, 2019 6:32:13 PM Hello main</span><br><span class="line">INFO: start process...</span><br><span class="line">Mar 02, 2019 6:32:13 PM Hello main</span><br><span class="line">WARNING: memory is running out...</span><br><span class="line">Mar 02, 2019 6:32:13 PM Hello main</span><br><span class="line">SEVERE: process will be terminated...</span><br></pre></td></tr></table></figure>
<p>对比可见，使用日志最大的好处是，它自动打印了时间、调用类、调用方法等很多有用的信息。</p>
<h4 id="日志的输出级别">3、日志的输出级别：</h4>
<p>再仔细观察发现上述输出，4条日志，只打印了3条，<code>logger.fine()</code>没有打印。这是因为，日志的输出可以设定级别。JDK的Logging定义了7个日志级别，从严重到普通：</p>
<ul>
<li>SEVERE</li>
<li>WARNING</li>
<li>INFO</li>
<li>CONFIG</li>
<li>FINE</li>
<li>FINER</li>
<li>FINEST</li>
</ul>
<p>​ 因为<strong>默认级别是INFO，因此，INFO级别以下的日志，不会被打印出来。</strong>使用日志级别的好处在于，<strong>调整级别，就可以屏蔽掉很多调试相关的日志输出。</strong></p>
<p>使用Java标准库内置的Logging有以下局限：</p>
<ul>
<li><p>Logging系统在JVM启动时读取配置文件并完成初始化，一旦开始运行<code>main()</code>方法，就无法修改配置；</p></li>
<li><p>配置不太方便，需要在JVM启动时传递参数<code>-Djava.util.logging.config.file=&lt;config-file-name&gt;</code>。</p></li>
</ul>
<p>​ <strong>因此，Java标准库内置的Logging使用并不是非常广泛，更广泛的是下一个模块中所说的Commons Logging 或 Log4j 或</strong></p>
<h3 id="八使用commons-logging">八、使用Commons Logging</h3>
<h4 id="简单介绍">1、简单介绍</h4>
<p>​ <strong>Commons Logging是一个第三方日志库，它是由Apache创建的日志模块</strong></p>
<p>​ Commons Logging的特色是，<strong>它可以挂接不同的日志系统，并通过配置文件指定挂接的日志系统。</strong>默认情况下，<strong>Commons Logging自动搜索并使用Log4j</strong>（Log4j是另一个流行的日志系统），如果没有找到Log4j，再使用JDK Logging。</p>
<h4 id="如何使用">2、如何使用：</h4>
<p>第一步，通过<code>LogFactory</code>获取<code>Log</code>类的实例；</p>
<p>第二步，使用<code>Log</code>实例的方法打日志。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.logging.Log;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.logging.LogFactory;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Log log = LogFactory.getLog(Main.class);</span><br><span class="line">        log.info(<span class="string">&quot;start...&quot;</span>);</span><br><span class="line">        log.warn(<span class="string">&quot;end.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="日志级别">3、日志级别：</h4>
<p>Commons Logging定义了6个日志级别：</p>
<ul>
<li>FATAL</li>
<li>ERROR</li>
<li>WARNING</li>
<li>INFO</li>
<li>DEBUG</li>
<li>TRACE</li>
</ul>
<p>默认级别是<code>INFO</code>。</p>
<h4 id="通常使用指南">4、通常使用指南</h4>
<ul>
<li>使用Commons Logging时，如果在静态方法中引用<code>Log</code>，通常直接定义一个静态类型变量：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在静态方法中引用Log:</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> Log log = LogFactory.getLog(Main.class);</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">&quot;foo&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>在实例方法中引用<code>Log</code>，通常定义一个实例变量：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在实例方法中引用Log:</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> Log log = LogFactory.getLog(getClass());</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">&quot;foo&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ <strong>注意到实例变量log的获取方式是<code>LogFactory.getLog(getClass())</code>，虽然也可以用<code>LogFactory.getLog(Person.class)</code>，但是前一种方式有个非常大的好处，就是子类可以直接使用该<code>log</code>实例。例如：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在子类中使用父类实例化的log:</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">bar</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">&quot;bar&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 由于Java类的动态特性，子类获取的<code>log</code>字段实际上相当于<code>LogFactory.getLog(Student.class)</code>，但却是从父类继承而来，并且无需改动代码。</p>
<h4 id="记录异常">5、记录异常：</h4>
<p>​ 此外，Commons Logging的日志方法，例如<code>info()</code>，除了标准的<code>info(String)</code>外，还提供了一个非常有用的重载方法：<code>info(String, Throwable)</code>，这使得记录异常更加简单：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    log.error(<span class="string">&quot;got exception!&quot;</span>, e);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="九使用log4j">九、使用Log4j</h3>
<h4 id="框架简介">1、框架简介：</h4>
<p>​ 上述介绍了Commons Logging，可以作为“日志接口”来使用。而真正的“日志实现”可以使用Log4j。</p>
<p>​ Log4j是一种非常流行的日志框架，最新版本是2.x。</p>
<p>​ Log4j是一个组件化设计的日志系统，它的架构大致如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">log.info(&quot;User signed in.&quot;);</span><br><span class="line"> │</span><br><span class="line"> │   ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐</span><br><span class="line"> ├──&gt;│ Appender │───&gt;│  Filter  │───&gt;│  Layout  │───&gt;│ Console  │</span><br><span class="line"> │   └──────────┘    └──────────┘    └──────────┘    └──────────┘</span><br><span class="line"> │</span><br><span class="line"> │   ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐</span><br><span class="line"> ├──&gt;│ Appender │───&gt;│  Filter  │───&gt;│  Layout  │───&gt;│   File   │</span><br><span class="line"> │   └──────────┘    └──────────┘    └──────────┘    └──────────┘</span><br><span class="line"> │</span><br><span class="line"> │   ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐</span><br><span class="line"> └──&gt;│ Appender │───&gt;│  Filter  │───&gt;│  Layout  │───&gt;│  Socket  │</span><br><span class="line">     └──────────┘    └──────────┘    └──────────┘    └──────────┘</span><br></pre></td></tr></table></figure>
<p>​ 当我们使用Log4j输出一条日志时，Log4j自动通过不同的Appender把同一条日志输出到不同的目的地。例如：</p>
<ul>
<li>console：输出到屏幕；</li>
<li>file：输出到文件；</li>
<li>socket：通过网络输出到远程计算机；</li>
<li>jdbc：输出到数据库</li>
</ul>
<p>​ <strong>在输出日志的过程中，通过Filter来过滤哪些log需要被输出，哪些log不需要被输出。例如，仅输出<code>ERROR</code>级别的日志。</strong></p>
<p>最后，通过Layout来格式化日志信息，例如，自动添加日期、时间、方法名称等信息。</p>
<p><strong>上述结构虽然复杂，但我们在实际使用的时候，并不需要关心Log4j的API，而是通过配置文件来配置它。</strong></p>
<h4 id="如何使用-1">2、如何使用：</h4>
<p>以XML配置为例，使用Log4j的时候，我们把一个<code>log4j2.xml</code>的文件放到<code>classpath</code>下就可以让Log4j读取配置文件并按照我们的配置来输出日志。下面是一个配置文件的例子：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">Properties</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 定义日志格式 --&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">&quot;log.pattern&quot;</span>&gt;</span>%d&#123;MM-dd HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125;%n%msg%n%n<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 定义文件名变量 --&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">&quot;file.err.filename&quot;</span>&gt;</span>log/err.log<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">&quot;file.err.pattern&quot;</span>&gt;</span>log/err.%i.log.gz<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">Properties</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 定义Appender，即目的地 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">Appenders</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 定义输出到屏幕 --&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">Console</span> <span class="attr">name</span>=<span class="string">&quot;console&quot;</span> <span class="attr">target</span>=<span class="string">&quot;SYSTEM_OUT&quot;</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 日志格式引用上面定义的log.pattern --&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PatternLayout</span> <span class="attr">pattern</span>=<span class="string">&quot;$&#123;log.pattern&#125;&quot;</span> /&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">Console</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 定义输出到文件,文件名引用上面定义的file.err.filename --&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">RollingFile</span> <span class="attr">name</span>=<span class="string">&quot;err&quot;</span> <span class="attr">bufferedIO</span>=<span class="string">&quot;true&quot;</span> <span class="attr">fileName</span>=<span class="string">&quot;$&#123;file.err.filename&#125;&quot;</span> <span class="attr">filePattern</span>=<span class="string">&quot;$&#123;file.err.pattern&#125;&quot;</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PatternLayout</span> <span class="attr">pattern</span>=<span class="string">&quot;$&#123;log.pattern&#125;&quot;</span> /&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">Policies</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- 根据文件大小自动切割日志 --&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">SizeBasedTriggeringPolicy</span> <span class="attr">size</span>=<span class="string">&quot;1 MB&quot;</span> /&gt;</span></span><br><span class="line">			<span class="tag">&lt;/<span class="name">Policies</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 保留最近10份 --&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">DefaultRolloverStrategy</span> <span class="attr">max</span>=<span class="string">&quot;10&quot;</span> /&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">RollingFile</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">Appenders</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">Loggers</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">Root</span> <span class="attr">level</span>=<span class="string">&quot;info&quot;</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 对info级别的日志，输出到console --&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">&quot;console&quot;</span> <span class="attr">level</span>=<span class="string">&quot;info&quot;</span> /&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 对error级别的日志，输出到err，即上面定义的RollingFile --&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">&quot;err&quot;</span> <span class="attr">level</span>=<span class="string">&quot;error&quot;</span> /&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">Root</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">Loggers</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">Configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>​ 虽然配置Log4j比较繁琐，但一旦配置完成，使用起来就非常方便。对上面的配置文件，凡是<code>INFO</code>级别的日志，会自动输出到屏幕，而<code>ERROR</code>级别的日志，不但会输出到屏幕，还会同时输出到文件。并且，一旦日志文件达到指定大小（1MB），Log4j就会自动切割新的日志文件，并最多保留10份。</p>
<p>​ 有了配置文件还不够，因为Log4j也是一个第三方库，我们需要从<a href="https://logging.apache.org/log4j/2.x/download.html">这里</a>下载Log4j，解压后，把以下3个jar包放到<code>classpath</code>中：</p>
<ul>
<li>log4j-api-2.x.jar</li>
<li>log4j-core-2.x.jar</li>
<li>log4j-jcl-2.x.jar</li>
</ul>
<p>​ 因为Commons Logging会自动发现并使用Log4j，所以，把上一节下载的<code>commons-logging-1.2.jar</code>也放到<code>classpath</code>中。</p>
<p>​ 要打印日志，只需要按Commons Logging的写法写，不需要改动任何代码，就可以得到Log4j的日志输出，类似：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">03-03 12:09:45.880 [main] INFO  com.itranswarp.learnjava.Main</span><br><span class="line">Start process...</span><br></pre></td></tr></table></figure>
<h4 id="最佳实践">3、最佳实践：</h4>
<p>​ 在开发阶段，始终使用Commons Logging接口来写入日志，并且开发阶段无需引入Log4j。如果需要把日志写入文件， 只需要把正确的配置文件和Log4j相关的jar包放入<code>classpath</code>，就可以自动把日志切换成使用Log4j写入，无需修改任何代码。</p>
<h3 id="十使用slf4j和logback">十、使用SLF4J和Logback</h3>
<p>​ SLF4J类似于Commons Logging，也是一个日志接口，而Logback类似于Log4j，是一个日志的实现。</p>
<p>​ SLF4J对Commons Logging的接口有何改进？在Commons Logging中，我们要打印日志，有时候得这么写：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> score = <span class="number">99</span>;</span><br><span class="line">p.setScore(score);</span><br><span class="line">log.info(<span class="string">&quot;Set score &quot;</span> + score + <span class="string">&quot; for Person &quot;</span> + p.getName() + <span class="string">&quot; ok.&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>拼字符串是一个非常麻烦的事情，所以SLF4J的日志接口改进成这样了：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> score = <span class="number">99</span>;</span><br><span class="line">p.setScore(score);</span><br><span class="line">logger.info(<span class="string">&quot;Set score &#123;&#125; for Person &#123;&#125; ok.&quot;</span>, score, p.getName());</span><br></pre></td></tr></table></figure>
<p>我们靠猜也能猜出来，SLF4J的日志接口传入的是一个带占位符的字符串，用后面的变量自动替换占位符，所以看起来更加自然。</p>
<p>具体可以参照更详细的官方网站的教程</p>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>exception</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记3——java核心类</title>
    <url>/2022/01/30/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B03%E2%80%94java%E6%A0%B8%E5%BF%83%E7%B1%BB/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1260469698963456</p>
<h3 id="一字符串和编码">一、字符串和编码</h3>
<p>​ Java字符串的一个重要特点就是字符串<strong>不可变</strong>。这种不可变性是通过内部的<code>private final char[]</code>字段，以及没有任何修改<code>char[]</code>的方法实现的。</p>
<h4 id="字符串比较">1、字符串比较</h4>
<p>​ 必须使用<code>equals()</code>方法而不能用<code>==</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String s1 = <span class="string">&quot;hello&quot;</span>;</span><br><span class="line">        String s2 = <span class="string">&quot;hello&quot;</span>;</span><br><span class="line">        System.out.println(s1 == s2);     <span class="comment">//true</span></span><br><span class="line">        System.out.println(s1.equals(s2));  <span class="comment">//true</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 从表面上看，两个字符串用<code>==</code>和<code>equals()</code>比较都为<code>true</code>，但<strong>实际上那只是Java编译器在编译期，会自动把所有相同的字符串当作一个对象放入常量池，自然<code>s1</code>和<code>s2</code>的引用就是相同的。</strong></p>
<p>​ 所以，这种<code>==</code>比较返回<code>true</code>纯属巧合。换一种写法，<code>==</code>比较就会失败：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String s1 = <span class="string">&quot;hello&quot;</span>;</span><br><span class="line">        String s2 = <span class="string">&quot;HELLO&quot;</span>.toLowerCase();</span><br><span class="line">        System.out.println(s1 == s2);</span><br><span class="line">        System.out.println(s1.equals(s2));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 要忽略大小写比较，使用<code>equalsIgnoreCase()</code>方法。</p>
<h4 id="搜索与提取子串">2、搜索与提取子串</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;Hello&quot;</span>.contains(<span class="string">&quot;ll&quot;</span>); <span class="comment">// true</span></span><br><span class="line"><span class="string">&quot;Hello&quot;</span>.indexOf(<span class="string">&quot;l&quot;</span>); <span class="comment">// 2</span></span><br><span class="line"><span class="string">&quot;Hello&quot;</span>.lastIndexOf(<span class="string">&quot;l&quot;</span>); <span class="comment">// 3</span></span><br><span class="line"><span class="string">&quot;Hello&quot;</span>.startsWith(<span class="string">&quot;He&quot;</span>); <span class="comment">// true</span></span><br><span class="line"><span class="string">&quot;Hello&quot;</span>.endsWith(<span class="string">&quot;lo&quot;</span>); <span class="comment">// true</span></span><br><span class="line"><span class="string">&quot;Hello&quot;</span>.substring(<span class="number">2</span>); <span class="comment">// &quot;llo&quot;</span></span><br><span class="line"><span class="string">&quot;Hello&quot;</span>.substring(<span class="number">2</span>, <span class="number">4</span>); <span class="string">&quot;ll&quot;</span></span><br></pre></td></tr></table></figure>
<h4 id="去除首尾空白字符">3、去除首尾空白字符：</h4>
<ul>
<li><p>使用<code>trim()</code>方法可以移除字符串首尾空白字符。<strong>空白字符包括空格，<code>\t</code>，<code>\r</code>，<code>\n</code></strong></p></li>
<li><p>另一个<code>strip()</code>方法也可以移除字符串首尾空白字符。它和<code>trim()</code>不同的是，类似中文的空格字符<code>\u3000</code>也会被移除</p></li>
</ul>
<h4 id="判断是否为空">4、判断是否为空：</h4>
<p><code>String</code>还提供了<code>isEmpty()</code>和<code>isBlank()</code>来判断字符串是否为空 和 空白字符串</p>
<p>空白字符串 代表 只含 <strong>空白字符</strong>的字符串。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;</span>.isEmpty(); <span class="comment">// true，因为字符串长度为0</span></span><br><span class="line"><span class="string">&quot;  &quot;</span>.isEmpty(); <span class="comment">// false，因为字符串长度不为0</span></span><br><span class="line"><span class="string">&quot;  \n&quot;</span>.isBlank(); <span class="comment">// true，因为只包含空白字符</span></span><br><span class="line"><span class="string">&quot; Hello &quot;</span>.isBlank(); <span class="comment">// false，因为包含非空白字符</span></span><br></pre></td></tr></table></figure>
<h4 id="替换子串">5、替换子串</h4>
<ul>
<li>根据字符或字符串替换</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s = <span class="string">&quot;hello&quot;</span>;</span><br><span class="line">s.replace(<span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;w&#x27;</span>); <span class="comment">// &quot;hewwo&quot;，所有字符&#x27;l&#x27;被替换为&#x27;w&#x27;</span></span><br><span class="line">s.replace(<span class="string">&quot;ll&quot;</span>, <span class="string">&quot;~~&quot;</span>); <span class="comment">// &quot;he~~o&quot;，所有子串&quot;ll&quot;被替换为&quot;~~&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>正则表达式替换</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s = <span class="string">&quot;A,,B;C ,D&quot;</span>;</span><br><span class="line">s.replaceAll(<span class="string">&quot;[\\,\\;\\s]+&quot;</span>, <span class="string">&quot;,&quot;</span>); <span class="comment">// &quot;A,B,C,D&quot;</span></span><br></pre></td></tr></table></figure>
<h4 id="分割拼接格式化字符串">6、分割、拼接、格式化字符串：</h4>
<ul>
<li>要分割字符串，使用<code>split()</code>方法，并且传入的也是正则表达式：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s = <span class="string">&quot;A,B,C,D&quot;</span>;</span><br><span class="line">String[] ss = s.split(<span class="string">&quot;\\,&quot;</span>); <span class="comment">// &#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>拼接字符串使用静态方法<code>join()</code>，它用指定的字符串连接字符串数组：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String[] arr = &#123;<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>, <span class="string">&quot;C&quot;</span>&#125;;</span><br><span class="line">String s = String.join(<span class="string">&quot;***&quot;</span>, arr); <span class="comment">// &quot;A***B***C&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>字符串提供了<code>formatted()</code>方法和<code>format()</code>静态方法，可以传入其他参数，替换占位符，然后生成新的字符串：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String s = <span class="string">&quot;Hi %s, your score is %d!&quot;</span>;</span><br><span class="line">        System.out.println(s.formatted(<span class="string">&quot;Alice&quot;</span>, <span class="number">80</span>));</span><br><span class="line">        System.out.println(String.format(<span class="string">&quot;Hi %s, your score is %.2f!&quot;</span>, <span class="string">&quot;Bob&quot;</span>, <span class="number">59.5</span>));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="类型转换">7、类型转换：</h4>
<p>要把任意基本类型或引用类型转换为字符串，可以使用静态方法<code>valueOf()</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String.valueOf(<span class="number">123</span>); <span class="comment">// &quot;123&quot;</span></span><br><span class="line">String.valueOf(<span class="number">45.67</span>); <span class="comment">// &quot;45.67&quot;</span></span><br><span class="line">String.valueOf(<span class="keyword">true</span>); <span class="comment">// &quot;true&quot;</span></span><br><span class="line">String.valueOf(<span class="keyword">new</span> Object()); <span class="comment">// 类似java.lang.Object@636be97c</span></span><br></pre></td></tr></table></figure>
<p>把字符串转换为其他类型，就需要根据情况。例如，把字符串转换为<code>int</code>类型：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> n1 = Integer.parseInt(<span class="string">&quot;123&quot;</span>); <span class="comment">// 123</span></span><br><span class="line"><span class="keyword">int</span> n2 = Integer.parseInt(<span class="string">&quot;ff&quot;</span>, <span class="number">16</span>); <span class="comment">// 按十六进制转换，255</span></span><br></pre></td></tr></table></figure>
<p>把字符串转换为<code>boolean</code>类型：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">boolean</span> b1 = Boolean.parseBoolean(<span class="string">&quot;true&quot;</span>); <span class="comment">// true</span></span><br><span class="line"><span class="keyword">boolean</span> b2 = Boolean.parseBoolean(<span class="string">&quot;FALSE&quot;</span>); <span class="comment">// false</span></span><br></pre></td></tr></table></figure>
<p>要特别注意，<code>Integer</code>有个<code>getInteger(String)</code>方法，它不是将字符串转换为<code>int</code>，而是把该字符串对应的系统变量转换为<code>Integer</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer.getInteger(<span class="string">&quot;java.version&quot;</span>); <span class="comment">// 版本号，11</span></span><br></pre></td></tr></table></figure>
<p><code>String</code>和<code>char[]</code>类型可以互相转换，方法是：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">char</span>[] cs = <span class="string">&quot;Hello&quot;</span>.toCharArray(); <span class="comment">// String -&gt; char[]</span></span><br><span class="line">String s = <span class="keyword">new</span> String(cs); <span class="comment">// char[] -&gt; String</span></span><br></pre></td></tr></table></figure>
<p>如果修改了<code>char[]</code>数组，<code>String</code>并不会改变：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">char</span>[] cs = <span class="string">&quot;Hello&quot;</span>.toCharArray();</span><br><span class="line">        String s = <span class="keyword">new</span> String(cs);</span><br><span class="line">        System.out.println(s);</span><br><span class="line">        cs[<span class="number">0</span>] = <span class="string">&#x27;X&#x27;</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 这是因为通过<code>new String(char[])</code>创建新的<code>String</code>实例时，它并不会直接引用传入的<code>char[]</code>数组，而是会复制一份，所以，修改外部的<code>char[]</code>数组不会影响<code>String</code>实例内部的<code>char[]</code>数组，因为这是两个不同的数组。</p>
<p><strong>设计注意</strong>：从<code>String</code>的不变性设计可以看出，如果传入的对象有可能改变，我们需要复制而不是直接引用。</p>
<h4 id="字符编码问题">8、字符编码问题：</h4>
<p>​ 为了统一全球所有语言的编码，全球统一码联盟发布了<code>Unicode</code>编码</p>
<p>​ <code>Unicode</code>编码需要两个或者更多字节表示</p>
<p>英文字符<code>'A'</code>的<code>ASCII</code>编码和<code>Unicode</code>编码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">         ┌────┐</span><br><span class="line">ASCII:   │ 41 │</span><br><span class="line">         └────┘</span><br><span class="line">         ┌────┬────┐</span><br><span class="line">Unicode: │ 00 │ 41 │</span><br><span class="line">         └────┴────┘</span><br></pre></td></tr></table></figure>
<p>英文字符的<code>Unicode</code>编码就是简单地在前面添加一个<code>00</code>字节。</p>
<p>中文字符<code>'中'</code>的<code>GB2312</code>编码和<code>Unicode</code>编码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">         ┌────┬────┐</span><br><span class="line">GB2312:  │ d6 │ d0 │</span><br><span class="line">         └────┴────┘</span><br><span class="line">         ┌────┬────┐</span><br><span class="line">Unicode: │ 4e │ 2d │</span><br><span class="line">         └────┴────┘</span><br></pre></td></tr></table></figure>
<p>​ 那我们经常使用的<code>UTF-8</code>又是什么编码呢？因为英文字符的<code>Unicode</code>编码高字节总是<code>00</code>，包含大量英文的文本会浪费空间，所以，出现了<code>UTF-8</code>编码，<strong>它是一种变长编码</strong>，<strong>用来把固定长度的<code>Unicode</code>编码变成1～4字节的变长编码</strong>。通过<code>UTF-8</code>编码，英文字符<code>'A'</code>的<code>UTF-8</code>编码变为<code>0x41</code>，正好和<code>ASCII</code>码一致，而中文<code>'中'</code>的<code>UTF-8</code>编码为3字节<code>0xe4b8ad</code>。</p>
<p>​ <strong><code>UTF-8</code>编码的另一个好处是容错能力强。</strong>如果传输过程中某些字符出错，不会影响后续字符，因为<code>UTF-8</code>编码依靠高字节位来确定一个字符究竟是几个字节，它经常用来作为传输编码。</p>
<p>​ 在Java中，<strong><code>char</code>类型实际上就是两个字节的<code>Unicode</code>编码</strong>。如果我们要手动把字符串转换成其他编码，可以这样做：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">byte</span>[] b1 = <span class="string">&quot;Hello&quot;</span>.getBytes(); <span class="comment">// 按系统默认编码转换，不推荐</span></span><br><span class="line"><span class="keyword">byte</span>[] b2 = <span class="string">&quot;Hello&quot;</span>.getBytes(<span class="string">&quot;UTF-8&quot;</span>); <span class="comment">// 按UTF-8编码转换</span></span><br><span class="line"><span class="keyword">byte</span>[] b2 = <span class="string">&quot;Hello&quot;</span>.getBytes(<span class="string">&quot;GBK&quot;</span>); <span class="comment">// 按GBK编码转换</span></span><br><span class="line"><span class="keyword">byte</span>[] b3 = <span class="string">&quot;Hello&quot;</span>.getBytes(StandardCharsets.UTF_8); <span class="comment">// 按UTF-8编码转换</span></span><br></pre></td></tr></table></figure>
<p><strong>注意：转换编码后，就不再是<code>char</code>类型，而是<code>byte</code>类型表示的数组。</strong></p>
<p>如果要把已知编码的<code>byte[]</code>转换为<code>String</code>，可以这样做：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">byte</span>[] b = ...</span><br><span class="line">String s1 = <span class="keyword">new</span> String(b, <span class="string">&quot;GBK&quot;</span>); <span class="comment">// 按GBK转换</span></span><br><span class="line">String s2 = <span class="keyword">new</span> String(b, StandardCharsets.UTF_8); <span class="comment">// 按UTF-8转换</span></span><br></pre></td></tr></table></figure>
<p><strong>始终牢记：Java的<code>String</code>和<code>char</code>在内存中总是以Unicode编码表示。</strong></p>
<h3 id="二stringbuilder">二、StringBuilder</h3>
<p>​ 在java的String类种，虽然我们可以直接拼接字符串，但是，在循环中，<strong>每次循环都会创建新的字符串对象，然后扔掉旧的字符串</strong>。这样，<strong>绝大部分字符串都是临时对象，不但浪费内存，还会影响GC效率</strong>。</p>
<p>​ 为了能高效拼接字符串，Java标准库提供了<code>StringBuilder</code>，它是一个可变对象，可以预分配缓冲区，这样，往<code>StringBuilder</code>中新增字符时，不会创建新的临时对象：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder(<span class="number">1024</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++) &#123;</span><br><span class="line">    sb.append(<span class="string">&#x27;,&#x27;</span>);</span><br><span class="line">    sb.append(i);</span><br><span class="line">&#125;</span><br><span class="line">String s = sb.toString();</span><br></pre></td></tr></table></figure>
<p><strong>链式操作</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">var</span> sb = <span class="keyword">new</span> StringBuilder(<span class="number">1024</span>);</span><br><span class="line">        sb.append(<span class="string">&quot;Mr &quot;</span>)</span><br><span class="line">          .append(<span class="string">&quot;Bob&quot;</span>)</span><br><span class="line">          .append(<span class="string">&quot;!&quot;</span>)</span><br><span class="line">          .insert(<span class="number">0</span>, <span class="string">&quot;Hello, &quot;</span>);</span><br><span class="line">        System.out.println(sb.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ <strong>进行链式操作的关键是：定义的<code>append()</code>方法会返回<code>this</code>，这样，就可以不断调用自身的其他方法</strong></p>
<p><strong>注意</strong>：对于普通的字符串<code>+</code>操作，并不需要我们将其改写为<code>StringBuilder</code>，因为Java编译器在编译时就自动把多个连续的<code>+</code>操作编码为<code>StringConcatFactory</code>的操作。在运行期，<code>StringConcatFactory</code>会自动把字符串连接操作优化为数组复制或者<code>StringBuilder</code>操作。</p>
<h3 id="三stringjoiner">三、StringJoiner</h3>
<ul>
<li>Java标准库还提供了一个<code>StringJoiner</code>可以用于使用分隔符拼接数组。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String[] names = &#123;<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;Grace&quot;</span>&#125;;</span><br><span class="line">        <span class="comment">// var sj = new StringJoiner(&quot;, &quot;);  //case1 指定分隔符 </span></span><br><span class="line">        <span class="keyword">var</span> sj = <span class="keyword">new</span> StringJoiner(<span class="string">&quot;, &quot;</span>, <span class="string">&quot;Hello &quot;</span>, <span class="string">&quot;!&quot;</span>); <span class="comment">//case2 指定开头和结尾字符</span></span><br><span class="line">        <span class="keyword">for</span> (String name : names) &#123;</span><br><span class="line">            sj.add(name);</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(sj.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// case1 Bob, Alice, Grace </span></span><br><span class="line"><span class="comment">// case2 Hello Bob, Alice, Grace! </span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>String</code>还提供了一个静态方法<code>join()</code>，这个方法在内部使用了<code>StringJoiner</code>来拼接字符串，在不需要指定“开头”和“结尾”的时候，用<code>String.join()</code>更方便：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String[] names = &#123;<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;Grace&quot;</span>&#125;;</span><br><span class="line"><span class="keyword">var</span> s = String.join(<span class="string">&quot;, &quot;</span>, names);</span><br></pre></td></tr></table></figure>
<h3 id="四包装类型">四、包装类型</h3>
<h4 id="什么是包装类型">1、什么是包装类型？</h4>
<p>java的数据类型分两种：</p>
<ul>
<li>基本类型：<code>byte</code>，<code>short</code>，<code>int</code>，<code>long</code>，<code>boolean</code>，<code>float</code>，<code>double</code>，<code>char</code></li>
<li>引用类型：所有<code>class</code>和<code>interface</code>类型</li>
</ul>
<p><strong>引用类型可以赋值为<code>null</code>，表示空，但基本类型不能赋值为<code>null</code></strong></p>
<p>如何把一个基本类型视为对象（引用类型）？</p>
<p>​ 比如，想要把<code>int</code>基本类型变成一个引用类型，我们可以定义一个<code>Integer</code>类，它只包含一个实例字段<code>int</code>，这样，<strong><code>Integer</code>类就可以视为<code>int</code>的包装类（Wrapper Class）</strong></p>
<p>实际上，因为包装类型非常有用，Java核心库为每种基本类型都提供了对应的包装类型：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">基本类型</th>
<th style="text-align: left;">对应的引用类型</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">boolean</td>
<td style="text-align: left;">java.lang.Boolean</td>
</tr>
<tr class="even">
<td style="text-align: left;">byte</td>
<td style="text-align: left;">java.lang.Byte</td>
</tr>
<tr class="odd">
<td style="text-align: left;">short</td>
<td style="text-align: left;">java.lang.Short</td>
</tr>
<tr class="even">
<td style="text-align: left;">int</td>
<td style="text-align: left;">java.lang.Integer</td>
</tr>
<tr class="odd">
<td style="text-align: left;">long</td>
<td style="text-align: left;">java.lang.Long</td>
</tr>
<tr class="even">
<td style="text-align: left;">float</td>
<td style="text-align: left;">java.lang.Float</td>
</tr>
<tr class="odd">
<td style="text-align: left;">double</td>
<td style="text-align: left;">java.lang.Double</td>
</tr>
<tr class="even">
<td style="text-align: left;">char</td>
<td style="text-align: left;">java.lang.Character</td>
</tr>
</tbody>
</table>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">100</span>;</span><br><span class="line">        <span class="comment">// 通过new操作符创建Integer实例(不推荐使用,会有编译警告):</span></span><br><span class="line">        Integer n1 = <span class="keyword">new</span> Integer(i);</span><br><span class="line">        <span class="comment">// 通过静态方法valueOf(int)创建Integer实例:</span></span><br><span class="line">        Integer n2 = Integer.valueOf(i);</span><br><span class="line">        <span class="comment">// 通过静态方法valueOf(String)创建Integer实例:</span></span><br><span class="line">        Integer n3 = Integer.valueOf(<span class="string">&quot;100&quot;</span>);</span><br><span class="line">        System.out.println(n3.intValue());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="auto-boxing">2、Auto Boxing</h4>
<p>因为<code>int</code>和<code>Integer</code>可以互相转换：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">100</span>;</span><br><span class="line">Integer n = Integer.valueOf(i);</span><br><span class="line"><span class="keyword">int</span> x = n.intValue();</span><br></pre></td></tr></table></figure>
<p>所以，Java编译器可以帮助我们自动在<code>int</code>和<code>Integer</code>之间转型：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer n = <span class="number">100</span>; <span class="comment">// 编译器自动使用Integer.valueOf(int)</span></span><br><span class="line"><span class="keyword">int</span> x = n; <span class="comment">// 编译器自动使用Integer.intValue()</span></span><br></pre></td></tr></table></figure>
<p>​ <strong>这种直接把<code>int</code>变为<code>Integer</code>的赋值写法，称为自动装箱（Auto Boxing），反过来，把<code>Integer</code>变为<code>int</code>的赋值写法，称为自动拆箱（Auto Unboxing）。</strong></p>
<p>​ 注意：<strong>自动装箱和自动拆箱只发生在编译阶段，目的是为了少写代码</strong>。</p>
<p>装箱和拆箱会影响代码的执行效率，因为编译后的<code>class</code>代码是严格区分基本类型和引用类型的。并且，自动拆箱执行时可能会报<code>NullPointerException</code>：</p>
<h4 id="不变类">3、不变类</h4>
<p>​ 所有的包装类型都是不变类，因此，一旦创建了<code>Integer</code>对象，该对象就是不变的。</p>
<p>​ 对两个<code>Integer</code>实例进行比较要特别注意：绝对不能用<code>==</code>比较，因为<code>Integer</code>是引用类型，必须使用<code>equals()</code>比较：</p>
<h4 id="进制转换">4、进制转换：</h4>
<p><code>Integer</code>类本身还提供了大量方法，例如，最常用的静态方法<code>parseInt()</code>可以把字符串解析成一个整数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> x1 = Integer.parseInt(<span class="string">&quot;100&quot;</span>); <span class="comment">// 100</span></span><br><span class="line"><span class="keyword">int</span> x2 = Integer.parseInt(<span class="string">&quot;100&quot;</span>, <span class="number">16</span>); <span class="comment">// 256,因为按16进制解析</span></span><br></pre></td></tr></table></figure>
<p><code>Integer</code>还可以把整数格式化为指定进制的字符串：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(Integer.toString(<span class="number">100</span>)); <span class="comment">// &quot;100&quot;,表示为10进制</span></span><br><span class="line">        System.out.println(Integer.toString(<span class="number">100</span>, <span class="number">36</span>)); <span class="comment">// &quot;2s&quot;,表示为36进制</span></span><br><span class="line">        System.out.println(Integer.toHexString(<span class="number">100</span>)); <span class="comment">// &quot;64&quot;,表示为16进制</span></span><br><span class="line">        System.out.println(Integer.toOctalString(<span class="number">100</span>)); <span class="comment">// &quot;144&quot;,表示为8进制</span></span><br><span class="line">        System.out.println(Integer.toBinaryString(<span class="number">100</span>)); <span class="comment">// &quot;1100100&quot;,表示为2进制</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="五javabean">五、JavaBean</h3>
<p>在Java中，有很多<code>class</code>的定义都符合这样的规范：</p>
<ul>
<li>若干<code>private</code>实例字段；</li>
<li>通过<code>public</code>方法来读写实例字段。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>.name; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123; <span class="keyword">this</span>.name = name; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>.age; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123; <span class="keyword">this</span>.age = age; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果读写方法符合以下这种命名规范, 那么这种<code>class</code>被称为<code>JavaBean</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 读方法:</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Type <span class="title">getXyz</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="comment">// 写方法:</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setXyz</span><span class="params">(Type value)</span></span></span><br></pre></td></tr></table></figure>
<p>​ 上面的字段是<code>xyz</code>，那么读写方法名分别以<code>get</code>和<code>set</code>开头，并且后接大写字母开头的字段名<code>Xyz</code>，因此两个读写方法名分别是<code>getXyz()</code>和<code>setXyz()</code>。</p>
<p><code>boolean</code>字段比较特殊，它的读方法一般命名为<code>isXyz()</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 读方法:</span><br><span class="line">public boolean isChild()</span><br><span class="line">// 写方法:</span><br><span class="line">public void setChild(boolean value)</span><br></pre></td></tr></table></figure>
<p>​ <strong>我们通常把一组对应的读方法（<code>getter</code>）和写方法（<code>setter</code>）称为属性（<code>property</code>）。例如，<code>name</code>属性</strong>：</p>
<ul>
<li>对应的读方法是<code>String getName()</code></li>
<li>对应的写方法是<code>setName(String)</code></li>
</ul>
<p><strong>只有<code>getter</code>的属性称为只读属性（read-only）</strong>，例如，定义一个age只读属性：</p>
<ul>
<li>对应的读方法是<code>int getAge()</code></li>
<li>无对应的写方法<code>setAge(int)</code></li>
</ul>
<p>类似的，只有<code>setter</code>的属性称为只写属性（write-only）。</p>
<h4 id="javabean-的作用">javabean 的作用：</h4>
<p>​ JavaBean主要用来传递数据，即把一组数据组合成一个JavaBean便于传输。此外，JavaBean可以方便地被IDE工具分析，生成读写属性的代码，主要用在图形界面的可视化设计中。</p>
<p>​ 通过IDE，可以快速生成<code>getter</code>和<code>setter</code></p>
<h4 id="如何枚举javabean的所有属性">如何枚举JavaBean的所有属性？</h4>
<p>要枚举一个JavaBean的所有属性，可以直接使用Java核心库提供的<code>Introspector</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        BeanInfo info = Introspector.getBeanInfo(Person.class);</span><br><span class="line">        <span class="keyword">for</span> (PropertyDescriptor pd : info.getPropertyDescriptors()) &#123;</span><br><span class="line">            System.out.println(pd.getName());</span><br><span class="line">            System.out.println(<span class="string">&quot;  &quot;</span> + pd.getReadMethod());</span><br><span class="line">            System.out.println(<span class="string">&quot;  &quot;</span> + pd.getWriteMethod());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">age</span></span><br><span class="line"><span class="comment">  public int Person.getAge()</span></span><br><span class="line"><span class="comment">  public void Person.setAge(int)</span></span><br><span class="line"><span class="comment">class</span></span><br><span class="line"><span class="comment">  public final native java.lang.Class java.lang.Object.getClass()</span></span><br><span class="line"><span class="comment">  null</span></span><br><span class="line"><span class="comment">name</span></span><br><span class="line"><span class="comment">  public java.lang.String Person.getName()</span></span><br><span class="line"><span class="comment">  public void Person.setName(java.lang.String) </span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<h3 id="六枚举类">六、枚举类</h3>
<h4 id="为什么需要枚举类">1、为什么需要枚举类：</h4>
<p>​ 在Java中，我们可以通过<code>static final</code>来定义常量,但有一个严重的问题就是，编译器无法检查每个值的合理性。比如说：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Weekday</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SUN = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MON = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TUE = <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> WED = <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> THU = <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> FRI = <span class="number">5</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SAT = <span class="number">6</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>注意到<code>Weekday</code>定义的常量范围是<code>0</code>~<code>6</code>，并不包含<code>7</code>，编译器无法检查不在枚举中的<code>int</code>值；</p></li>
<li><p>定义的常量仍可与其他变量比较，但其用途并非是枚举星期</p></li>
</ul>
<p>​ 为了让编译器能自动检查某个值在枚举的集合内，并且，不同用途的枚举需要不同的类型来标记，不能混用，我们可以使用<code>enum</code>来定义枚举类：</p>
<h4 id="如何定义枚举类">2、如何定义枚举类？</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Weekday day = Weekday.SUN;</span><br><span class="line">        <span class="keyword">if</span> (day == Weekday.SAT || day == Weekday.SUN) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Work at home!&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Work at office!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">Weekday</span> </span>&#123;</span><br><span class="line">    SUN, MON, TUE, WED, THU, FRI, SAT;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>定义枚举类是通过关键字<code>enum</code>实现的，我们只需依次列出枚举的常量名。</p>
<p>使用<code>enum</code>定义枚举有如下好处：</p>
<ul>
<li><p>首先，<code>enum</code>常量本身带有类型信息，即<code>Weekday.SUN</code>类型是<code>Weekday</code>，编译器会自动检查出类型错误。</p></li>
<li><p>其次，不可能引用到非枚举的值，因为无法通过编译。</p></li>
<li><p>最后，不同类型的枚举不能互相比较或者赋值，因为类型不符。</p></li>
</ul>
<h4 id="enum的比较">3、enum的比较：</h4>
<p>​ <code>enum</code>类型的每个常量在JVM中只有一个唯一实例，所以可以直接用<code>==</code>比较</p>
<h4 id="enum类型">4、enum类型：</h4>
<p>通过<code>enum</code>定义的枚举类，和其他的<code>class</code>没有任何区别。</p>
<p><code>enum</code>定义的类型就是<code>class</code>，只不过它有以下几个特点：</p>
<ul>
<li><p>定义的<code>enum</code>类型总是继承自<code>java.lang.Enum</code>，且无法被继承；</p></li>
<li><p>只能定义出<code>enum</code>的实例，而无法通过<code>new</code>操作符创建<code>enum</code>的实例；</p></li>
<li><p>定义的每个实例都是引用类型的唯一实例；</p></li>
<li><p>可以将<code>enum</code>类型用于<code>switch</code>语句。</p>
<p>例如，我们定义的<code>Color</code>枚举类：</p></li>
</ul>
<h4 id="enum示例">5、Enum示例：</h4>
<p>如下所示定义的enum:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">Color</span> </span>&#123;</span><br><span class="line">    RED, GREEN, BLUE;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么编译器编译出的<code>class</code>大概就像这样：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Color</span> <span class="keyword">extends</span> <span class="title">Enum</span> </span>&#123; <span class="comment">// 继承自Enum，标记为final class</span></span><br><span class="line">    <span class="comment">// 每个实例均为全局唯一:</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Color RED = <span class="keyword">new</span> Color();</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Color GREEN = <span class="keyword">new</span> Color();</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Color BLUE = <span class="keyword">new</span> Color();</span><br><span class="line">    <span class="comment">// private构造方法，确保外部无法调用new操作符:</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Color</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以，编译后的<code>enum</code>类和普通<code>class</code>并没有任何区别。但是我们自己无法按定义普通<code>class</code>那样来定义<code>enum</code>，必须使用<code>enum</code>关键字，这是Java语法规定的。</p>
<p>因为<strong><code>enum</code>是一个<code>class</code>，每个枚举的值都是<code>class</code>实例</strong>，因此，这些实例有一些方法：</p>
<ul>
<li>name()</li>
</ul>
<p>返回常量名，例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s = Weekday.SUN.name(); <span class="comment">// &quot;SUN&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>ordinal()</li>
</ul>
<p>返回定义的常量的顺序，从0开始计数，例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> n = Weekday.MON.ordinal(); <span class="comment">// 1</span></span><br></pre></td></tr></table></figure>
<p>改变枚举常量定义的顺序就会导致<code>ordinal()</code>返回值发生变化。例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">Weekday</span> </span>&#123;</span><br><span class="line">    SUN, MON, TUE, WED, THU, FRI, SAT;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">Weekday</span> </span>&#123;</span><br><span class="line">    MON, TUE, WED, THU, FRI, SAT, SUN;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 的<code>ordinal</code>就是不同的。如果在代码中编写了类似<code>if(x.ordinal()==1)</code>这样的语句，就要保证<code>enum</code>的枚举顺序不能变。新增的常量必须放在最后。</p>
<h4 id="书写健壮的enum代码">6、书写健壮的Enum代码</h4>
<p><code>Weekday</code>的枚举常量如果要和<code>int</code>转换，使用<code>ordinal()</code>不是非常方便, 比如这样写：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String task = Weekday.MON.ordinal() + <span class="string">&quot;/ppt&quot;</span>;</span><br><span class="line">saveToFile(task);</span><br></pre></td></tr></table></figure>
<p>​ 但是，如果不小心修改了枚举的顺序，编译器是无法检查出这种逻辑错误的。<strong>要编写健壮的代码，就不要依靠<code>ordinal()</code>的返回值。</strong> <strong>因为<code>enum</code>本身是<code>class</code>，所以我们可以定义<code>private</code>的构造方法，并且，给每个枚举常量添加字段：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Weekday day = Weekday.SUN;</span><br><span class="line">        <span class="keyword">if</span> (day.dayValue == <span class="number">6</span> || day.dayValue == <span class="number">0</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Work at home!&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Work at office!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">Weekday</span> </span>&#123;</span><br><span class="line">    MON(<span class="number">1</span>), TUE(<span class="number">2</span>), WED(<span class="number">3</span>), THU(<span class="number">4</span>), FRI(<span class="number">5</span>), SAT(<span class="number">6</span>), SUN(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> dayValue;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Weekday</span><span class="params">(<span class="keyword">int</span> dayValue)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.dayValue = dayValue;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这样就无需担心顺序的变化，新增枚举常量时，也需要指定一个<code>int</code>值。</p>
<p>默认情况下，对枚举常量调用<code>toString()</code>会返回和<code>name()</code>一样的字符串。但是，<code>toString()</code>可以被覆写，而<code>name()</code>则不行。我们可以给<code>Weekday</code>添加<code>toString()</code>方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">Weekday</span> </span>&#123;</span><br><span class="line">    MON(<span class="number">1</span>, <span class="string">&quot;星期一&quot;</span>), TUE(<span class="number">2</span>, <span class="string">&quot;星期二&quot;</span>), WED(<span class="number">3</span>, <span class="string">&quot;星期三&quot;</span>), THU(<span class="number">4</span>, <span class="string">&quot;星期四&quot;</span>), FRI(<span class="number">5</span>, <span class="string">&quot;星期五&quot;</span>), SAT(<span class="number">6</span>, <span class="string">&quot;星期六&quot;</span>), SUN(<span class="number">0</span>, <span class="string">&quot;星期日&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> dayValue;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String chinese;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Weekday</span><span class="params">(<span class="keyword">int</span> dayValue, String chinese)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.dayValue = dayValue;</span><br><span class="line">        <span class="keyword">this</span>.chinese = chinese;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.chinese;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="enum适合用在switch语句中">7、<code>enum</code>适合用在<code>switch</code>语句中</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Weekday day = Weekday.SUN;</span><br><span class="line">        <span class="keyword">switch</span>(day) &#123;</span><br><span class="line">        <span class="keyword">case</span> MON:</span><br><span class="line">        <span class="keyword">case</span> TUE:</span><br><span class="line">        <span class="keyword">case</span> WED:</span><br><span class="line">        <span class="keyword">case</span> THU:</span><br><span class="line">        <span class="keyword">case</span> FRI:</span><br><span class="line">            System.out.println(<span class="string">&quot;Today is &quot;</span> + day + <span class="string">&quot;. Work at office!&quot;</span>);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> SAT:</span><br><span class="line">        <span class="keyword">case</span> SUN:</span><br><span class="line">            System.out.println(<span class="string">&quot;Today is &quot;</span> + day + <span class="string">&quot;. Work at home!&quot;</span>);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;cannot process &quot;</span> + day);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">Weekday</span> </span>&#123;</span><br><span class="line">    MON, TUE, WED, THU, FRI, SAT, SUN;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="七记录类">七、记录类</h3>
<h4 id="以往类的繁琐之处">1、以往类的繁琐之处：</h4>
<p>使用<code>String</code>、<code>Integer</code>等类型的时候，这些类型都是不变类，一个不变类具有以下特点：</p>
<ol type="1">
<li>定义class时使用<code>final</code>，无法派生子类；</li>
<li>每个字段使用<code>final</code>，保证创建实例后无法修改任何字段。</li>
</ol>
<p>假设我们希望定义一个<code>Point</code>类，有<code>x</code>、<code>y</code>两个变量，同时它是一个不变类，可以这么写：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Point</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> x;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> y;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Point</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.x = x;</span><br><span class="line">        <span class="keyword">this</span>.y = y;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">x</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.x;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">y</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.y;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里演示<code>Point</code>不变类的写法目的是，这些代码写起来都非常简单，但是很繁琐。</p>
<h4 id="record-记录类">2、record 记录类:</h4>
<p>​ 从Java 14开始，引入了新的<code>Record</code>类。我们定义<code>Record</code>类时，使用关键字<code>record</code>。把上述<code>Point</code>类改写为<code>Record</code>类，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Point p = <span class="keyword">new</span> Point(<span class="number">123</span>, <span class="number">456</span>);</span><br><span class="line">        System.out.println(p.x());</span><br><span class="line">        System.out.println(p.y());</span><br><span class="line">        System.out.println(p);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> record <span class="title">Point</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<p>如果要把上述定义，以class的形式改写代码的话，应当如下所示：（其实也就是编译器会帮我们编译成如下的代码）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public final class Point extends Record &#123;</span><br><span class="line">    private final int x;</span><br><span class="line">    private final int y;</span><br><span class="line"></span><br><span class="line">    public Point(int x, int y) &#123;</span><br><span class="line">        this.x = x;</span><br><span class="line">        this.y = y;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public int x() &#123;</span><br><span class="line">        return this.x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public int y() &#123;</span><br><span class="line">        return this.y;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return String.format(&quot;Point[x=%s, y=%s]&quot;, x, y);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public boolean equals(Object o) &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">    public int hashCode() &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 除了用<code>final</code>修饰class以及每个字段外，编译器还自动为我们创建了构造方法，和字段名同名的方法，以及覆写<code>toString()</code>、<code>equals()</code>和<code>hashCode()</code>方法。</p>
<p>​ <strong>换句话说，使用<code>record</code>关键字，可以一行写出一个不变类。</strong></p>
<p>​ <strong>和<code>enum</code>类似，我们自己不能直接从<code>Record</code>派生，只能通过<code>record</code>关键字由编译器实现继承。</strong></p>
<h4 id="构造方法">3、构造方法：</h4>
<p>​ 编译器默认按照<code>record</code>声明的变量顺序自动创建一个构造方法，并在方法内给字段赋值。那么问题来了，如果我们要检查参数，应该怎么办？</p>
<p>​ <strong>假设<code>Point</code>类的<code>x</code>、<code>y</code>不允许负数，我们就得给<code>Point</code>的构造方法加上检查逻辑：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> record <span class="title">Point</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> Point &#123;</span><br><span class="line">        <span class="keyword">if</span> (x &lt; <span class="number">0</span> || y &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 注意到方法<code>public Point &#123;...&#125;</code>被称为Compact Constructor，它的目的是让我们编写检查逻辑，<strong>编译器最终生成的构造方法如下：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Point</span> <span class="keyword">extends</span> <span class="title">Record</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Point</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 这是我们编写的Compact Constructor:</span></span><br><span class="line">        <span class="keyword">if</span> (x &lt; <span class="number">0</span> || y &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 这是编译器继续生成的赋值代码:</span></span><br><span class="line">        <span class="keyword">this</span>.x = x;</span><br><span class="line">        <span class="keyword">this</span>.y = y;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>作为<code>record</code>的<code>Point</code>仍然可以添加静态方法。一种常用的静态方法是<code>of()</code>方法，用来创建<code>Point</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> record <span class="title">Point</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Point <span class="title">of</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Point(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Point <span class="title">of</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Point(x, y);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样我们可以写出更简洁的代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> z = Point.of();</span><br><span class="line"><span class="keyword">var</span> p = Point.of(<span class="number">123</span>, <span class="number">456</span>);</span><br></pre></td></tr></table></figure>
<h4 id="总结">4、总结：</h4>
<p>从Java 14开始，提供新的<code>record</code>关键字，可以非常方便地定义Data Class：</p>
<ul>
<li>使用<code>record</code>定义的是不变类；</li>
<li>可以编写Compact Constructor对参数进行验证；</li>
<li>可以定义静态方法。</li>
</ul>
<h3 id="八biginteger">八、BigInteger</h3>
<p>​ 在Java中，由CPU原生提供的整型最大范围是64位<code>long</code>型整数。<strong>使用<code>long</code>型整数可以直接通过CPU指令进行计算，速度非常快</strong></p>
<p>​ 如果我们使用的整数范围超过了<code>long</code>型，就只能用软件来模拟一个大整数。<code>java.math.BigInteger</code>就是用来表示任意大小的整数。<code>BigInteger</code>内部用一个<code>int[]</code>数组来模拟一个非常大的整数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BigInteger bi = <span class="keyword">new</span> BigInteger(<span class="string">&quot;1234567890&quot;</span>);</span><br><span class="line">System.out.println(bi.pow(<span class="number">5</span>)); <span class="comment">// 2867971860299718107233761438093672048294900000</span></span><br></pre></td></tr></table></figure>
<p>对<code>BigInteger</code>做运算的时候，只能使用实例方法，例如，加法运算：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BigInteger i1 = <span class="keyword">new</span> BigInteger(<span class="string">&quot;1234567890&quot;</span>);</span><br><span class="line">BigInteger i2 = <span class="keyword">new</span> BigInteger(<span class="string">&quot;12345678901234567890&quot;</span>);</span><br><span class="line">BigInteger sum = i1.add(i2); <span class="comment">// 12345678902469135780</span></span><br></pre></td></tr></table></figure>
<p><code>BigInteger</code>和<code>Integer</code>、<code>Long</code>一样，也是不可变类，并且也继承自<code>Number</code>类。因为<code>Number</code>定义了转换为基本类型的几个方法：</p>
<ul>
<li>转换为<code>byte</code>：<code>byteValue()</code></li>
<li>转换为<code>short</code>：<code>shortValue()</code></li>
<li>转换为<code>int</code>：<code>intValue()</code></li>
<li>转换为<code>long</code>：<code>longValue()</code></li>
<li>转换为<code>float</code>：<code>floatValue()</code></li>
<li>转换为<code>double</code>：<code>doubleValue()</code></li>
</ul>
<p>​ 因此，通过上述方法，可以把<code>BigInteger</code>转换成基本类型。如果<code>BigInteger</code>表示的范围超过了基本类型的范围，转换时将丢失高位信息，即结果不一定是准确的。如果需要准确地转换成基本类型，可以使用<code>intValueExact()</code>、<code>longValueExact()</code>等方法，在转换时如果超出范围，将直接抛出<code>ArithmeticException</code>异常。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BigInteger i = <span class="keyword">new</span> BigInteger(<span class="string">&quot;123456789000&quot;</span>);</span><br><span class="line">System.out.println(i.longValue()); <span class="comment">// 123456789000</span></span><br><span class="line">System.out.println(i.multiply(i).longValueExact()); <span class="comment">// java.lang.ArithmeticException: BigInteger out of long range</span></span><br></pre></td></tr></table></figure>
<p>​ 使用<code>longValueExact()</code>方法时，如果超出了<code>long</code>型的范围，会抛出<code>ArithmeticException</code>。</p>
<h3 id="九bigdecimal">九、BigDecimal</h3>
<h4 id="bigdecimal简介">1、BigDecimal简介</h4>
<p>和<code>BigInteger</code>类似，<code>BigDecimal</code>可以表示<strong>一个任意大小且精度完全准确</strong>的浮点数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BigDecimal bd = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;123.4567&quot;</span>);</span><br><span class="line">System.out.println(bd.multiply(bd)); <span class="comment">// 15241.55677489</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>BigDecimal</code>用<code>scale()</code>输出小数位数，例如：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BigDecimal d1 = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;123.45&quot;</span>);</span><br><span class="line">BigDecimal d2 = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;123.4500&quot;</span>);</span><br><span class="line">BigDecimal d3 = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;1234500&quot;</span>);</span><br><span class="line">System.out.println(d1.scale()); <span class="comment">// 2,两位小数</span></span><br><span class="line">System.out.println(d2.scale()); <span class="comment">// 4</span></span><br><span class="line">System.out.println(d3.scale()); <span class="comment">// 0</span></span><br></pre></td></tr></table></figure>
<ul>
<li>通过<code>BigDecimal</code>的<code>stripTrailingZeros()</code>方法，可以将一个<code>BigDecimal</code>格式化为一个相等的，但去掉了末尾0的<code>BigDecimal</code>：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BigDecimal d1 = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;123.4500&quot;</span>);</span><br><span class="line">BigDecimal d2 = d1.stripTrailingZeros();</span><br><span class="line">System.out.println(d1.scale()); <span class="comment">// 4</span></span><br><span class="line">System.out.println(d2.scale()); <span class="comment">// 2,因为去掉了00</span></span><br><span class="line"></span><br><span class="line">BigDecimal d3 = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;1234500&quot;</span>);</span><br><span class="line">BigDecimal d4 = d3.stripTrailingZeros();</span><br><span class="line">System.out.println(d3.scale()); <span class="comment">// 0</span></span><br><span class="line">System.out.println(d4.scale()); <span class="comment">// -2</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>如果一个<code>BigDecimal</code>的<code>scale()</code>返回负数，例如，<code>-2</code>，表示这个数是个整数，并且末尾有2个0。</p></li>
<li><p>可以对一个<code>BigDecimal</code>设置它的<code>scale</code>，如果精度比原始值低，那么按照指定的方法进行四舍五入或者直接截断：</p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        BigDecimal d1 = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;123.456789&quot;</span>);</span><br><span class="line">        BigDecimal d2 = d1.setScale(<span class="number">4</span>, RoundingMode.HALF_UP); <span class="comment">// 四舍五入，123.4568</span></span><br><span class="line">        BigDecimal d3 = d1.setScale(<span class="number">4</span>, RoundingMode.DOWN); <span class="comment">// 直接截断，123.4567</span></span><br><span class="line">        System.out.println(d2);</span><br><span class="line">        System.out.println(d3);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对<code>BigDecimal</code>做加、减、乘时，精度不会丢失，但是做除法时，存在无法除尽的情况，这时，就必须指定精度以及如何进行截断：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BigDecimal d1 = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;123.456&quot;</span>);</span><br><span class="line">BigDecimal d2 = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;23.456789&quot;</span>);</span><br><span class="line">BigDecimal d3 = d1.divide(d2, <span class="number">10</span>, RoundingMode.HALF_UP); <span class="comment">// 保留10位小数并四舍五入</span></span><br><span class="line">BigDecimal d4 = d1.divide(d2); <span class="comment">// 报错：ArithmeticException，因为除不尽</span></span><br></pre></td></tr></table></figure>
<p>还可以对<code>BigDecimal</code>做除法的同时求余数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        BigDecimal n = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;12.345&quot;</span>);</span><br><span class="line">        BigDecimal m = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;0.12&quot;</span>);</span><br><span class="line">        BigDecimal[] dr = n.divideAndRemainder(m);</span><br><span class="line">        System.out.println(dr[<span class="number">0</span>]); <span class="comment">// 102</span></span><br><span class="line">        System.out.println(dr[<span class="number">1</span>]); <span class="comment">// 0.105</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用<code>divideAndRemainder()</code>方法时，返回的数组包含两个<code>BigDecimal</code>，分别是商和余数，其中商总是整数，余数不会大于除数。我们可以利用这个方法判断两个<code>BigDecimal</code>是否是整数倍数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BigDecimal n = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;12.75&quot;</span>);</span><br><span class="line">BigDecimal m = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;0.15&quot;</span>);</span><br><span class="line">BigDecimal[] dr = n.divideAndRemainder(m);</span><br><span class="line"><span class="keyword">if</span> (dr[<span class="number">1</span>].signum() == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// n是m的整数倍</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="比较bigdecimal">2、比较BigDecimal</h4>
<p>在比较两个<code>BigDecimal</code>的值是否相等时，要特别注意，使用<code>equals()</code>方法不但要求两个<code>BigDecimal</code>的值相等，还要求它们的<code>scale()</code>相等：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BigDecimal d1 = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;123.456&quot;</span>);</span><br><span class="line">BigDecimal d2 = <span class="keyword">new</span> BigDecimal(<span class="string">&quot;123.45600&quot;</span>);</span><br><span class="line">System.out.println(d1.equals(d2)); <span class="comment">// false,因为scale不同</span></span><br><span class="line">System.out.println(d1.equals(d2.stripTrailingZeros())); <span class="comment">// true,因为d2去除尾部0后scale变为2</span></span><br><span class="line">System.out.println(d1.compareTo(d2)); <span class="comment">// 0</span></span><br></pre></td></tr></table></figure>
<p>必须使用<code>compareTo()</code>方法来比较，它根据两个值的大小分别返回负数、正数和<code>0</code>，分别表示小于、大于和等于。</p>
<p>总是使用compareTo()比较两个BigDecimal的值，不要使用equals()！</p>
<p>如果查看<code>BigDecimal</code>的源码，可以发现，实际上一个<code>BigDecimal</code>是通过一个<code>BigInteger</code>和一个<code>scale</code>来表示的，即<code>BigInteger</code>表示一个完整的整数，而<code>scale</code>表示小数位数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BigDecimal</span> <span class="keyword">extends</span> <span class="title">Number</span> <span class="keyword">implements</span> <span class="title">Comparable</span>&lt;<span class="title">BigDecimal</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> BigInteger intVal;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> scale;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>BigDecimal</code>也是从<code>Number</code>继承的，也是不可变对象。</p>
<h3 id="十常用工具类">十、常用工具类</h3>
<h4 id="math">1、Math</h4>
<ul>
<li>求绝对值：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Math.abs(-<span class="number">100</span>); <span class="comment">// 100</span></span><br><span class="line">Math.abs(-<span class="number">7.8</span>); <span class="comment">// 7.8</span></span><br></pre></td></tr></table></figure>
<ul>
<li>取最大或最小值:</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Math.max(<span class="number">100</span>, <span class="number">99</span>); <span class="comment">// 100</span></span><br><span class="line">Math.min(<span class="number">1.2</span>, <span class="number">2.3</span>); <span class="comment">// 1.2</span></span><br></pre></td></tr></table></figure>
<ul>
<li>计算<span class="math inline">\(x^y\)</span>次方：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Math.pow(<span class="number">2</span>, <span class="number">10</span>); <span class="comment">// 2的10次方=1024</span></span><br></pre></td></tr></table></figure>
<ul>
<li>计算√x：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Math.sqrt(<span class="number">2</span>); <span class="comment">// 1.414...</span></span><br></pre></td></tr></table></figure>
<ul>
<li>计算ex次方：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Math.exp(<span class="number">2</span>); <span class="comment">// 7.389...</span></span><br></pre></td></tr></table></figure>
<ul>
<li>计算以e为底的对数：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Math.log(<span class="number">4</span>); <span class="comment">// 1.386...</span></span><br></pre></td></tr></table></figure>
<ul>
<li>计算以10为底的对数：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Math.log10(<span class="number">100</span>); <span class="comment">// 2</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Math还提供了几个数学常量：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> pi = Math.PI; <span class="comment">// 3.14159...</span></span><br><span class="line"><span class="keyword">double</span> e = Math.E; <span class="comment">// 2.7182818...</span></span><br><span class="line">Math.sin(Math.PI / <span class="number">6</span>); <span class="comment">// sin(π/6) = 0.5</span></span><br></pre></td></tr></table></figure>
<ul>
<li>生成一个随机数x，x的范围是<code>0 &lt;= x &lt; 1</code>：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Math.random(); <span class="comment">// 0.53907... 每次都不一样</span></span><br></pre></td></tr></table></figure>
<h4 id="random">2、Random</h4>
<p><code>Random</code>用来创建伪随机数。所谓伪随机数，是指只要给定一个初始的种子，产生的随机数序列是完全一样的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Random r = <span class="keyword">new</span> Random();</span><br><span class="line">r.nextInt(); <span class="comment">// 2071575453,每次都不一样</span></span><br><span class="line">r.nextInt(<span class="number">10</span>); <span class="comment">// 5,生成一个[0,10)之间的int</span></span><br><span class="line">r.nextLong(); <span class="comment">// 8811649292570369305,每次都不一样</span></span><br><span class="line">r.nextFloat(); <span class="comment">// 0.54335...生成一个[0,1)之间的float</span></span><br><span class="line">r.nextDouble(); <span class="comment">// 0.3716...生成一个[0,1)之间的double</span></span><br></pre></td></tr></table></figure>
<p>这是因为我们创建<code>Random</code>实例时，如果不给定种子，就使用系统当前时间戳作为种子，因此每次运行时，种子不同，得到的伪随机数序列就不同。</p>
<p>如果我们在创建<code>Random</code>实例时指定一个种子，就会得到完全确定的随机数序列：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Random r = <span class="keyword">new</span> Random(<span class="number">12345</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            System.out.println(r.nextInt(<span class="number">100</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 51, 80, 41, 28, 55...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="securerandom">3、SecureRandom</h4>
<p>​ 有伪随机数，就有真随机数。实际上真正的真随机数只能通过量子力学原理来获取，而我们想要的是一个不可预测的安全的随机数，<code>SecureRandom</code>就是用来创建安全的随机数的：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SecureRandom sr = <span class="keyword">new</span> SecureRandom();</span><br><span class="line">System.out.println(sr.nextInt(<span class="number">100</span>));</span><br></pre></td></tr></table></figure>
<p><code>SecureRandom</code>的安全性是通过操作系统提供的安全的随机种子来生成随机数。这个种子是通过CPU的热噪声、读写磁盘的字节、网络流量等各种随机事件产生的“熵”。</p>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>core class</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记2——java面向对象编程基础</title>
    <url>/2022/01/29/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B02%E2%80%94java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1260467032946976</p>
<h3 id="一类的方法">一、类的方法</h3>
<h4 id="一个类通过定义public方法就可以给外部代码暴露一些操作的接口">1、一个类通过定义public方法，就可以给外部代码暴露一些操作的接口。</h4>
<p>一般而言内部的变量设置为private，对内部变量的操作由暴露的接口进行，这样内部能够自己保证逻辑一致性。</p>
<p>在public的方法内部，我们就有机会检查参数对不对。比如：<code>setName()</code>方法可以做检查，例如，不允许传入<code>null</code>和空字符串：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (name == <span class="keyword">null</span> || name.isBlank()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;invalid name&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.name = name.strip(); <span class="comment">// 去掉首尾空格</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="在方法内部可以使用一个隐含的变量this它始终指向当前实例">2、在方法内部，可以使用一个隐含的变量<code>this</code>，它始终指向当前实例</h4>
<p>如果没有命名冲突，可以省略<code>this</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name; <span class="comment">// 相当于this.name</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="可变参数">3、可变参数：</h4>
<p>可变参数用<code>类型...</code>定义，可变参数相当于数组类型,可变参数可以保证无法传入<code>null</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Group</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String[] names;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setNames</span><span class="params">(String... names)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.names = names;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Group g = <span class="keyword">new</span> Group();</span><br><span class="line">g.setNames(<span class="string">&quot;Xiao Ming&quot;</span>, <span class="string">&quot;Xiao Hong&quot;</span>, <span class="string">&quot;Xiao Jun&quot;</span>); <span class="comment">// 传入3个String</span></span><br><span class="line">g.setNames(<span class="string">&quot;Xiao Ming&quot;</span>, <span class="string">&quot;Xiao Hong&quot;</span>); <span class="comment">// 传入2个String</span></span><br><span class="line">g.setNames(<span class="string">&quot;Xiao Ming&quot;</span>); <span class="comment">// 传入1个String</span></span><br><span class="line">g.setNames(); <span class="comment">// 传入0个String</span></span><br></pre></td></tr></table></figure>
<h4 id="参数传递">4、参数传递：</h4>
<p>引用类型参数的传递，调用方的变量，和接收方的参数变量，指向的是同一个对象。双方任意一方对这个对象的修改，都会影响对方（因为指向同一个对象嘛）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 一个陷阱题目</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Person p = <span class="keyword">new</span> Person();</span><br><span class="line">        String bob = <span class="string">&quot;Bob&quot;</span>;</span><br><span class="line">        p.setName(bob); <span class="comment">// 传入bob变量</span></span><br><span class="line">        System.out.println(p.getName()); <span class="comment">// &quot;Bob&quot;</span></span><br><span class="line">        bob = <span class="string">&quot;Alice&quot;</span>; <span class="comment">// bob改名为Alice</span></span><br><span class="line">        System.out.println(p.getName()); <span class="comment">// &quot;Bob&quot;还是&quot;Alice&quot;?</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//回答： 应该是Alice：</span></span><br><span class="line"><span class="comment">// String bob = &quot;Bob&quot;; 的时候，为其分配内存，然后bob 是一个指针，指向该内存，比如0x0011.</span></span><br><span class="line"><span class="comment">// bob = &quot;Alice&quot; 时发生了重新赋值</span></span><br><span class="line"><span class="comment">// 但在java中String类型是不可变的，当发生改变时，会重新分配内存，即生成一个新的内存地址，例如：0x0022,这个时候，现在的bob的内存指向&quot;Alice&quot;，也就是指向0x0022，而p.bob的内存指向仍为0x0011，也就是Bob，所以输出的依旧是Bob。</span></span><br></pre></td></tr></table></figure>
<h3 id="二构造方法">二、构造方法：</h3>
<h4 id="没有在构造方法中初始化字段时引用类型的字段默认是null数值类型的字段用默认值int类型默认值是0布尔类型默认值是false">1、没有在构造方法中初始化字段时，引用类型的字段默认是<code>null</code>，数值类型的字段用默认值，<code>int</code>类型默认值是<code>0</code>，布尔类型默认值是<code>false</code>：</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name; <span class="comment">// 默认初始化为null</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age; <span class="comment">// 默认初始化为0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="可以定义多个构造方法">2、可以定义多个构造方法：</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="既对字段进行初始化又在构造方法中对字段进行初始化">3、既对字段进行初始化，又在构造方法中对字段进行初始化：</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Person &#123;</span><br><span class="line">    private String name = &quot;Unamed&quot;;</span><br><span class="line">    private int age = 10;</span><br><span class="line"></span><br><span class="line">    public Person(String name, int age) &#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">        this.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当我们创建对象的时候，<code>new Person("Xiao Ming", 12)</code>得到的对象实例，字段的初始值是啥？</p>
<p>在Java中，创建对象实例的时候，按照如下顺序进行初始化：</p>
<ol type="1">
<li>先初始化字段，例如，<code>int age = 10;</code>表示字段初始化为<code>10</code>，<code>double salary;</code>表示字段默认初始化为<code>0</code>，<code>String name;</code>表示引用类型字段默认初始化为<code>null</code>；</li>
<li>执行构造方法的代码进行初始化。</li>
</ol>
<p>​ 因此，<strong>构造方法的代码由于后运行</strong>，所以，<code>new Person("Xiao Ming", 12)</code>的字段值最终由构造方法的代码确定。</p>
<h4 id="一个构造方法可以调用其他构造方法这样做的目的是便于代码复用调用其他构造方法的语法是this">4、一个构造方法可以调用其他构造方法，这样做的目的是便于代码复用。调用其他构造方法的语法是<code>this(…)</code>：</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(name, <span class="number">18</span>); <span class="comment">// 调用另一个构造方法Person(String, int)</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(<span class="string">&quot;Unnamed&quot;</span>); <span class="comment">// 调用另一个构造方法Person(String)</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="三方法重载overload">三、方法重载Overload：</h3>
<ul>
<li><p>方法重载是指多个方法的方法名相同，但各自的参数不同；</p></li>
<li><p>重载方法应该完成类似的功能，参考<code>String</code>的<code>indexOf()</code>；</p></li>
<li><p><strong>重载方法返回值类型应该相同,各自的参数应当不同</strong></p></li>
</ul>
<h3 id="四继承">四、继承：</h3>
<h4 id="基础继承与protected关键字">1、基础继承与Protected关键字：</h4>
<ul>
<li><p>Java使用<code>extends</code>关键字来实现继承：</p></li>
<li><p>注意：子类自动获得了父类的所有字段，严禁定义与父类重名的字段！</p></li>
<li><p>Java只允许一个class继承自一个类，因此，一个类有且仅有一个父类。</p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;...&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;...&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;...&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;...&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 不要重复name和age字段/方法,</span></span><br><span class="line">    <span class="comment">// 只需要定义新增score字段/方法:</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> score;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getScore</span><span class="params">()</span> </span>&#123; … &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setScore</span><span class="params">(<span class="keyword">int</span> score)</span> </span>&#123; … &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>子类无法访问父类的<code>private</code>字段或者<code>private</code>方法</li>
<li>父类中用<code>protected</code>修饰的字段可以被子类访问</li>
<li><code>protected</code>关键字可以把字段和方法的访问权限控制在继承树内部，一个<code>protected</code>字段和方法可以被其子类，以及子类的子类所访问</li>
</ul>
<h4 id="super关键字">2、Super关键字：</h4>
<ul>
<li><p><code>super</code>关键字表示父类（超类）。子类引用父类的字段时，可以用<code>super.fieldName</code>。</p></li>
<li><p>观察以下代码，为何会编译错误？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Student s = <span class="keyword">new</span> Student(<span class="string">&quot;Xiao Ming&quot;</span>, <span class="number">12</span>, <span class="number">89</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> String name;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">int</span> score;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">(String name, <span class="keyword">int</span> age, <span class="keyword">int</span> score)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.score = score;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>在Java中，任何<code>class</code>的构造方法，第一行语句必须是调用父类的构造方法。如果没有明确地调用父类的构造方法，编译器会帮我们自动加一句<code>super();</code>，所以，<code>Student</code>类的构造方法实际上是这样：</p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">int</span> score;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">(String name, <span class="keyword">int</span> age, <span class="keyword">int</span> score)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(); <span class="comment">// 自动调用父类的构造方法</span></span><br><span class="line">        <span class="keyword">this</span>.score = score;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但是，<code>Person</code>类并没有无参数的构造方法，因此，编译失败。</p>
<p>解决方法是调用<code>Person</code>类存在的某个构造方法。例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">int</span> score;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">(String name, <span class="keyword">int</span> age, <span class="keyword">int</span> score)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(name, age); <span class="comment">// 调用父类的构造方法Person(String, int)</span></span><br><span class="line">        <span class="keyword">this</span>.score = score;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>结论：如果父类没有默认的构造方法，子类就必须显式调用<code>super()</code>并给出参数以便让编译器定位到父类的一个合适的构造方法。且子类不会继承任何父类的构造方法。子类默认的构造方法是编译器自动生成的，不是继承的。</li>
</ul>
<h4 id="阻止继承">3、阻止继承：</h4>
<ul>
<li>正常情况下，只要某个class没有<code>final</code>修饰符，那么任何类都可以从该class继承。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> String name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// compile error: 不允许继承自Person</span></span><br><span class="line">Student extends Person &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>从Java 15开始，允许使用<code>sealed</code>修饰class，并通过<code>permits</code>明确写出能够从该class继承的子类名称。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> sealed <span class="class"><span class="keyword">class</span> <span class="title">Shape</span> <span class="title">permits</span> <span class="title">Rect</span>, <span class="title">Circle</span>, <span class="title">Triangle</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：这种<code>sealed</code>类主要用于一些框架，防止继承被滥用。</li>
</ul>
<p><code>sealed</code>类在Java 15中目前是预览状态，要启用它，必须使用参数<code>--enable-preview</code>和<code>--source 15</code>。</p>
<h4 id="向上转型">4、向上转型：</h4>
<p>​ 把一个子类类型赋值给父类类型的变量，被称为向上转型（upcasting）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Student s = <span class="keyword">new</span> Student();</span><br><span class="line">Person p = s; <span class="comment">// upcasting, ok</span></span><br><span class="line">Object o1 = p; <span class="comment">// upcasting, ok</span></span><br><span class="line">Object o2 = s; <span class="comment">// upcasting, ok</span></span><br></pre></td></tr></table></figure>
<p>​ 注意到继承树是<code>Student &gt; Person &gt; Object</code>，所以，可以把<code>Student</code>类型转型为<code>Person</code>，或者更高层次的<code>Object</code>。</p>
<h4 id="向下转型">5、向下转型：</h4>
<p>​ 如果把一个父类类型强制转型为子类类型，就是向下转型（downcasting）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Person p1 = <span class="keyword">new</span> Student(); <span class="comment">// upcasting, ok</span></span><br><span class="line">Person p2 = <span class="keyword">new</span> Person();</span><br><span class="line">Student s1 = (Student) p1; <span class="comment">// ok</span></span><br><span class="line">Student s2 = (Student) p2; <span class="comment">// runtime error! ClassCastException!</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>Person</code>类型<code>p1</code>实际指向<code>Student</code>实例，</p></li>
<li><p><code>Person</code>类型变量<code>p2</code>实际指向<code>Person</code>实例。</p></li>
</ul>
<p>在向下转型的时候：</p>
<ul>
<li><p>把<code>p1</code>转型为<code>Student</code>会成功，因为<code>p1</code>确实指向<code>Student</code>实例，</p></li>
<li><p>把<code>p2</code>转型为<code>Student</code>会失败，因为<code>p2</code>的实际类型是<code>Person</code>，不能把父类变为子类，因为子类功能比父类多，多的功能无法凭空变出来。</p></li>
</ul>
<p>因此，向下转型很可能会失败。失败的时候，Java虚拟机会报<code>ClassCastException</code>。</p>
<p>为了避免向下转型出错，Java提供了<code>instanceof</code>操作符，可以先判断一个实例究竟是不是某种类型：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Person p = <span class="keyword">new</span> Person();</span><br><span class="line">System.out.println(p <span class="keyword">instanceof</span> Person); <span class="comment">// true</span></span><br><span class="line">System.out.println(p <span class="keyword">instanceof</span> Student); <span class="comment">// false</span></span><br><span class="line"></span><br><span class="line">Student s = <span class="keyword">new</span> Student();</span><br><span class="line">System.out.println(s <span class="keyword">instanceof</span> Person); <span class="comment">// true</span></span><br><span class="line">System.out.println(s <span class="keyword">instanceof</span> Student); <span class="comment">// true</span></span><br><span class="line"></span><br><span class="line">Student n = <span class="keyword">null</span>;</span><br><span class="line">System.out.println(n <span class="keyword">instanceof</span> Student); <span class="comment">// false</span></span><br></pre></td></tr></table></figure>
<p><code>instanceof</code>实际上判断一个变量所指向的实例是否是指定类型，或者这个类型的子类。如果一个引用变量为<code>null</code>，那么对任何<code>instanceof</code>的判断都为<code>false</code>。</p>
<p>利用<code>instanceof</code>，在向下转型前可以先判断：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Person p = <span class="keyword">new</span> Student();</span><br><span class="line"><span class="keyword">if</span> (p <span class="keyword">instanceof</span> Student) &#123;</span><br><span class="line">    <span class="comment">// 只有判断成功才会向下转型:</span></span><br><span class="line">    Student s = (Student) p; <span class="comment">// 一定会成功</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="五多态">五、多态：</h3>
<h4 id="覆写-override">1、覆写 Override</h4>
<p>在继承关系中，子类如果定义了一个与父类方法签名完全相同的方法，被称为覆写（Override）。</p>
<ul>
<li><p>OverRide传入参数与返回值都应该相同。</p></li>
<li><p>加上<code>@Override</code>可以让编译器帮助检查是否进行了正确的覆写。希望进行覆写，但是不小心写错了方法签名，编译器会报错。</p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span> <span class="comment">// Compile error!</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(String s)</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="多态">2、多态</h4>
<p>Java的实例方法调用是基于运行时的实际类型的动态调用，而非变量的声明类型。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Person p = <span class="keyword">new</span> Student();</span><br><span class="line">p.run(); <span class="comment">// 运行的时Student的run()方法</span></span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">runTwice</span><span class="params">(Person p)</span> </span>&#123;</span><br><span class="line">    p.run();</span><br><span class="line">    p.run(); <span class="comment">//它传入的参数类型是Person，我们是无法知道传入的参数实际类型究竟是Person，还是Student，还是Person的其他子类，因此，也无法确定调用的是不是Person类定义的run()方法。</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ <strong>多态的特性就是，运行期才能动态决定调用的子类方法。对某个类型调用某个方法，执行的实际方法可能是某个子类的覆写方法。这种不确定性的方法调用，究竟有什么作用？</strong></p>
<p>​ 示例如下：</p>
<ul>
<li>假设我们定义一种收入，需要给它报税，那么先定义一个<code>Income</code>类</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Income</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">double</span> income;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">getTax</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> income * <span class="number">0.1</span>; <span class="comment">// 税率10%</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于工资收入，可以减去一个基数，那么我们可以从<code>Income</code>派生出<code>Salary</code>，并覆写<code>getTax()</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Salary</span> <span class="keyword">extends</span> <span class="title">Income</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">getTax</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (income &lt;= <span class="number">5000</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> (income - <span class="number">5000</span>) * <span class="number">0.2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果你享受国务院特殊津贴，那么按照规定，可以全部免税：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StateCouncilSpecialAllowance</span> <span class="keyword">extends</span> <span class="title">Income</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">getTax</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在，我们要编写一个报税的财务软件，对于一个人的所有收入进行报税，可以这么写：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 给一个有普通收入、工资收入和享受国务院特殊津贴的小伙伴算税:</span></span><br><span class="line">        Income[] incomes = <span class="keyword">new</span> Income[] &#123;</span><br><span class="line">            <span class="keyword">new</span> Income(<span class="number">3000</span>),</span><br><span class="line">            <span class="keyword">new</span> Salary(<span class="number">7500</span>),</span><br><span class="line">            <span class="keyword">new</span> StateCouncilSpecialAllowance(<span class="number">15000</span>)</span><br><span class="line">        &#125;;</span><br><span class="line">        System.out.println(totalTax(incomes));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">totalTax</span><span class="params">(Income... incomes)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> total = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (Income income: incomes) &#123;</span><br><span class="line">            total = total + income.getTax();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> total;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>观察<code>totalTax()</code>方法：利用多态，<code>totalTax()</code>方法只需要和<code>Income</code>打交道，它完全不需要知道<code>Salary</code>和<code>StateCouncilSpecialAllowance</code>的存在，就可以正确计算出总的税。如果我们要新增一种稿费收入，只需要从<code>Income</code>派生，然后正确覆写<code>getTax()</code>方法就可以。把新的类型传入<code>totalTax()</code>，不需要修改任何代码。</strong></p>
<h4 id="覆写object类的方法">3、覆写Object类的方法：</h4>
<p>​ 因为所有的<code>class</code>最终都继承自<code>Object</code>，而<code>Object</code>定义了几个重要的方法：</p>
<ul>
<li><code>toString()</code>：把instance输出为<code>String</code>；</li>
<li><code>equals()</code>：判断两个instance是否逻辑相等；</li>
<li><code>hashCode()</code>：计算一个instance的哈希值。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 显示更有意义的字符串:</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Person:name=&quot;</span> + name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 比较是否相等:</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 当且仅当o为Person类型:</span></span><br><span class="line">        <span class="keyword">if</span> (o <span class="keyword">instanceof</span> Person) &#123;</span><br><span class="line">            Person p = (Person) o;</span><br><span class="line">            <span class="comment">// 并且name字段相同时，返回true:</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.name.equals(p.name);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算hash:</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.name.hashCode();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="super调用">4、Super调用</h4>
<p>在子类的覆写方法中，如果要调用父类的被覆写的方法，可以通过<code>super</code>来调用。例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Student extends Person &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">hello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 调用父类的hello()方法:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">super</span>.hello() + <span class="string">&quot;!&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="final-字段">5、Final 字段</h4>
<ul>
<li>如果一个父类不允许子类对它的某个方法进行覆写，可以把该方法标记为<code>final</code>。用<code>final</code>修饰的方法不能被<code>Override</code>：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> String name;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> String <span class="title">hello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Hello, &quot;</span> + name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>对于一个类的实例字段，同样可以用<code>final</code>修饰。用<code>final</code>修饰的字段在初始化后不能被修改。例如:</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> String name = <span class="string">&quot;Unamed&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="六抽象类">六、抽象类</h3>
<ul>
<li>如果父类的方法本身不需要实现任何功能，仅仅是为了定义方法签名，目的是让子类去覆写它，那么，可以把父类的方法声明为抽象方法：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//编译错误</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//编译成功</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>通过<code>abstract</code>定义的方法是抽象方法，它只有定义，没有实现。抽象方法定义了子类必须实现的接口规范；</p></li>
<li><p>定义了抽象方法的class必须被定义为抽象类，从抽象类继承的子类必须实现抽象方法；</p></li>
<li><p>如果不实现抽象方法，则该子类仍是一个抽象类；</p></li>
</ul>
<h3 id="七面向抽象编程">七、面向抽象编程：</h3>
<p>我们定义了抽象类<code>Person</code>，以及具体的<code>Student</code>、<code>Teacher</code>子类的时候，我们可以通过抽象类<code>Person</code>类型去引用具体的子类的实例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Person s = <span class="keyword">new</span> Student();</span><br><span class="line">Person t = <span class="keyword">new</span> Teacher();</span><br></pre></td></tr></table></figure>
<p>这种引用抽象类的好处在于，我们对其进行方法调用，并不关心<code>Person</code>类型变量的具体子类型：</p>
<p>这种尽量引用高层类型，避免引用实际子类型的方式，称之为面向抽象编程。</p>
<p>面向抽象编程的本质就是：</p>
<ul>
<li>上层代码只定义规范（例如：<code>abstract class Person</code>）；</li>
<li>不需要子类就可以实现业务逻辑（正常编译）；</li>
<li>具体的业务逻辑由不同的子类实现，调用者并不关心。</li>
</ul>
<h3 id="八接口interface">八、接口Interface</h3>
<h4 id="基础知识">1、基础知识</h4>
<p>如果一个抽象类没有字段，所有方法全部都是抽象方法：就可以把该抽象类改写为接口：<code>interface</code>。</p>
<p>在Java中，使用<code>interface</code>可以声明一个接口：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function">String <span class="title">getName</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所谓<code>interface</code>，就是比抽象类还要抽象的纯抽象接口，因为它连字段都不能有。因为接口定义的所有方法默认都是<code>public abstract</code>的，所以这两个修饰符不需要写出来（写不写效果都一样)</p>
<ul>
<li><p>当一个具体的<code>class</code>去实现一个<code>interface</code>时，需要使用<code>implements</code>关键字。</p></li>
<li><p>```java class Student implements Person { private String name;</p>
<pre><code>public Student(String name) &#123;
    this.name = name;
&#125;

@Override
public void run() &#123;
    System.out.println(this.name + &quot; run&quot;);
&#125;

@Override
public String getName() &#123;
    return this.name;
&#125;</code></pre>
<p>} <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 一个类可以实现多个`interface`</span><br><span class="line"></span><br><span class="line">- ```</span><br><span class="line">  class Student implements Person, Hello &#123; // 实现了两个interface</span><br><span class="line">      ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h4 id="接口继承使用extends">2、接口继承：使用extends</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">hello</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Person</span> <span class="keyword">extends</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function">String <span class="title">getName</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="default方法">3、default方法：</h3>
<p>在接口中，可以定义<code>default</code>方法。</p>
<p><strong>实现类可以不必覆写<code>default</code>方法。</strong></p>
<p><code>default</code>方法的目的是，当我们需要给接口新增一个方法时，将会涉及到修改全部子类。</p>
<p>但如果新增的是<code>default</code>方法，那么子类就不必全部修改，只需要在需要覆写的地方去覆写新增方法。</p>
<h3 id="九静态字段和静态方法">九、静态字段和静态方法：</h3>
<h4 id="静态字段">1、静态字段：</h4>
<p>​ 所有实例共享一个静态字段。推荐用类名来访问静态字段。可以把静态字段理解为描述<code>class</code>本身的字段（非实例字段）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Person.number = <span class="number">99</span>;</span><br><span class="line">System.out.println(Person.number);</span><br></pre></td></tr></table></figure>
<h4 id="静态方法">2、静态方法：</h4>
<ul>
<li><p>调用实例方法必须通过一个实例变量，而调用静态方法则不需要实例变量，通过类名就可以调用</p></li>
<li><p>静态方法内部，无法访问<code>this</code>变量，也无法访问实例字段，它只能访问静态字段</p></li>
</ul>
<h4 id="接口的静态字段">3、接口的静态字段：</h4>
<ul>
<li>因为<code>interface</code>是一个纯抽象类，所以它不能定义实例字段。但是，<code>interface</code>是可以有静态字段的，并且静态字段必须为<code>final</code>类型：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MALE = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> FEMALE = <span class="number">2</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 实际上，因为<code>interface</code>的字段只能是<code>public static final</code>类型，所以我们可以把这些修饰符都去掉，上述代码可以简写为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 编译器会自动加上public statc final:</span></span><br><span class="line">    <span class="keyword">int</span> MALE = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> FEMALE = <span class="number">2</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="十包">十、包：</h3>
<h4 id="简介">1、简介：</h4>
<p>如果小军写了一个<code>Arrays</code>类，恰好JDK也自带了一个<code>Arrays</code>类，如何解决类名冲突？</p>
<p>在Java中，我们使用<code>package</code>来解决名字冲突。</p>
<p><strong>Java定义了一种名字空间，称之为包：<code>package</code>。一个类总是属于某个包，类名（比如<code>Person</code>）只是一个简写，真正的完整类名是<code>包名.类名</code>。</strong></p>
<p>例如：</p>
<ul>
<li><p>小明的<code>Person</code>类存放在包<code>ming</code>下面，因此，完整类名是<code>ming.Person</code>；</p></li>
<li><p>小红的<code>Person</code>类存放在包<code>hong</code>下面，因此，完整类名是<code>hong.Person</code>；</p></li>
<li><p>小军的<code>Arrays</code>类存放在包<code>mr.jun</code>下面，因此，完整类名是<code>mr.jun.Arrays</code>；</p></li>
</ul>
<h4 id="如何申明包">2、如何申明包：</h4>
<p>在定义<code>class</code>的时候，我们需要在第一行声明这个<code>class</code>属于哪个包。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> ming; <span class="comment">// 申明包名ming</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意：包没有父子关系。java.util和java.util.zip是不同的包，两者没有任何继承关系。</p>
<h4 id="组织java文件">3、组织java文件：</h4>
<p>我们还需要按照包结构把上面的Java文件组织起来。假设以<code>package_sample</code>作为根目录，<code>src</code>作为源码目录，那么所有文件结构就是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package_sample</span><br><span class="line">└─ src</span><br><span class="line">    ├─ hong</span><br><span class="line">    │  └─ Person.java</span><br><span class="line">    │  ming</span><br><span class="line">    │  └─ Person.java</span><br><span class="line">    └─ mr</span><br><span class="line">       └─ jun</span><br><span class="line">          └─ Arrays.java</span><br></pre></td></tr></table></figure>
<h4 id="包作用域">4、包作用域：</h4>
<p>​ 位于同一个包的类，可以访问包作用域的字段和方法。不用<code>public</code>、<code>protected</code>、<code>private</code>修饰的字段和方法就是包作用域。例如，<code>Person</code>类定义在<code>hello</code>包下面：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> hello;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 包作用域:</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">hello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Hello!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>Main</code>类也定义在<code>hello</code>包下面：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> hello;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Person p = <span class="keyword">new</span> Person();</span><br><span class="line">        p.hello(); <span class="comment">// 可以调用，因为Main和Person在同一个包</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="包的引入">5、包的引入：</h4>
<h4 id="在一个class中我们总会引用其他的class由3种方法">1、在一个<code>class</code>中，我们总会引用其他的<code>class</code>，由3种方法：</h4>
<ul>
<li>写完整类名：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Person.java</span></span><br><span class="line"><span class="keyword">package</span> ming;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        mr.jun.Arrays arrays = <span class="keyword">new</span> mr.jun.Arrays();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>用<code>import</code>语句，导入小军的<code>Arrays</code>，然后写简单类名：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Person.java</span></span><br><span class="line"><span class="keyword">package</span> ming;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 导入完整类名:</span></span><br><span class="line"><span class="keyword">import</span> mr.jun.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Arrays arrays = <span class="keyword">new</span> Arrays();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>在写<code>import</code>的时候，可以使用<code>*</code>，表示把这个包下面的所有<code>class</code>都导入进来（但不包括子包的<code>class</code>）：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Person.java</span></span><br><span class="line"><span class="keyword">package</span> ming;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 导入mr.jun包的所有class:</span></span><br><span class="line"><span class="keyword">import</span> mr.jun.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Arrays arrays = <span class="keyword">new</span> Arrays();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="java编译器如何查找">2、Java编译器如何查找？</h4>
<p>Java编译器最终编译出的<code>.class</code>文件只使用完整类名，因此，在代码中，当编译器遇到一个<code>class</code>名称时：</p>
<ul>
<li>如果是完整类名，就直接根据完整类名查找这个<code>class</code>；</li>
<li>如果是简单类名，按下面的顺序依次查找：
<ul>
<li>查找当前<code>package</code>是否存在这个<code>class</code>；</li>
<li>查找<code>import</code>的包是否包含这个<code>class</code>；</li>
<li>查找<code>java.lang</code>包是否包含这个<code>class</code>。</li>
</ul></li>
</ul>
<h4 id="最佳实践">3、最佳实践：</h4>
<p>为了避免名字冲突，我们需要确定唯一的包名。推荐的做法是使用倒置的作用域名来确保唯一性。例如：</p>
<ul>
<li>org.apache</li>
<li>org.apache.commons.log</li>
<li>com.liaoxuefeng.sample</li>
</ul>
<p>子包就可以根据功能自行命名。</p>
<h3 id="十一作用域">十一、作用域：</h3>
<h4 id="public">1、public</h4>
<ul>
<li><p>定义为<code>public</code>的<code>class</code>、<code>interface</code>可以被其他任何类访问</p></li>
<li><p>定义为<code>public</code>的<code>field</code>、<code>method</code>可以被其他类访问，前提是首先有访问<code>class</code>的权限：</p></li>
</ul>
<h4 id="private">2、private</h4>
<ul>
<li><p>定义为<code>private</code>的<code>field</code>、<code>method</code>无法被其他类访问</p></li>
<li><p><code>private</code>访问权限被限定在<code>class</code>的内部，而且与方法声明顺序<em>无关</em>。推荐把<code>private</code>方法放到后面，因为<code>public</code>方法定义了类对外提供的功能，阅读代码的时候，应该先关注<code>public</code>方法</p></li>
<li><p>Java支持嵌套类，如果一个类内部还定义了嵌套类，那么，嵌套类拥有访问<code>private</code>的权限：</p></li>
<li><p><strong>嵌套类</strong>：</p></li>
<li><p>定义在一个<code>class</code>内部的<code>class</code>称为嵌套类（<code>nested class</code>），Java支持好几种嵌套类。</p></li>
<li><p>``` public class Main { public static void main(String[] args) { Inner i = new Inner(); i.hi(); }</p>
<pre><code>// private方法:
private static void hello() &#123;
    System.out.println(&quot;private hello!&quot;);
&#125;

// 静态内部类:
static class Inner &#123;
    public void hi() &#123;
        Main.hello();
    &#125;
&#125;</code></pre>
<p>}</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">#### 3、protected</span><br><span class="line"></span><br><span class="line">- `protected`作用于继承关系。定义为`protected`的字段和方法可以被子类访问，以及子类的子类</span><br><span class="line"></span><br><span class="line">#### 4、package：</span><br><span class="line"></span><br><span class="line">- 包作用域是指一个类允许访问同一个`package`的没有`public`、`private`修饰的`class`，以及没有`public`、`protected`、`private`修饰的字段和方法。</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">package abc;</span><br><span class="line">// package权限的类:</span><br><span class="line">class Hello &#123;</span><br><span class="line">    // package权限的方法:</span><br><span class="line">    void hi() &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>只要在同一个包，就可以访问<code>package</code>权限的<code>class</code>、<code>field</code>和<code>method</code>：</p></li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> abc;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 可以访问package权限的类:</span></span><br><span class="line">        Hello h = <span class="keyword">new</span> Hello();</span><br><span class="line">        <span class="comment">// 可以调用package权限的方法:</span></span><br><span class="line">        h.hi();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="final">5、final</h4>
<p><code>final</code>与访问权限不冲突，它有很多作用：</p>
<ul>
<li>用<code>final</code>修饰<code>class</code>可以阻止被继承：</li>
<li>用<code>final</code>修饰<code>method</code>可以阻止被子类覆写</li>
<li>用<code>final</code>修饰<code>field</code>可以阻止被重新赋值</li>
<li>用<code>final</code>修饰局部变量可以阻止被重新赋值</li>
</ul>
<h4 id="最佳实践-1">6、最佳实践：</h4>
<ul>
<li><p>如果不确定是否需要<code>public</code>，就不声明为<code>public</code>，即尽可能少地暴露对外的字段和方法。</p></li>
<li><p>把方法定义为<code>package</code>权限有助于测试，因为测试类和被测试类只要位于同一个<code>package</code>，测试代码就可以访问被测试类的<code>package</code>权限方法。</p></li>
<li><p><strong>一个<code>.java</code>文件只能包含一个<code>public</code>类，但可以包含多个非<code>public</code>类。如果有<code>public</code>类，文件名必须和<code>public</code>类的名字相同。</strong></p></li>
</ul>
<h3 id="十二内部类">十二、内部类：</h3>
<p>Java的内部类分为好几种，通常情况用得不多，但也需要了解它们是如何使用的</p>
<h4 id="inner-class">1、Inner Class</h4>
<p>如果一个类定义在另一个类的内部，这个类就是Inner Class：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Outer</span> </span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Inner</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 定义了一个Inner Class</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Inner Class的实例不能单独存在，必须依附于一个Outer Class的实例</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Outer outer = <span class="keyword">new</span> Outer(<span class="string">&quot;Nested&quot;</span>); <span class="comment">// 实例化一个Outer</span></span><br><span class="line">        Outer.Inner inner = outer.<span class="function">new <span class="title">Inner</span><span class="params">()</span></span>; <span class="comment">// 实例化一个Inner</span></span><br><span class="line">        inner.hello();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Outer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    Outer(String name) &#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Inner</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">hello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Hello, &quot;</span> + Outer.<span class="keyword">this</span>.name);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Outer.Inner inner = outer.new Inner();</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Inner Class和普通Class相比，除了能引用Outer实例外，还有一个额外的“特权”，<strong>就是可以修改Outer Class的<code>private</code>字段</strong>，因为Inner Class的作用域在Outer Class内部，所以能访问Outer Class的<code>private</code>字段和方法。</p></li>
<li><p>观察Java编译器编译后的<code>.class</code>文件可以发现，<code>Outer</code>类被编译为<code>Outer.class</code>，而<code>Inner</code>类被编译为<code>Outer$Inner.class</code>。</p></li>
</ul>
<h4 id="anonymous-class">2、Anonymous Class</h4>
<p>​ 有一种定义Inner Class的方法，它不需要在Outer Class中明确地定义这个Class，而是在方法内部，通过匿名类（Anonymous Class）来定义。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Outer outer = <span class="keyword">new</span> Outer(<span class="string">&quot;Nested&quot;</span>);</span><br><span class="line">        outer.asyncHello();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Outer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    Outer(String name) &#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">asyncHello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Runnable r = <span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Hello, &quot;</span> + Outer.<span class="keyword">this</span>.name);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="keyword">new</span> Thread(r).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 观察<code>asyncHello()</code>方法，我们在方法内部实例化了一个<code>Runnable</code>。<code>Runnable</code>本身是接口，接口是不能实例化的，所以这里实际上是定义了一个实现了<code>Runnable</code>接口的匿名类，并且通过<code>new</code>实例化该匿名类，然后转型为<code>Runnable</code>。在定义匿名类的时候就必须实例化它，定义匿名类的写法如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Runnable r = <span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">    <span class="comment">// 实现必要的抽象方法...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>匿名类和Inner Class一样，可以访问Outer Class的<code>private</code>字段和方法。之所以我们要定义匿名类，是因为在这里我们通常不关心类名，比直接定义Inner Class可以少写很多代码。</p>
<h4 id="static-nested-class-静态内部类">3、Static Nested Class 静态内部类：</h4>
<p>最后一种内部类和Inner Class类似，但是使用<code>static</code>修饰，称为静态内部类（Static Nested Class）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Outer.StaticNested sn = <span class="keyword">new</span> Outer.StaticNested();</span><br><span class="line">        sn.hello();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Outer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String NAME = <span class="string">&quot;OUTER&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    Outer(String name) &#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">StaticNested</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">hello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Hello, &quot;</span> + Outer.NAME);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ 用<code>static</code>修饰的内部类和Inner Class有很大的不同，它不再依附于<code>Outer</code>的实例，而是一个完全独立的类，因此无法引用<code>Outer.this</code>，但它可以访问<code>Outer</code>的<code>private</code>静态字段和静态方法。如果把<code>StaticNested</code>移到<code>Outer</code>之外，就失去了访问<code>private</code>的权限。</p>
<h4 id="总结">4、总结：</h4>
<p>Java的内部类可分为Inner Class、Anonymous Class和Static Nested Class三种：</p>
<ul>
<li>Inner Class和Anonymous Class本质上是相同的，都必须依附于Outer Class的实例，即隐含地持有<code>Outer.this</code>实例，并拥有Outer Class的<code>private</code>访问权限；</li>
<li>Static Nested Class是独立类，但拥有Outer Class的<code>private</code>访问权限。</li>
</ul>
<h3 id="十三classpath-和-jar">十三、classpath 和 jar</h3>
<h4 id="classpath是什么">1、classpath是什么？</h4>
<p>​ <code>classpath</code>是JVM用到的一个环境变量，它用来指示JVM如何搜索<code>class</code></p>
<p>​ 因为Java是编译型语言，源码文件是<code>.java</code>，而编译后的<code>.class</code>文件才是真正可以被JVM执行的字节码。因此，JVM需要知道，如果要加载一个<code>abc.xyz.Hello</code>的类，应该去哪搜索对应的<code>Hello.class</code>文件。</p>
<p>​ 所以，<code>classpath</code>就是一组目录的集合，它设置的搜索路径与操作系统相关。例如，在Windows系统上，用<code>;</code>分隔，带空格的目录用<code>""</code>括起来，可能长这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\work\project1\bin;C:\shared;&quot;D:\My Documents\project1\bin&quot;</span><br></pre></td></tr></table></figure>
<p>​ 在Linux系统上，用<code>:</code>分隔，可能长这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/shared:/usr/local/bin:/home/liaoxuefeng/bin</span><br></pre></td></tr></table></figure>
<p>​ 现在我们假设<code>classpath</code>是<code>.;C:\work\project1\bin;C:\shared</code>，当JVM在加载<code>abc.xyz.Hello</code>这个类时，会依次查找：</p>
<ul>
<li><当前目录>.class</li>
<li>C:.class</li>
<li>C:.class</li>
</ul>
<p>​ 注意到<code>.</code>代表当前目录。如果JVM在某个路径下找到了对应的<code>class</code>文件，就不再往后继续搜索。如果所有路径下都没有找到，就报错。</p>
<h4 id="如何设定classpath">2、如何设定classpath?</h4>
<p>​ 在启动JVM时设置<code>classpath</code>变量,实际上就是给<code>java</code>命令传入<code>-classpath</code>或<code>-cp</code>参数：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">java -classpath .;C:\work\project1\bin;C:\shared abc.xyz.Hello</span><br></pre></td></tr></table></figure>
<p>​ 没有设置系统环境变量，也没有传入<code>-cp</code>参数，那么JVM默认的<code>classpath</code>为<code>.</code>，即当前目录：</p>
<p>​ 在IDE中运行Java程序，IDE自动传入的<code>-cp</code>参数是当前工程的<code>bin</code>目录和引入的jar包</p>
<p><strong>注意：不要把任何Java核心库添加到classpath中！JVM根本不依赖classpath加载核心库！</strong></p>
<h4 id="jar包是什么">3、jar包是什么？：</h4>
<p>​ 如果有很多<code>.class</code>文件，散落在各层目录中，肯定不便于管理。如果能把目录打一个包，变成一个文件，就方便多了。</p>
<p>​ jar包就是用来干这个事的，它可以把<code>package</code>组织的目录层级，以及各个目录下的所有文件（包括<code>.class</code>文件和其他文件）都打成一个jar文件，这样一来，无论是备份，还是发给客户，就简单多了。</p>
<p>​ jar包实际上就是一个zip格式的压缩文件，而jar包相当于目录。如果我们要执行一个jar包的<code>class</code>，就可以把jar包放到<code>classpath</code>中：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ava -cp ./hello.jar abc.xyz.Hello</span><br></pre></td></tr></table></figure>
<p>​ 这样JVM会自动在<code>hello.jar</code>文件里去搜索某个类。</p>
<h4 id="如何创建jar包">4、如何创建jar包？</h4>
<p>​ 因为jar包就是zip包，所以，直接在资源管理器中，找到正确的目录，点击右键，在弹出的快捷菜单中选择“发送到”，“压缩(zipped)文件夹”，就制作了一个zip文件。然后，把后缀从<code>.zip</code>改为<code>.jar</code>，一个jar包就创建成功。</p>
<p>假设编译输出的目录结构是这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package_sample</span><br><span class="line">└─ bin</span><br><span class="line">   ├─ hong</span><br><span class="line">   │  └─ Person.class</span><br><span class="line">   │  ming</span><br><span class="line">   │  └─ Person.class</span><br><span class="line">   └─ mr</span><br><span class="line">      └─ jun</span><br><span class="line">         └─ Arrays.class</span><br></pre></td></tr></table></figure>
<p>这里需要特别注意的是，jar包里的第一层目录，不能是<code>bin</code>，而应该是<code>hong</code>、<code>ming</code>、<code>mr</code>。应当如下所示：</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/pic.png" alt="pic" /><figcaption aria-hidden="true">pic</figcaption>
</figure>
<h4 id="特殊文件-manifest.mf">4、特殊文件 MANIFEST.MF：</h4>
<p>​ jar包还可以包含一个特殊的<code>/META-INF/MANIFEST.MF</code>文件，<code>MANIFEST.MF</code>是纯文本，可以指定<code>Main-Class</code>和其它信息。JVM会自动读取这个<code>MANIFEST.MF</code>文件，如果存在<code>Main-Class</code>，我们就不必在命令行指定启动的类名，而是用更方便的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">java -jar hello.jar</span><br></pre></td></tr></table></figure>
<p>​ jar包还可以包含其它jar包，这个时候，就需要在<code>MANIFEST.MF</code>文件里配置<code>classpath</code>了。</p>
<p>​ 在大型项目中，不可能手动编写<code>MANIFEST.MF</code>文件，再手动创建zip包。Java社区提供了大量的开源构建工具，例如<a href="https://www.liaoxuefeng.com/wiki/1252599548343744/1255945359327200">Maven</a>，可以非常方便地创建jar包。</p>
<h3 id="十四模块">十四、模块：</h3>
<h4 id="java9之前的程序打包运行">1、java9之前的程序打包运行：</h4>
<p>从Java 9开始，JDK又引入了模块（Module）</p>
<p><code>.class</code>文件是JVM看到的最小可执行文件，而一个大型程序需要编写很多Class，并生成一堆<code>.class</code>文件，很不便于管理，所以，<code>jar</code>文件就是<code>class</code>文件的容器。</p>
<p>在Java 9之前，一个大型Java程序会生成自己的jar文件，同时引用依赖的第三方jar文件，而JVM自带的Java标准库，实际上也是以jar文件形式存放的，这个文件叫<code>rt.jar</code>，一共有60多M。</p>
<p>如果是自己开发的程序，除了一个自己的<code>app.jar</code>以外，还需要一堆第三方的jar包，运行一个Java程序，一般来说，命令行写这样：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java -cp app.jar:a.jar:b.jar:c.jar com.liaoxuefeng.sample.Main</span><br></pre></td></tr></table></figure>
<p>如果漏写了某个运行时需要用到的jar，那么在运行期极有可能抛出<code>ClassNotFoundException</code>。</p>
<p>所以，jar只是用于存放class的容器，<strong>它并不关心class之间的依赖</strong>。</p>
<h4 id="java9之后引入的模块">2、java9之后引入的模块：</h4>
<p>​ 主要为了解决依赖的问题。</p>
<p>​ 如果<code>a.jar</code>必须依赖另一个<code>b.jar</code>才能运行，那我们应该给<code>a.jar</code>加点说明啥的，让程序在编译和运行的时候能自动定位到<code>b.jar</code>，这种自带“依赖关系”的class容器就是模块。</p>
<p>为了表明Java模块化的决心，从Java 9开始，原有的Java标准库已经由一个单一巨大的<code>rt.jar</code>分拆成了几十个模块，这些模块以<code>.jmod</code>扩展名标识，可以在<code>$JAVA_HOME/jmods</code>目录下找到它们：</p>
<ul>
<li>java.base.jmod</li>
<li>java.compiler.jmod</li>
<li>java.datatransfer.jmod</li>
<li>java.desktop.jmod</li>
<li>...</li>
</ul>
<p>​ 这些<code>.jmod</code>文件每一个都是一个模块，模块名就是文件名。例如：模块<code>java.base</code>对应的文件就是<code>java.base.jmod</code>。模块之间的依赖关系已经被写入到模块内的<code>module-info.class</code>文件了。所有的模块都直接或间接地依赖<code>java.base</code>模块，只有<code>java.base</code>模块不依赖任何模块，它可以被看作是“根模块”，好比所有的类都是从<code>Object</code>直接或间接继承而来。</p>
<p>​ 把一堆class封装为jar仅仅是一个打包的过程，而把一堆class封装为模块则不但需要打包，还需要写入依赖关系，并且还可以包含二进制代码（通常是JNI扩展）。此外，模块支持多版本，即在同一个模块中可以为不同的JVM提供不同的版本。</p>
<h4 id="如何编写模块">3、如何编写模块：</h4>
<p>​ 首先，创建模块和原有的创建Java项目是完全一样的，以<code>oop-module</code>工程为例，它的目录结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">oop-module</span><br><span class="line">├── bin</span><br><span class="line">├── build.sh</span><br><span class="line">└── src</span><br><span class="line">    ├── com</span><br><span class="line">    │   └── itranswarp</span><br><span class="line">    │       └── sample</span><br><span class="line">    │           ├── Greeting.java</span><br><span class="line">    │           └── Main.java</span><br><span class="line">    └── module-info.java</span><br></pre></td></tr></table></figure>
<p>其中，<code>bin</code>目录存放编译后的class文件，<code>src</code>目录存放源码，按包名的目录结构存放，仅仅在<code>src</code>目录下多了一个<code>module-info.java</code>这个文件，这就是模块的描述文件。在这个模块中，它长这样：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">module</span> hello.world &#123;</span><br><span class="line">	<span class="keyword">requires</span> java.base; <span class="comment">// 可不写，任何模块都会自动引入java.base</span></span><br><span class="line">	<span class="keyword">requires</span> java.xml;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中，<code>module</code>是关键字，后面的<code>hello.world</code>是模块的名称，它的命名规范与包一致。花括号的<code>requires xxx;</code>表示这个模块需要引用的其他模块名。除了<code>java.base</code>可以被自动引入外，这里我们引入了一个<code>java.xml</code>的模块。</p>
<p>当我们使用模块声明了依赖关系后，才能使用引入的模块。例如，<code>Main.java</code>代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.itranswarp.sample;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 必须引入java.xml模块后才能使用其中的类:</span></span><br><span class="line"><span class="keyword">import</span> javax.xml.XMLConstants;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		Greeting g = <span class="keyword">new</span> Greeting();</span><br><span class="line">		System.out.println(g.hello(XMLConstants.XML_NS_PREFIX));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果把<code>requires java.xml;</code>从<code>module-info.java</code>中去掉，编译将报错。可见，模块的重要作用就是声明依赖关系。</p>
<h4 id="命令行编译并创建模块">4、 命令行编译并创建模块：</h4>
<p>首先，我们把工作目录切换到<code>oop-module</code>，在当前目录下编译所有的<code>.java</code>文件，并存放到<code>bin</code>目录下，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ javac -d bin src/module-info.java src/com/itranswarp/sample/*.java</span><br></pre></td></tr></table></figure>
<p>如果编译成功，现在项目结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">oop-module</span><br><span class="line">├── bin</span><br><span class="line">│   ├── com</span><br><span class="line">│   │   └── itranswarp</span><br><span class="line">│   │       └── sample</span><br><span class="line">│   │           ├── Greeting.class</span><br><span class="line">│   │           └── Main.class</span><br><span class="line">│   └── module-info.class</span><br><span class="line">└── src</span><br><span class="line">    ├── com</span><br><span class="line">    │   └── itranswarp</span><br><span class="line">    │       └── sample</span><br><span class="line">    │           ├── Greeting.java</span><br><span class="line">    │           └── Main.java</span><br><span class="line">    └── module-info.java</span><br></pre></td></tr></table></figure>
<p>注意到<code>src</code>目录下的<code>module-info.java</code>被编译到<code>bin</code>目录下的<code>module-info.class</code>。</p>
<p>下一步，我们需要把bin目录下的所有class文件先打包成jar，在打包的时候，注意传入<code>--main-class</code>参数，让这个jar包能自己定位<code>main</code>方法所在的类：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ jar --create --file hello.jar --main-class com.itranswarp.sample.Main -C bin .</span><br></pre></td></tr></table></figure>
<p>现在我们就在当前目录下得到了<code>hello.jar</code>这个jar包，它和普通jar包并无区别，可以直接使用命令<code>java -jar hello.jar</code>来运行它。但是我们的目标是创建模块，所以，继续使用JDK自带的<code>jmod</code>命令把一个jar包转换成模块：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ jmod create --class-path hello.jar hello.jmod</span><br></pre></td></tr></table></figure>
<p>于是，在当前目录下我们又得到了<code>hello.jmod</code>这个模块文件，这就是最后打包出来的传说中的模块！</p>
<h4 id="运行模块">5、运行模块：</h4>
<p>要运行一个jar，我们使用<code>java -jar xxx.jar</code>命令。要运行一个模块，我们只需要指定模块名。试试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ java --module-path hello.jmod --module hello.world</span><br></pre></td></tr></table></figure>
<p>结果是一个错误：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error occurred during initialization of boot layer</span><br><span class="line">java.lang.module.FindException: JMOD format not supported at execution time: hello.jmod</span><br></pre></td></tr></table></figure>
<p>原因是<code>.jmod</code>不能被放入<code>--module-path</code>中。换成<code>.jar</code>就没问题了：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ java --module-path hello.jar --module hello.world</span><br><span class="line">Hello, xml!</span><br></pre></td></tr></table></figure>
<h4 id="打包jre">6、打包JRE</h4>
<p>我们可以利用创建的<code>hello.jmod</code>来打包JRE。</p>
<p>​ 为了支持模块化，Java 9首先带头把自己的一个巨大无比的<code>rt.jar</code>拆成了几十个<code>.jmod</code>模块，原因就是，运行Java程序的时候，实际上我们用到的JDK模块，并没有那么多。不需要的模块，完全可以删除。</p>
<p>​ 过去发布一个Java应用程序，要运行它，必须下载一个完整的JRE，再运行jar包。而完整的JRE块头很大，有100多M。怎么给JRE瘦身呢？</p>
<p>​ 现在，JRE自身的标准库已经分拆成了模块，只需要带上程序用到的模块，其他的模块就可以被裁剪掉。怎么裁剪JRE呢？并不是说把系统安装的JRE给删掉部分模块，而是“复制”一份JRE，但只带上用到的模块。为此，JDK提供了<code>jlink</code>命令来干这件事。命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ jlink --module-path hello.jmod --add-modules java.base,java.xml,hello.world --output jre/</span><br></pre></td></tr></table></figure>
<p>我们在<code>--module-path</code>参数指定了我们自己的模块<code>hello.jmod</code>，然后，在<code>--add-modules</code>参数中指定了我们用到的3个模块<code>java.base</code>、<code>java.xml</code>和<code>hello.world</code>，用<code>,</code>分隔。最后，在<code>--output</code>参数指定输出目录。</p>
<p>现在，在当前目录下，我们可以找到<code>jre</code>目录，这是一个完整的并且带有我们自己<code>hello.jmod</code>模块的JRE。试试直接运行这个JRE：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ jre/bin/java --module hello.world</span><br><span class="line">Hello, xml!</span><br></pre></td></tr></table></figure>
<p>要分发我们自己的Java应用程序，只需要把这个<code>jre</code>目录打个包给对方发过去，对方直接运行上述命令即可，既不用下载安装JDK，也不用知道如何配置我们自己的模块，极大地方便了分发和部署。</p>
<h4 id="访问权限">7、访问权限：</h4>
<p>Java的class访问权限分为public、protected、private和默认的包访问权限。</p>
<p>引入模块后，这些访问权限的规则就要稍微做些调整。</p>
<p>确切地说，<strong>class的这些访问权限只在一个模块内有效</strong>，模块和模块之间，例如，a模块要访问b模块的某个class，必要条件是b模块明确地导出了可以访问的包。</p>
<p>举个例子：我们编写的模块<code>hello.world</code>用到了模块<code>java.xml</code>的一个类<code>javax.xml.XMLConstants</code>，我们之所以能直接使用这个类，是因为模块<code>java.xml</code>的<code>module-info.java</code>中声明了若干导出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">module</span> java.xml &#123;</span><br><span class="line">    <span class="keyword">exports</span> java.xml;</span><br><span class="line">    <span class="keyword">exports</span> javax.xml.catalog;</span><br><span class="line">    <span class="keyword">exports</span> javax.xml.datatype;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>只有它声明的导出的包，外部代码才被允许访问。换句话说，如果外部代码想要访问我们的<code>hello.world</code>模块中的<code>com.itranswarp.sample.Greeting</code>类，我们必须将其导出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">module</span> hello.world &#123;</span><br><span class="line">    <span class="keyword">exports</span> com.itranswarp.sample;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">requires</span> java.base;</span><br><span class="line">	<span class="keyword">requires</span> java.xml;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因此，模块进一步隔离了代码的访问权限。</p>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>oop</tag>
      </tags>
  </entry>
  <entry>
    <title>java系列笔记1——java基础</title>
    <url>/2022/01/28/Java%E7%AC%94%E8%AE%B0/java%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B01%E2%80%94java%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<p>参考教程网址：https://www.liaoxuefeng.com/wiki/1252599548343744/1260467032946976</p>
<h3 id="一第一个java程序">一、第一个Java程序</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Hello, world!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​ java规定，某个类定义的<code>public static void main(String[] args)</code>是java程序的固定入口方法，因此，java程序总是从<code>main</code>方法开始执行。</p>
<h3 id="二如何运行java程序">二、如何运行Java程序：</h3>
<p>1、用javac把Hello.java编译成字节码文件Hello.class，</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> javac Hello.java</span></span><br></pre></td></tr></table></figure>
<p>2、然后用java命令执行这个字节码文件。javac是编译器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> java Hello</span></span><br><span class="line">Hello, world!</span><br></pre></td></tr></table></figure>
<p>给虚拟机传递的参数<code>Hello</code>是我们定义的类名，虚拟机自动查找对应的class文件并执行</p>
<p><strong>注意</strong>：</p>
<p>一个Java源码只能定义一个<code>public</code>类型的class，并且class名称和文件名要完全一致；</p>
<p>使用<code>javac</code>可以将<code>.java</code>源码编译成<code>.class</code>字节码；</p>
<p>使用<code>java</code>可以运行一个已编译的Java程序，参数是类名。</p>
<h3 id="三程序基本结构">三、程序基本结构：</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 可以用来自动创建文档的注释</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 向屏幕输出文本:</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Hello, world!&quot;</span>);</span><br><span class="line">        <span class="comment">/* 多行注释开始</span></span><br><span class="line"><span class="comment">        注释内容</span></span><br><span class="line"><span class="comment">        注释结束 */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="comment">// class定义结束</span></span><br></pre></td></tr></table></figure>
<p><code>public</code>是访问修饰符，表示该<code>class</code>是公开的。不写<code>public</code>，也能正确编译，但是这个类将无法从命令行执行。</p>
<h3 id="四数据类型">四、数据类型</h3>
<p><strong>基本数据类型：</strong></p>
<ul>
<li>整数类型：byte，short，int，long</li>
<li>浮点数类型：float，double</li>
<li>字符类型：char</li>
<li>布尔类型：boolean</li>
</ul>
<p><strong>引用类型</strong>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s = <span class="string">&quot;hello&quot;</span>;</span><br></pre></td></tr></table></figure>
<p>引用类型的变量类似于C语言的指针，它内部存储一个“地址”，指向某个对象在内存的位置</p>
<p><strong>常量</strong>：</p>
<p>如果加上<code>final</code>修饰符，这个变量就变成了常量：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="keyword">double</span> PI = <span class="number">3.14</span>; <span class="comment">// PI是一个常量</span></span><br></pre></td></tr></table></figure>
<p><strong>var关键字:</strong></p>
<p>如果想省略变量类型，可以使用<code>var</code>关键字</p>
<h3 id="五浮点数运算">五、浮点数运算：</h3>
<p>​ 由于浮点数存在运算误差，所以比较两个浮点数是否相等常常会出现错误的结果。正确的比较方法是判断两个浮点数之差的绝对值是否小于一个很小的数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> r = Math.abs(x - y);</span><br><span class="line"><span class="comment">// 再判断绝对值是否足够小:</span></span><br><span class="line"><span class="keyword">if</span> (r &lt; <span class="number">0.00001</span>) &#123;</span><br><span class="line">    <span class="comment">// 可以认为相等</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 不相等</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>整数运算在除数为<code>0</code>时会报错，而浮点数运算在除数为<code>0</code>时，不会报错，但会返回几个特殊值：</p>
<ul>
<li><code>NaN</code>表示Not a Number</li>
<li><code>Infinity</code>表示无穷大</li>
<li><code>-Infinity</code>表示负无穷大</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> d1 = <span class="number">0.0</span> / <span class="number">0</span>; <span class="comment">// NaN</span></span><br><span class="line"><span class="keyword">double</span> d2 = <span class="number">1.0</span> / <span class="number">0</span>; <span class="comment">// Infinity</span></span><br><span class="line"><span class="keyword">double</span> d3 = -<span class="number">1.0</span> / <span class="number">0</span>; <span class="comment">// -Infinity</span></span><br></pre></td></tr></table></figure>
<h3 id="六字符类型">六、字符类型：</h3>
<p>一个<code>char</code>保存一个Unicode字符：</p>
<p>因为Java在内存中总是使用Unicode表示字符，所以，一个英文字符和一个中文字符都用一个<code>char</code>类型表示，它们都占用两个字节。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int n1 = &#x27;A&#x27;; // 字母“A”的Unicodde编码是65</span><br><span class="line">int n2 = &#x27;中&#x27;; // 汉字“中”的Unicode编码是20013</span><br><span class="line">还可以直接用转义字符\u+Unicode编码来表示一个字符：</span><br><span class="line">// 注意是十六进制:</span><br><span class="line">char c3 = &#x27;\u0041&#x27;; // &#x27;A&#x27;，因为十六进制0041 = 十进制65</span><br><span class="line">char c4 = &#x27;\u4e2d&#x27;; // &#x27;中&#x27;，因为十六进制4e2d = 十进制20013</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如果用<code>+</code>连接字符串和其他数据类型，会将其他数据类型先自动转型为字符串，再连接：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> age = <span class="number">25</span>;</span><br><span class="line">        String s = <span class="string">&quot;age is &quot;</span> + age;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从Java 13开始，字符串可以用<code>"""..."""</code>表示多行字符串（Text Blocks）了。</p>
<p>多行字符串前面共同的空格会被去掉，即：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String s = <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                   SELECT * FROM</span></span><br><span class="line"><span class="string">                     users</span></span><br><span class="line"><span class="string">                   WHERE id &gt; 100</span></span><br><span class="line"><span class="string">                   ORDER BY name DESC</span></span><br><span class="line"><span class="string">                   &quot;</span><span class="string">&quot;&quot;</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>不可见特性：</strong></p>
<p>Java的字符串除了是一个引用类型外，还有个重要特点，就是字符串不可变。考察以下代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String s = <span class="string">&quot;hello&quot;</span>;</span><br><span class="line">        System.out.println(s); <span class="comment">// 显示 hello</span></span><br><span class="line">        s = <span class="string">&quot;world&quot;</span>;</span><br><span class="line">        System.out.println(s); <span class="comment">// 显示 world</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>观察执行结果，难道字符串<code>s</code>变了吗？其实变的不是字符串，而是变量<code>s</code>的“指向”。</p>
<p><strong>空值null:</strong></p>
<p>引用类型的变量可以指向一个空值<code>null</code>，它表示不存在，即该变量不指向任何对象。例如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String s1 = <span class="keyword">null</span>; <span class="comment">// s1是null</span></span><br><span class="line">String s2; <span class="comment">// 没有赋初值值，s2也是null</span></span><br></pre></td></tr></table></figure>
<h3 id="七数组">七、数组：</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span>[] ns = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">5</span>];</span><br><span class="line"><span class="keyword">int</span>[] ns = &#123; <span class="number">68</span>, <span class="number">79</span>, <span class="number">91</span>, <span class="number">85</span>, <span class="number">62</span> &#125;;</span><br><span class="line">System.out.println(ns.length); <span class="comment">// 5</span></span><br></pre></td></tr></table></figure>
<h3 id="八流程控制手段">八、流程控制手段</h3>
<h4 id="输出">1、输出</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.out.print(<span class="string">&quot;A,&quot;</span>); <span class="comment">//输出后不换行</span></span><br><span class="line">System.out.println() <span class="comment">//输出后自动换行</span></span><br><span class="line">System.out.printf(<span class="string">&quot;%.2f\n&quot;</span>, d); <span class="comment">// 格式化输出，显示两位小数3.14</span></span><br></pre></td></tr></table></figure>
<h4 id="输入">2、输入：</h4>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner scanner = <span class="keyword">new</span> Scanner(System.in); <span class="comment">// 创建Scanner对象</span></span><br><span class="line">        System.out.print(<span class="string">&quot;Input your name: &quot;</span>); <span class="comment">// 打印提示</span></span><br><span class="line">        String name = scanner.nextLine(); <span class="comment">// 读取一行输入并获取字符串</span></span><br><span class="line">        System.out.print(<span class="string">&quot;Input your age: &quot;</span>); <span class="comment">// 打印提示</span></span><br><span class="line">        <span class="keyword">int</span> age = scanner.nextInt(); <span class="comment">// 读取一行输入并获取整数</span></span><br><span class="line">        System.out.printf(<span class="string">&quot;Hi, %s, you are %d\n&quot;</span>, name, age); <span class="comment">// 格式化输出</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>通过<code>import</code>语句导入<code>java.util.Scanner</code></li>
<li>创建<code>Scanner</code>对象并传入<code>System.in</code></li>
<li>有了<code>Scanner</code>对象后，要读取用户输入的字符串，使用<code>scanner.nextLine()</code>，要读取用户输入的整数，使用<code>scanner.nextInt()</code>。<code>Scanner</code>会自动转换数据类型，因此不必手动转换。</li>
</ul>
<h3 id="九判断引用类型变量内容是否相等">九、判断引用类型变量内容是否相等：</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String s1 = <span class="string">&quot;hello&quot;</span>;</span><br><span class="line">        String s2 = <span class="string">&quot;HELLO&quot;</span>.toLowerCase();</span><br><span class="line">        System.out.println(s1);</span><br><span class="line">        System.out.println(s2);</span><br><span class="line">        <span class="keyword">if</span> (s1 == s2) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;s1 == s2&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;s1 != s2&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 输出结果是 s1 != s2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>要判断引用类型的变量内容是否相等，必须使用<code>equals()</code>方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">s1.equals(s2)</span><br></pre></td></tr></table></figure>
<h3 id="十新switch表达式">十、新Switch表达式：</h3>
<p>​ 从Java 12开始，<code>switch</code>语句升级为更简洁的表达式语法，使用类似模式匹配（Pattern Matching）的方法，保证只有一种路径会被执行，并且不需要<code>break</code>语句</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String fruit = <span class="string">&quot;apple&quot;</span>;</span><br><span class="line"><span class="keyword">switch</span> (fruit) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;apple&quot;</span> -&gt; System.out.println(<span class="string">&quot;Selected apple&quot;</span>);</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;pear&quot;</span> -&gt; System.out.println(<span class="string">&quot;Selected pear&quot;</span>);</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;mango&quot;</span> -&gt; &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Selected mango&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;Good choice!&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">default</span> -&gt; System.out.println(<span class="string">&quot;No fruit selected&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="十一switch中的yield返回值">十一、Switch中的Yield返回值：</h3>
<p>大多数时候，在<code>switch</code>表达式内部，我们会返回简单的值。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String fruit = <span class="string">&quot;orange&quot;</span>;</span><br><span class="line">        <span class="keyword">int</span> opt = <span class="keyword">switch</span> (fruit) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;apple&quot;</span> -&gt; <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;pear&quot;</span>, <span class="string">&quot;mango&quot;</span> -&gt; <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">default</span> -&gt; &#123;</span><br><span class="line">                <span class="keyword">int</span> code = fruit.hashCode();</span><br><span class="line">                yield code; <span class="comment">// switch语句返回值</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        System.out.println(<span class="string">&quot;opt = &quot;</span> + opt);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="十二for-each-循环">十二、for each 循环</h3>
<p>和<code>for</code>循环相比，<code>for each</code>循环的变量n不再是计数器，而是直接对应到数组的每个元素。<code>for each</code>循环的写法也更简洁。但是，<code>for each</code>循环无法指定遍历顺序，也无法获取数组的索引。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] ns = &#123; <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span> &#125;;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> n : ns) &#123;</span><br><span class="line">            System.out.println(n);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="十三排序库arrays.sort">十三、排序库：Arrays.sort()</h3>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] ns = &#123; <span class="number">28</span>, <span class="number">12</span>, <span class="number">89</span>, <span class="number">73</span>, <span class="number">65</span>, <span class="number">18</span>, <span class="number">96</span>, <span class="number">50</span>, <span class="number">8</span>, <span class="number">36</span> &#125;;</span><br><span class="line">        Arrays.sort(ns);</span><br><span class="line">        System.out.println(Arrays.toString(ns));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="十四多维数组">十四、多维数组：</h3>
<h4 id="二维数组">1) 二维数组</h4>
<p>二维数组的每个数组元素的长度并不要求相同</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[][] ns = &#123;</span><br><span class="line">            &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span> &#125;,</span><br><span class="line">            &#123; <span class="number">5</span>, <span class="number">6</span> &#125;,</span><br><span class="line">            &#123; <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span> &#125;</span><br><span class="line">		&#125;;</span><br><span class="line">        System.out.println(ns.length); <span class="comment">// 3</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>打印多维数组可以使用<code>Arrays.deepToString()</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[][] ns = &#123;</span><br><span class="line">            &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span> &#125;,</span><br><span class="line">            &#123; <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span> &#125;,</span><br><span class="line">            &#123; <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span> &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        System.out.println(Arrays.deepToString(ns));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="十五命令行参数">十五、命令行参数：</h3>
<p>命令行参数类型是<code>String[]</code>数组； 命令行参数由JVM接收用户输入并传给<code>main</code>方法；如何解析命令行参数需要由程序自己实现。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (String arg : args) &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="string">&quot;-version&quot;</span>.equals(arg)) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;v 1.0&quot;</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>java系列笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>《UniFormer:Unifying Convolution and Self-attention for Visual Recognition》</title>
    <url>/2022/01/27/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B06%E2%80%94%E2%80%94%E3%80%8AUniFormer%20Unifying%20Convolution%20and%20Self%20Attention%20for%20Visual%20Recognition%E3%80%8B/</url>
    <content><![CDATA[<h4 id="论文名称uniformer-unifying-convolution-and-self-attention-for-visual-recognition">论文名称：《UniFormer: Unifying Convolution and Self-attention for Visual Recognition》</h4>
<h4 id="论文地址-httpsarxiv.orgpdf2201.09450.pdf">论文地址： https://arxiv.org/pdf/2201.09450.pdf</h4>
<h2 id="关键词">1、关键词：</h2>
<p>​ Visual Recognition、CNN、SelfAttention、Transformer</p>
<h2 id="领域背景visual-recognition">2、领域背景—Visual Recognition：</h2>
<p>​ 表征学习一直是视觉识别领域的比较基本的研究。主要在面临向图像以及视频数据的时候，会存在两个问题：</p>
<ul>
<li>1、局部区域（空间、时间、时空）中的视觉内容往往是相似的，也就意味着局部冗余会比较大，这会导致不必要的计算量</li>
<li>2、全局的依赖关系比较复杂，在不同区域的目标物体之间都会存在一些动态的关系。</li>
</ul>
<h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2>
<p>​ 基于领域背景中的两个主要问题，主流的解决方案就是CNN和ViT，但是它们都只针对上述问题中的某一个做出了解决，而忽略了另一个。如下所示：</p>
<ul>
<li><p>CNN能够通过小范围的卷积，降低局部冗余，减小计算量，但在捕获全局依赖中很有限制。</p></li>
<li><p>ViT能够通过Self-Attention很有效的捕获长距离的依赖，但是盲目的去计算所有tokens之间的相似关系，会带来很大的计算冗余。其在浅层网络上很难有效的去编码局部的特征。</p>
<ul>
<li>本文的作者进行了一个实验：如下图所示，显示的是ViT网络的第三层的Attention图，我们发现虽然Attention机制计算了全局的token之间的关系，但最终在锚点（绿色方框）学习到的有效信息均来源于其很小的一个邻域范围（红色填充方框）。所以,ViT花费了非常大的计算复杂度，去编码了一个局部的视觉表示特征。</li>
<li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220129104342853.png" /></li>
</ul></li>
</ul>
<h2 id="主要设计思想">4、主要设计思想：</h2>
<p>​ 基于上述的工作描述，作者提出了UniFormer，将3D卷积和时空自注意力机制结合在一个简洁的<a href="https://so.csdn.net/so/search?q=transformer&amp;spm=1001.2101.3001.7020">transformer</a>结构中，能够同时处理局部冗余和全局依赖，其主要包括三个模块：</p>
<ul>
<li>Dynamic Position Embedding( DPE )</li>
<li>Multi-Head Relation Aggregator ( MHRA ) <strong>【和CNN与ViT的主要不同所在】</strong></li>
<li>Feed-Forward Network ( FFN )</li>
</ul>
<p>UniFormer与其他transformer的区别主要在于MHRA模块：</p>
<ul>
<li><p>在浅层，aggregator利用一个小的learnable matrix学习局部的token之间的相似性关系，通过聚合小的3D邻域的token信息极大地减少计算量。在深层，aggregator学习全局token之间的相似性关系，可以灵活的建立远距离图像区域或视频帧的token之间的长程依赖关系。</p></li>
<li><p>最后，通过以分层方式逐步堆叠局部和全局 UniFormer 块，我们可以灵活地整合它们的协作能力来促进Representation Learning。 最后，我们为视觉识别提供了一个通用且强大的主干，并通过简单而精细的适应成功地解决了各种下游视觉任务。</p></li>
</ul>
<h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2>
<h3 id="uniformer-block">1) UniFormer Block</h3>
<p>​ 下面图片就是UniFormer Block的整体架构，其中在标注维度的地方，所有标红的字符都是仅对于视频输入有效，代表了输入视频的帧数，如果输入是个图像，那么这些标红的值应该都 = 1.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127163758510.png" /></p>
<ul>
<li><strong>概述</strong>：Uniformer Block 的整体架构如上所示，分割成了几个阶段，每个阶段中由三个核心的模块重复堆叠L次组成，下面就是单词堆叠中，所有的模块的大致介绍：
<ul>
<li>Dynamic Position Embedding( DPE )</li>
<li>Multi-Head Relation Aggregator ( MHRA )</li>
<li>Feed-Forward Network ( FFN )</li>
<li>我们首先引入 DPE 将位置信息动态集成到所有Tokens（等式1）。它支持任意输入分辨率，并充分利用Tokens的顺序以获得更好的视觉识别效果。 然后，我们使用 MHRA ，其利用每个Token的上下文token，通过关系学习的方式，来增强每个Token（等式 2）。 通过在浅层和深层灵活设计Token的相似性，我们的 MHRA 可以巧妙地统一卷积和自注意力机制，以减少局部冗余并学习全局依赖性。 最后，我们像传统的 ViTs 一样添加 FFN，它由两个线性层和一个非线性函数GELU组成（等式 3）。 通道数先扩大4倍再恢复，因此每个token会被单独增强.</li>
</ul></li>
<li><strong>输入</strong>：<span class="math inline">\(X_{in} \in R^{C \times T \times H \times W}\)</span>, T 为视频帧数，当输入为图像时，T = 1</li>
<li><strong>输出</strong>：<span class="math inline">\(Z\)</span>特征空间向量</li>
<li><strong>公式表达</strong>：
<ul>
<li><span class="math inline">\(X = DPE(X_{in}) + X_{in}\)</span></li>
<li><span class="math inline">\(Y = MHRA(Norm(X)) + X\)</span></li>
<li><span class="math inline">\(Z = FFN(Norm(Y)) + Y\)</span></li>
</ul></li>
</ul>
<h3 id="multi-head-relation-aggregator">2) Multi-Head Relation Aggregator</h3>
<ul>
<li><p><strong>概述</strong>：该模块可以巧妙地统一卷积和自注意力机制，以减少局部冗余并学习全局依赖性。MHRA使用multi-head机制来计算token间的关系，公式表达如下：</p></li>
<li><p><strong>公式表达</strong>：</p>
<ul>
<li><strong>MHRA模块的第n个head</strong>： <span class="math inline">\(R_n(X) = A_nV_n(X)\)</span>，输入向量<span class="math inline">\(X \in R^{C \times T \times H \times W}\)</span>，我们会将其首先Reshape成一个token的序列<span class="math inline">\(X \in R^{L \times C}\)</span>, 其中 <span class="math inline">\(L = T \times H \times W\)</span></li>
<li><strong>总体</strong>： <span class="math inline">\(MHRA(X) = Concat(R_1(X);R_2(X);...;R_N(X))U\)</span>，<span class="math inline">\(U \in R^{C \times C}\)</span>是一个可学习矩阵，用于聚合N个Head的内容</li>
</ul></li>
<li><p><strong>单个Head内部变换细节介绍</strong>：</p>
<ul>
<li>每个RA包含<strong>token context encoding </strong>和 <strong>token affinity learning</strong>两步</li>
<li>1、我们应用<strong>线性变换</strong>将<strong>原始标记</strong>编码为<strong>上下文标记</strong>：<span class="math inline">\(V_n(X) \in R^{L \times \frac{C}{N}}\)</span></li>
<li>2、An是 <strong>token相似度</strong>，RA可以在其指导下来概括上下文信息，进行总结</li>
</ul></li>
<li><p><strong>浅层网络中的Local MHRA</strong>:</p>
<ul>
<li><p><strong>概述：</strong>在先前的比较中，我们发现：在浅层网络计算全局Self-Attention最终学到的也只是局部的信息，所以我们在浅层网络中，现在只在局部区域内来进行计算。因此，在浅层网络中，我们将Local Affinity作为一个<strong>可学习参数的矩阵</strong>。</p></li>
<li><p><strong>具体而言：</strong>给定anchor token <span class="math inline">\(X_i\)</span> , 局部RA学习该token和一个小邻域<span class="math inline">\(\Omega_i^{t \times h \times w}\)</span>内的其他token的相似性。<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220129110111062.png" alt="image-20220129110111062" /></p>
<p><span class="math inline">\(a_n \in R^{t \times h \times w}\)</span> , <span class="math inline">\(X_j\)</span> 代表邻域内的任一一个token，<span class="math inline">\(i-j\)</span> 表示token i和j的相对位置关系</p></li>
<li><p><strong>注意</strong>：因为tokens的感受野很小，相邻token之间的视觉内容在浅层中会发生微妙的变化。在这种情况下，没有必要去让token相似度动态的进行变化，因此我们使用一个可学习的参数矩阵来描述局部的token相似度，token的相似度仅取决于两个token之间的相对位置关系。</p></li>
<li><p><strong>与CNN的比较</strong>：其可以视为是 PWConv-DWConv-PWConv的组合，但是该论文中的Uniformer块是基于一个通用的Transformer的格式进行设计的，也就是说除了MHRA之外，还带有DPE和FFN，这一简单的继承能够非常有效的加强token的表示能力。</p></li>
</ul></li>
<li><p><strong>深层网络中的Global MHRA</strong>:</p>
<ul>
<li><p><strong>概述</strong>：在深层网络中，在更广阔的空间中获取长距离依赖很重要。因此，我们通过在全局条件下比较内容来进行token相似度的计算。</p></li>
<li><p><strong>具体而言</strong>：给定anchor token <span class="math inline">\(X_i\)</span> , 局部RA学习该token和所有其他token 的相似性</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220129111415083.png" /></p>
<p><span class="math inline">\(X_j\)</span> 代表全局范围<span class="math inline">\(size = T \times H \times W\)</span>内的任意一个token，<span class="math inline">\(Q_n 和 K_n\)</span>是两个不同的线性变换。</p></li>
<li><p><strong>与Transformer的比较</strong>：</p>
<p>可以被实例化为一个时空self-attention，<span class="math inline">\(Q_n 、K_n、V_n\)</span>代表Query、Key、Value.但是，UniformerBlock又和传统的ViT块不同：</p>
<ul>
<li>1、以往的video tranformer在视频域中分割开了时间和空间的attention，为了减少在计算token相似度比较时候的计算量。但是这会不可避免地恶化token之间的时空关系。相比之下，我们的模块联合编码时空token关系以生成更具辨别力的视频表示以进行识别。因为我们在浅层的时候大大的节省了token比较的计算量，所以整体模型上还是能到达一个计算量-准确度比较好的一个平衡。</li>
<li>2、其次，我们在 UniFormer 中采用动态位置嵌入 (DPE) 代替绝对位置嵌入。它是卷积风格的（见下节），可以克服排列不变性，对不同输入长度的视觉tokens友好</li>
</ul></li>
</ul></li>
</ul>
<h3 id="dynamic-position-embedding">3) Dynamic Position Embedding</h3>
<ul>
<li><p><strong>概述：</strong>先前绝大多数的位置编码都采用<strong>绝对或相对位置嵌入</strong>，然而绝对位置嵌入必须通过微调对各种输入大小进行插值，而相对位置嵌入由于自注意力机制的修改容易无法很好地工作。最近，有人提出了卷积位置编码，具体而言，卷积位置编码CPE，可以隐式地将位置信息通过卷积操作进行编码，这样可以让Transformer去处理任意的输入大小，并且提高识别性能。鉴于其即插即用地性质，我们将其拿过来作为我们的DPE模块：</p></li>
<li><p><span class="math inline">\(DPE(X_{in})= DWConv(X_in)\)</span> , DWConv 代表 depth-wise 卷积，zero padding</p></li>
<li><p><strong>原因</strong>：</p>
<ul>
<li><p>1、depthwise 卷积对任意的输入大小很友好（使用其时空版本对视频中的 3D 位置信息进行编码很简单）</p></li>
<li><p>2、很轻量化，能够较好的平衡计算量与准确性的平衡。</p></li>
<li><p>3、增加Zero padding，它可以通过逐步查询它们的邻居来帮助token了解它们的绝对位置</p>
<p>（Finally, we add extra zero paddings, since it can help tokens be aware of their absolute positions by querying their neighbors progressively 论文3.3最后一句话 ）</p></li>
</ul>
<!-- Zero Padding ？ 如何逐步查询令居进而帮助token了解绝对位置？ --></li>
</ul>
<h2 id="衍生框架-framework">6、衍生框架 FrameWork</h2>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220129115021079.png" /></p>
<h3 id="图像分类框架">1) 图像分类框架：</h3>
<ul>
<li>Stage1 和 Stage2 用的是 Local Uniformer Blocks
<ul>
<li>使用 PWConv-DWConv-PWConv 进行实例化</li>
</ul></li>
<li>Stage3 和 Stage4 用的是 Global Uniformer Blocks
<ul>
<li>使用 multi-head self-attention 进行实例化，heads = 64</li>
</ul></li>
<li>DPE 都实例化成 DWConv， spatial size = 3 x 3</li>
<li>FFN 的 Expand Ratio = 4</li>
<li>卷积 用 BN， self-attention 用 LN</li>
<li>特征降维：
<ul>
<li>Stage1前： $ 4  ， stride = 4 $</li>
<li>其他Stage前： $ 2  ， stride = 2 $</li>
</ul></li>
</ul>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127163758510.png" /></p>
<h3 id="视频分类-密集预测-等其他框架详见论文主体">2） 视频分类 &amp;&amp; 密集预测 等其他框架详见论文主体</h3>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Transformer</tag>
        <tag>Visual Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title>《HandWritting Transformers》(更新中)</title>
    <url>/2022/01/26/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B05%E2%80%94%E2%80%94%E3%80%8AHandWrittng%20Transformers%E3%80%8B/</url>
    <content><![CDATA[<h4 id="论文名称handwriting-transformers">论文名称：《<strong>Handwriting Transformers</strong>》</h4>
<h4 id="论文地址-httpsarxiv.orgabs2104.03964">论文地址： https://arxiv.org/abs/2104.03964</h4>
<h2 id="关键词">1、关键词：</h2>
<p>​ 手写字体生成（英文）、GAN、Transformer</p>
<h2 id="领域背景手写字体生成">2、领域背景—手写字体生成：</h2>
<p>​ 自动的手写文字生成对于一些书写障碍的人十分重要。通常使用的方法是利用GAN来进行离线的手写文字图像生成。</p>
<h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2>
<p>​ 两类生成方法： 基于笔画的在线生成方法（需要记录时序数据） 和 基于图像的离线生成方法。</p>
<ul>
<li><p>GANwriting ：该方法利用在少量信息中提取样式特征和预定义固定长度的文本内容来进行文本生成。</p></li>
<li><p>我们的方法：与GANwritting相似，我们的方法也是在少量的风格样例中去提取风格特征，但又与 GANwriting 不同，我们的方法具有生成任意长度的风格化文本的灵活性。 我们能够同时捕获全局和局部的书写风格。</p></li>
</ul>
<h2 id="主要设计思想">4、主要设计思想：</h2>
<p>两个主要改进的Motivation:</p>
<p>1、除了字/行级别的样式-内容纠缠之外，字符级别的样式和内容之间的纠缠有助于模仿特定字符的写作风格以及泛化到词汇外的内容。</p>
<p>2、为了生成准确的风格文本图像，需要模仿全局的（例如墨水宽度、倾斜度等）和局部的（例如字符风格、连字等）风格特征。</p>
<h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2>
<p><strong>问题公式化描述：</strong></p>
<ul>
<li><span class="math inline">\(i \in W\)</span>, <span class="math inline">\(W\)</span>包含M个作者，<span class="math inline">\(i\)</span>代表某一个作者</li>
<li>一个手写文字图像集合 P</li>
<li>输入的文本内容<span class="math inline">\(A = \{a_j\}_{j=1}^Q\)</span>，视为一个Word String的集合</li>
<li>每个WordString 包含长度不定的字符，字符来自于字符集C</li>
<li>字符集C包含 字母表、标点、数字</li>
<li><span class="math inline">\(\hat X_i^t\)</span>代表依据新的文本内容t，生成的作者<span class="math inline">\(i\)</span>的新图像</li>
</ul>
<h3 id="整体架构overall-pipeline">1) 整体架构（Overall Pipeline）</h3>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127103226508.png" /></p>
<ul>
<li><strong>概述</strong>：先利用CNN，将风格图像变换至高维特征空间，然后进入Transformer Encoder中，生成特征潜在向量Z，输入到Transformer Decoder中，与Query Words进行结合，解码输出F，再通过CNN Decoder，最终生成图像，然后进入到不同的判别器中，基于不同的角度，定义了4个损失函数，进行函数的优化。</li>
<li>Query Words进入到Transformer Decoder中前，需要进行词嵌入编码，对每个字符，定义了一个可学习的词嵌入向量，<span class="math inline">\(q_c \in R^{512}\)</span>，这样一种基于字符的表示，以及基于transformer的序列处理方式，帮助模型能够生成任意长度的手写单词，并且能够更好的泛化到词汇表外的数据。
<ul>
<li><span class="math inline">\(G_\theta\)</span>生成器，用于合成手写文本图像</li>
<li><span class="math inline">\(D_\Psi\)</span>判别器，用于确保生成图像的 手写风格的真实性（即确保看上去是手写图像）</li>
<li><span class="math inline">\(R_\phi\)</span>识别器，用于保持文本内容正确</li>
<li><span class="math inline">\(S_\eta\)</span>风格分类器，用于确保迁移的手写风格正确性（即确保生成的风格与Style Example一致）</li>
</ul></li>
<li><strong>输入</strong>： <span class="math inline">\(a_j \in A\)</span> &amp;&amp; <span class="math inline">\(X_i^s\)</span>文本 + 风格图像示例</li>
<li><strong>输出</strong>： <span class="math inline">\(\hat X_i^t\)</span>新生成的风格化手写字符图像（文本内容为 t ）</li>
</ul>
<h3 id="encoder-network-tepsilon">2) Encoder Network <span class="math inline">\(T\epsilon\)</span></h3>
<ul>
<li><strong>概述</strong>：将风格手写示例图编码至风格特征空间向量</li>
<li><strong>输入</strong>：<span class="math inline">\(X_i^s\)</span>风格手写示例图</li>
<li><strong>输出</strong>：<span class="math inline">\(Z \in R^{N \times d}\)</span>风格特征空间向量</li>
<li><strong>网络结构</strong>：
<ul>
<li><strong>Part1 : CNN Encoder</strong>
<ul>
<li>采用<strong>ResNet18</strong>: 为每个style image生成低分辨率的激活图<span class="math inline">\(h_{ij} \in R^{h \times w \times d}\)</span></li>
<li>将<span class="math inline">\(h_{ij}\)</span>在空间维度上展平，得到一个Sequence, <span class="math inline">\(n \times d\)</span>，其中<span class="math inline">\(n = h \times w\)</span>, 这个序列可以被视为风格图像某区域的描述子</li>
<li>同时，因为总共我们会提供P张风格示例图像，将所有风格示例图像中提取出的Sequence进行拼接，得到一个Tensor，<span class="math inline">\(H_i \in R^{N \times d}\)</span>其中，<span class="math inline">\(N = n \times P\)</span></li>
</ul></li>
<li><strong>Part2: Transformer-Based Encoder</strong>
<ul>
<li>L层，每一层都由multihead-self-attention 和 MLP模块组成。重复L遍，很标准的Transformer Encoder（论文中没有细讲有无Residual）</li>
<li>为保留提供的输入序列的未知信息，采用固定位置编码</li>
</ul></li>
</ul></li>
<li><strong>意义</strong>：对局部的和全局的手写风格图像特征建模，例如倾斜、歪斜、字符形状、连字、墨水宽度等</li>
</ul>
<h3 id="decoder-network-t_d">3) Decoder Network <span class="math inline">\(T_d\)</span></h3>
<ul>
<li><strong>概述</strong>：结合输入的字符序列，生成风格化的手写字符图像</li>
<li><strong>输入</strong>：<span class="math inline">\(Z \in R^{N \times d}\)</span>风格特征空间向量 + <span class="math inline">\(A\)</span>输入的字符序列</li>
<li><strong>输出</strong>：<span class="math inline">\(\hat X_i^t\)</span>新生成的风格化手写字符图像（文本内容为 t ）</li>
<li><strong>网络结构</strong>：
<ul>
<li><strong>Part1: Query Words 处理</strong>
<ul>
<li>将Query Words 编码成 Query Embedding，其为可学习的位置编码参数，加入到之后Transformer-Based Decoder中。简单来讲，这一步的作用就是要让每一个Query Embedding都在提供的样式图像中，能够去查找感兴趣的区域，从而进一步推断所有查询字符的风格属性。</li>
<li>如下图所示，t对应的位置编码，需要去学习的就是在我们提供的样例风格图像中，寻找字符t相关的位置（感兴趣的区域）。</li>
<li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127112528447.png" /></li>
</ul></li>
<li><strong>Part2: Transformer Based Decoder:</strong>
<ul>
<li>注意1：和self-attention不同，其key &amp; value 来源于Encoder的输出， query向量来源于Decoder自己的层。</li>
<li>注意2：在所有的Transformer Based Decoder中，我们是并行的去处理每一个Query Embeddings的，每个Query Embedding就代表一个字符。</li>
<li>将上一步的Query Embeddings 经过连续的 Transformer Based Decoder以后，会累积一些风格信息，产生一个输出 <span class="math inline">\(F_{a_j} \in R^{m_j \times d}\)</span>. <span class="math inline">\(m_j\)</span>是某个单词的字符数</li>
<li>然后，我们会将<span class="math inline">\(N(0,1)\)</span>的噪音，加入到 <span class="math inline">\(F_{a_j}\)</span>中，来模拟自然情况下的外部干扰变化。</li>
<li><strong>举例而言</strong>：
<ul>
<li>一个m个字符的单词</li>
<li>我们会将m个Embeddings Vectors连接起来，然后再通过一个FC层，得到一个<span class="math inline">\(m_j \times 8192\)</span>的矩阵，我们将其Reshape成 <span class="math inline">\(512 \times 4 \times 4m_j\)</span>,然后输入到CNN Decoder中。</li>
</ul></li>
</ul></li>
<li><strong>Part3: CNN Decoder</strong>:
<ul>
<li>4 个残差模块 + tanh激活函数</li>
<li>获得最终的输出图像</li>
</ul></li>
</ul></li>
</ul>
<h3 id="training-loss-objectives">4) Training &amp; Loss Objectives</h3>
<ul>
<li><p><strong>概述</strong>：总共由4个Loss函数组成，每个Loss负责不同的部分，在网络结构概述中已经有所提及。</p>
<ul>
<li>1、<span class="math inline">\(D_\Psi\)</span>判别器，用于确保生成图像的 手写风格的真实性（即确保看上去是手写图像）</li>
</ul>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127115520938.png" /></p>
<ul>
<li><p>2、<span class="math inline">\(R_\phi\)</span>手写文本识别器，用于保持文本内容正确，使用CRNN搭建，使用CTC Loss函数，比较query words和识别输出的差别。识别器 Rφ 仅针对真实的、标记的、手写样本进行优化训练，但它用于鼓励 Gθ 生成具有准确内容的可读文本。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127115740239.png" /></p></li>
<li><p>3、<span class="math inline">\(S_\eta\)</span>风格分类器，用于确保迁移的手写风格正确性（即确保生成的风格与Style Example一致） ，其能够预测一个给定的手写图像的作者。使用Cross-Entropy来定义Loss函数：其只在真实的样例上利用如下损失函数进行训练：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127115916996.png" /></p></li>
<li><p>4、利用Cycle Loss 来确保编码的style features由循环一致性。这个损失函数能够让decoder最大程度上在解码阶段保留风格信息，使得我们能够从生成的图像中重建出最开始的风格特征序列。给定生成的图像 <span class="math inline">\(\hat X_i^t\)</span>，我们使用 编码器<span class="math inline">\(T\epsilon\)</span>来重建风格特征向量<span class="math inline">\(\hat Z\)</span>。如下所示的循环损失<span class="math inline">\(L_c\)</span>用于最小化<span class="math inline">\(Z\)</span>和 <span class="math inline">\(\hat Z\)</span>之间的L1差距</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127120112639.png" /></p>
<p>循环损失对解码器施加了正则化，以一致地模仿生成的样式文本图像中的写作风格。</p></li>
</ul></li>
<li><p>总的来说，我们以端到端的方式训练我们的 HWT 模型，损失目标如下</p></li>
</ul>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127120324671.png" /></p>
<ul>
<li>同时，我们观察到平衡网络 Sη 和 Rφ 的梯度有助于使用我们的损失公式进行训练。根据 [3]，我们将 ∇Sη 和 ∇Rφ 归一化，使其具有与对抗性损失梯度相同的标准偏差 (σ)</li>
<li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220127120433481.png" /></li>
<li><span class="math inline">\(\alpha\)</span>是一个超参数，在训练我们的模型的时候被固定为1</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>HandWritting Generation</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Tmux常用命令</title>
    <url>/2022/01/26/%E5%B7%A5%E5%85%B7%E7%B1%BB%E7%AC%94%E8%AE%B0/tmux_command/</url>
    <content><![CDATA[<h3 id="tmux常用命令">Tmux常用命令</h3>
<ul>
<li><p>创建并指定session名字 tmux new -s $session_name</p></li>
<li><p>删除session Ctrl+b :kill-session</p></li>
<li><p>临时退出session Ctrl+b d</p></li>
<li><p>列出session tmux ls</p></li>
<li><p>进入已存在的session tmux a -t $session_name</p></li>
<li><p>删除所有session Ctrl+b :kill-server</p></li>
<li><p>删除指定session tmux kill-session -t $session_name</p></li>
<li><p>开启光标</p></li>
</ul>
<p>​ ctrl + b 按下后松开 再立马按 [</p>
<ul>
<li>关闭光标</li>
</ul>
<p>​ ctrl + q 按下后松开 再立马按 [</p>
]]></content>
      <categories>
        <category>工具类使用笔记</category>
      </categories>
      <tags>
        <tag>Tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记14——ResNet详解及为什么能解决深度网络退化问题</title>
    <url>/2022/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B014%E2%80%94ResNet%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E8%A7%A3%E5%86%B3%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E9%80%80%E5%8C%96%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="一深度网络退化背景">一、深度网络退化背景</h2>
<p>​ 对于卷积神经网络，深度是一个很重要的因素。深度卷积网络自然的整合了低中高不同层次的特征，特征的层次可以靠加深网络的层次来丰富。因此在构建卷积网络时，网络的深度越高，可抽取的特征层次就越丰富越抽象。所以一般我们会倾向于使用更深层次的网络结构，以便取得更高层次的特征。但是更深层的网络结构有的时候并不会带来更好的结果，层数一旦过多以后，就会导致表现明显下降，这就是深度网络的退化问题。</p>
<p>​ 深度网络的退化问题到底是由于什么导致的呢？过拟合？还是梯度消失？梯度爆炸？</p>
<p>​ 其实都不是。如下图论文中显示的所示，显然其在训练集上的误差也很大，就不可能是过拟合问题。而梯度消失或梯度爆炸，通过加入BN层，就能够通过规整数据分布来解决这个问题，所以应当也不是梯度消失或爆炸的问题。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS82Y2M3OThjNC0wMTNmLTRkNjQtYmY5Yi0yZDg0YWExOTQzNzYucG5n.png" /></p>
<p>​ 那么根本原因是什么呢？</p>
<p>​ 在<strong>MobileNet V2</strong>的论文中提到，由于非线性激活函数Relu的存在，每次输入到输出的过程都几乎是不可逆的，这也造成了许多<strong>不可逆的信息损失</strong>。我们试想一下，一个特征的一些有用的信息损失了，那他的表现还能做到持平吗？显然不可能做到持平。随着网络层数的加深，造成了许多不可逆的信息损失，最终导致了深度网络的退化问题。</p>
<h2 id="二resnet提出初衷与详解">二、ResNet提出初衷与详解</h2>
<p>​ 我们选择加深网络的层数，是希望深层的网络的表现能比浅层好，或者是<strong>希望它的表现至少和浅层网络持平（相当于直接复制浅层网络的特征）</strong>，可实际的结果却让我们大吃一惊（深度网络退化），这是为什么呢？</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS81OGU1NDU3Yy1lZWEwLTRmMjctOGVlMS04ZDJhMTk4YmJkOTcucG5n.png" /></p>
<p>​ 如图所示，这是一个直观的例子，我们把右边的网络理解为左边浅层网络加深了三层（框起来的部分），假如我们希望右边的深层网络与左边的浅层网络持平，即是希望框起来的三层跟没加一样，也就是加的三层的输入等于输出。我们假设这三层的输入为x，输出为H(x)，那么深层网络与浅层网络表现持平的直观理解即是H(x)=x，这种让输出等于输入的方式，就是ResNet论文中提到的<strong>恒等映射（identity mapping)</strong>。</p>
<p>​ 所以<strong>ResNet的初衷，就是让网络拥有这种恒等映射的能力，能够在加深网络的时候，至少能保证深层网络的表现至少和浅层网络持平</strong>。</p>
<p>​ 所以，现在我们知道，<strong>如果想要让深度神经网络不退化，根本的原因就是如何去做到恒等映射</strong>。然而现有的神经网络很难拟合潜在的恒等映射函数，H(x) = x，因为神经网络内部总会做一些参数的调整。但我们如果把恒等映射作为网络的一部分，将网络设计为H(x) = F(x) + x的形式，即如残差结构中所示那样，网络的输出是由x 和 F(x) 相加得到的，那么就可以把问题转化为 让中间多出来的那三层，去学习一个残差函数F(x) = H(x) - x。只要学习到的残差函数，能够使得F(x) = 0，那么就构成了一个恒等映射。最终网络的结果就不会比失去这几层要差。<strong>并且，让网络去拟合一个残差比拟合一个恒等映射容易得多（原因见后）</strong></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS9mNGI5ZjRjYi04NWE4LTRmNjQtYjc0Ny1lMzU0ZDdjNmM2YmUucG5n.png" /></p>
<p>​ ResNet中，<strong>shortcut connection</strong>，<strong>通过跳接在激活函数前，将上一层（或几层）的输出与本层输出相加，将求和的结果输入到激活函数作为本层的输出</strong></p>
<p>​</p>
<h2 id="三为什么resnet能解决深度网络退化问题">三、为什么ResNet能解决深度网络退化问题？</h2>
<p>1、加了残差结构后就是给了输入x多一个选择，在神经网络学习到这层的参数是冗余的时候它可以选择直接走这条“跳接”曲线，跳过这个冗余层，而不需要再去拟合参数使得输出H(x)等于x。</p>
<p>2、因为学习残差的计算量比学习恒等映射小。假设普通网络为A，残差网络为B，输入为2，输出为2（输入和输出一样是为了模拟冗余层需要恒等映射的情况），那么普通网络就是A (2) = 2，而残差网络就是B ( 2 ) = F ( 2 ) + 2 = 2，显然残差网络中的F ( 2 ) = 0 。我们知道网络中权重一般会初始化成0附近的数，那么我们就很容易理解，为什么让F(2)（拟合0会比A (2) = 2 容易了</p>
<p>3、我们知道ReLU能够将负数激活为0，而正数输入等于输出。这相当于过滤了负数的线性变化，让F(x)=0变得更加容易。</p>
<p>4、我们知道残差网络可以表示成H ( x ) = F ( x ) + x ，这就说明了在求输出H ( x ) 对输入x 的倒数（梯度），也就是在反向传播的时候，H ′ ( x ) = F ′ ( x ) + 1，残差结构的这个常数1也能保证在求梯度的时候梯度不会消失。</p>
<h2 id="四一些细节问题">四、一些细节问题：</h2>
<p>​ 在ResNet中，残差连接的相加，指的是逐元素相加，在ReSNet的网络示意图中，有的Skip-Connection是实线，有的是虚线，<strong>虚线的代表这些模块前后的维度不一致，因为去掉残差结构的Plain网络还是和VGG一样，也就是每隔n层进行下采样但深度翻倍（VGG通过池化层下采样，ResNet通过卷积）</strong>：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS81YjZhMTNiZi00ZTU1LTRlMzYtOWY1NC05YzAzYzhkMmVkY2EucG5n.png" /></p>
<ul>
<li><p>空间上不一致时，需要给输入的X做一个线性的映射：调整一下H*W维度</p></li>
<li><p>深度上不一致时，有两种解决方法，一种是在跳接过程中加一个1×1的卷积层进行升维，另一种则是直接补零（先做下采样）。</p></li>
<li><p>针对比较深的神经网络，作者也考虑到计算量，会先用1×1的卷积将输入的256维降到64维，然后通过1×1恢复。这样做的目的是减少参数量和计算量。</p></li>
</ul>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cHM6Ly9pbWdrci5jbi1iai51ZmlsZW9zLmNvbS85M2RiMDgzYi05NTk3LTRjZmUtODJmZC04NzRlNTI2NjViNmIucG5n.png" /></p>
<p>引用：https://blog.csdn.net/cristiano20/article/details/104309948</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>ResNet</tag>
        <tag>Degradation</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记13——DropOut详解及为什么能够防止过拟合</title>
    <url>/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B013%E2%80%94DropOut%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E5%A4%9F%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88/</url>
    <content><![CDATA[<h2 id="一dropout是什么">一、DropOut是什么？</h2>
<p>​ 过拟合是Deep Neural Networks(DNN)网络存在的问题之一。过拟合的特点是模型对训练数据的拟合非常好，但对测试数据的拟合却非常差，具体表现为loss和在训练集上的错误率非常低，而在验证集或测试集上却都要高很多。针对解决过拟合问题设计出来的方法很多，dropout就是其中一种最简单，也是最有效的方法。</p>
<p>​ 在训练DNN网络的过程中，对于每一个神经元，以p的概率被随机的drop out，也就是将其值置零。这样，在该轮前传和反传的过程中，该神经元将失去作用，相当于不存在，如下图所示。DropOut整体来说，是在训练过程中以一定的概率的使神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/dropout.jpeg" /></p>
<h2 id="二dropout阶段在训练阶段和测试阶段的区别">二、DropOut阶段在训练阶段和测试阶段的区别：</h2>
<h3 id="训练阶段">1、训练阶段：</h3>
<p>​ 在前向传播时，假设有这一层n个神经元，则我们可以假设每个神经元的概率都是0~1(可以通过python得到随机的值)，然后小于p的就失活，即不参与训练。</p>
<p>​ 在反向传播时，也只对参与训练的神经元进行参数更新。</p>
<p>​ 每次训练的时候，又是n个神经元，重新进行dropout</p>
<p>​ <strong>Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。</strong></p>
<h3 id="测试阶段">2、测试阶段：</h3>
<p>​ 在测试时，应该用整个训练好的模型，不需要进行dropout。</p>
<p>​ 参与学习的节点和那些被隐藏的节点需要以一定的概率p加权求和，综合计算得到网络的输出。</p>
<p>​ 即预测的时候，每一个单元的参数要预乘以p。为什么要预乘以p呢？</p>
<p>​ 因为我们训练的时候会随机的丢弃一些神经元，但是预测的时候就没办法随机丢弃了。<strong>如果丢弃一些神经元，这会带来结果不稳定的问题，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，这是实际系统不能接受的</strong>，用户可能认为模型预测不准。那么<strong>一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的</strong>。</p>
<p>​ <strong>比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是<span class="math inline">\(p \times x+(1-p) \times 0 = p \times x\)</span>。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望。</strong></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220121204115930.png" /></p>
<h2 id="三dropout如何防止过拟合">三、DropOut如何防止过拟合？</h2>
<p><strong>（1）数据层面</strong></p>
<p>​ 对于每一个dropout后的网络，进行训练时，相当于做了Data Augmentation。比如，对于某一层，dropout一些单元后，形成的结果是(1.5，0，2.5，0，1，2，0)，其中0是被drop的单元，那么总能找到一个样本，使得结果也是如此。这样每一次dropout其实都相当于增加了样本。</p>
<p><strong>（2）模型层面</strong></p>
<ul>
<li><p><strong>在较大程度上减小了网络的大小：</strong>在这个“残缺”的网络中，让神经网络学习数据中的局部特征（即部分分布式特征），但这些特征也足以进行输出正确的结果。</p></li>
<li><p><strong>取平均的作用：</strong> 如果正常的模型（没有dropout），我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。（例如 3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果）。<strong>这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。每次训练随机dropout掉不同的隐藏神经元，网络结构已经不同，这就类似在训练不同的网络，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</strong></p></li>
<li><p><strong>减少神经元之间共适应关系：</strong> 因为dropout导致两个神经元不一定每次都在一个网络中出现，这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况， 迫使网络去学习更加鲁棒的特征。换句话说，假如神经网络是在做出某种预测，<strong>它不应该对一些特定的线索片段太过敏感</strong>，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。</p></li>
</ul>
<p>引用：</p>
<p>https://zhuanlan.zhihu.com/p/175142160</p>
<p>https://www.zhihu.com/question/402485242/answer/1553947864</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>OverFitting</tag>
        <tag>DropOut</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记12——BatchNorm详解及为什么能防止过拟合</title>
    <url>/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B012%E2%80%94BatchNorm%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88/</url>
    <content><![CDATA[<h2 id="一详解batchnorm原理">一、详解BatchNorm原理：</h2>
<p>​ BatchNorm是一种能够加速深度神经网络收敛，避免过拟合的方法，那么为什么呢？首先我们需要探讨一下这个问题，为什么深度神经网络<strong>随着网络深度加深，训练起来越困难，收敛越来越慢？</strong></p>
<p>在回答这个问题前需要首先了解两个概念：</p>
<p>​ <strong>1、独立同分布（IID）</strong>：即假设训练数据和测试数据是满足相同分布的。它是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障</p>
<p>​ <strong>2、Covariate shift</strong>：<strong>如果ML系统实例集合&lt;X,Y&gt;中的输入值X的分布老是变，这不符合IID假设</strong>，网络模型很难<strong>稳定的学规律</strong>。</p>
<p>​ 所以，之所以深度神经网络随着网络深度加深，训练越来越困难是因为，对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是<strong>在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift（ICS）”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层</strong></p>
<p>​ BatchNorm的基本思想就是能不能<strong>让每个隐层节点的激活输入分布固定下来呢</strong>？这样就避免了“Internal Covariate Shift”问题了。</p>
<p>所以BN实质上就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的一种方法。</p>
<p>​ BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的<strong>激活输入值随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近</strong>（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这<strong>导致反向传播时低层神经网络的梯度消失</strong>，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>，<strong>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是<strong>这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p>
<p>​ <strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</strong>因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。BN说到底就是这么个机制，方法很简单，道理很深刻。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/aHR0cDovL3FsLm1hZ2ljLXNldmVuLnRvcC91cGxvYWQvMjAyMC8zL2ltYWdlLTVkMjU4MDQxNGI0YTRkNjhiODEzMDMxMDZlMzY5YzNiLnBuZw.png" /></p>
<h2 id="二batchnorm在训练阶段和测试阶段的做法与意义">二、BatchNorm在训练阶段和测试阶段的做法与意义：</h2>
<h3 id="训练阶段">1、训练阶段：</h3>
<p>​ 首先计算均值和方差（每次训练给一个批量，计算批量的均值方差），然后归一化，然后缩放和平移，完事！</p>
<h3 id="测试阶段">2、测试阶段：</h3>
<p>​ 每次只输入一张图片，这怎么计算批量的均值和方差，于是，就有了代码中下面两行，在训练的时候实现计算好mean、 var，测试的时候直接拿来用就可以了，不用计算均值和方差。</p>
<p>​ 用训练集来估计总体均值 μ 和总体标准差 σ 。</p>
<p>​ 较为简单的做法就是把每个mini-batch的均值和方差都保存下来，然后训练完了求均值的均值，方差的均值即可。</p>
<p>​ 在测试阶段应用BatchNorm的含义，应该就是要让测试集的测试精度与整个训练网络保持一致。</p>
<h2 id="三batchnorm的两个参数gamma和beta有什么作用">三、BatchNorm的两个参数<span class="math inline">\({\gamma}\)</span>和<span class="math inline">\(\beta\)</span>有什么作用？</h2>
<p><img src="https://www.zhihu.com/equation?tex=y%3D\frac%7Bx-\mathrm%7BE%7D%5Bx%5D%7D%7B\sqrt%7B\operatorname%7BVar%7D%5Bx%5D%2B\epsilon%7D%7D+*+\gamma%2B\beta+" /></p>
<h3 id="如果只做归一化为什么是学不到任何东西的">1、如果只做归一化，为什么是学不到任何东西的？</h3>
<p>​ 如果在每一层之后都归一化成0-1的高斯分布（减均值除方差）那么数据的分布一直都是高斯分布，数据分布都是固定的了，这样即使加更多层就没有意义了，<strong>深度网络就是想学习数据的分布发现规律性，BN就是不让学习的数据分布偏离太远</strong></p>
<h3 id="两个参数的作用">2、两个参数的作用</h3>
<p>​ 为了减小Internal Covariate Shift，对神经网络的每一层做归一化不就可以了，假设将每一层输出后的数据都归一化到0均值，1方差，满足正胎分布，但是，此时有一个问题，<strong>如果每一层的数据分布都是标准正太分布，导致其完全学习不到输入数据的特征，因为，费劲心思学习到的特征分布被归一化了，因此，直接对每一层做归一化显然是不合理的。</strong>但是如果稍作修改，加入可训练的参数做归一化，那就是BatchNorm 实现的了。</p>
<p>​ 接下来详细介绍一下这额外的两个参数，之前也说过如果直接做归一化不做其他处理，神经网络是学不到任何东西的，但是加入这两个参数后，事情就不一样了。先考虑特殊情况下，如果γ 和β 分别等于此batch的标准差和均值，那么<span class="math inline">\(y_i\)</span>就还原到归一化前的x了吗，也即是缩放平移到了归一化前的分布，相当于batch norm没有起作用，<span class="math inline">\(β\)</span>和γ 分别称之为 平移参数和缩放参数 。这样就<strong>保证了每一次数据经过归一化后还保留的有学习来的特征，同时又能完成归一化这个操作，加速训练</strong>。</p>
<p>引用：https://www.cnblogs.com/hoojjack/p/12350707.html</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>OverFitting</tag>
        <tag>Batch Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记11——Training Data、Validation Data、Testing Data含义及作用</title>
    <url>/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B011%E2%80%94Training%20Data%E3%80%81Validation%20Data%E3%80%81Testing%20Data%E5%90%AB%E4%B9%89%E5%8F%8A%E4%BD%9C%E7%94%A8/</url>
    <content><![CDATA[<p>我们在进行一个机器学习的任务的时候，往往会将所有数据划分成三块——Training Data、Validation Data、Testing Data,它们各自在训练的过程中扮演何种角色呢？</p>
<h4 id="一trainvaltest的含义与作用">一、Train、Val、Test的含义与作用：</h4>
<p>顾名思义，三个数据集合它们的简单含义如下：</p>
<ul>
<li>训练集(train)：训练模型，用来拟合模型的数据集；</li>
<li>验证集(val)：评估模型，训练过程中提供相对于train的无偏估计的数据集，同时用来调整超参数和特征选择，实际参与训练</li>
<li>测试集(test)：最终模型训练好之后，用来提供相对于train+valid的无偏估计的数据集。</li>
</ul>
<p>​ 一般我们会将最开始划分的Training Set分割为Training Data和Validation Data两个集合，一般而言比例为9：1。我们使用划分后的Training Data进行训练，在每个Epoch结束后使用训练期间机器没有见到过的Validation进行验证，依据验证集得到的Loss值来进行模型好坏的衡量。</p>
<p>​ 话句话说，Validation Data　其实就是用来避免过拟合的，在训练过程中，我们通常用它来确定一些超参数（比如根据validation data上的accuracy来确定early stopping的epoch大小、根据validation data确定learning rate等等）。</p>
<p>​ 那为啥不直接在Testing data上做这些呢？因为如果在Testing data做这些，那么随着训练的进行，我们的网络实际上就是在一点一点地overfitting我们的Testing data，导致最后得到的Testing accuracy没有任何参考意义。因此，Training data的作用是计算梯度更新权重，Validation data在每个Epoch结束后进行验证，Testing data则给出一个accuracy以判断网络的好坏。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/20210202115236662.png" /></p>
<p><strong>如上所示的训练划分容易带来一些显而易见的问题：</strong></p>
<ul>
<li>如果样本数量太少，验证集和测试集更少，无法在统计学上代表数据</li>
<li>划分数据前时，进行不同的随机打乱则得到的模型性能差别可能很大，可能训练集中的数据都偏向于某一类，而验证集的数据偏向于另一类</li>
</ul>
<h4 id="二n-fold-cross-validation">二、N-Fold Cross Validation</h4>
<p>此时，就可以使用常见的叫做N-Fold Cross Validation，（K折交叉验证）：</p>
<p>​ 如下图所示,我们将Training Set分为N个集合(示例中为3个),其中N-1个集合用于训练,1个集合用于验证,然后每轮Epoch中,都执行N遍,每一遍都拿不同的集合用于训练与验证,然后计算一遍Loss值,最终选取平均Loss最小的那一组参数进行模型的更新.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114111918585.png" /></p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>Training Set</tag>
        <tag>Validation Set</tag>
        <tag>Testing Set</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记10——L1、L2正则化以及为什么正则化能够防止过拟合</title>
    <url>/2022/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B010%E2%80%94L1%20L2%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AD%A3%E5%88%99%E5%8C%96%E8%83%BD%E5%A4%9F%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88/</url>
    <content><![CDATA[<p>在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）。其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在Training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集，对训练集外的数据却不work。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/OhX9eFQ.jpg" /></p>
<p>​ 在ML2021课程系列笔记2中，提及了一些防止过拟合的内容，本篇用于详细解释其中正则化的部分：</p>
<h3 id="一什么是l1l2正则化">一、什么是L1、L2正则化？</h3>
<h4 id="l1-正则化">L1 正则化：</h4>
<p>​ 简单而言，L1正则化就是在Loss函数后面增加一个正则化项，L1正则化的公式如下,</p>
<p>​ <span class="math inline">\(C_0\)</span>为原来的损失函数，即所有权重w的绝对值的和. n是训练集的样本大小，λ 是正则项系数，C为加了正则化后的损失函数</p>
<p>​ <span class="math inline">\(C = C_0 + \frac{\lambda}{n} \sum_{w}|w|\)</span></p>
<h4 id="l2-正则化">L2 正则化：</h4>
<p>​ 简单而言，L2正则化也是在Loss函数后面增加一个正则化项，L2正则化的公式如下,与上含义类似。</p>
<p>​ <span class="math inline">\(C = C_0 + \frac{\lambda}{2n} \sum_w w^2\)</span></p>
<h3 id="二l1l2正则化如何避免overfitting">二、L1、L2正则化如何避免OverFitting？</h3>
<h4 id="l1l2正则化能降低权重值w">1、L1、L2正则化能降低权重值w</h4>
<p>​ 我们以L2正则化项为例，进行解释，首先我们让C对偏置项b和对权重系数w进行求导，得到如下：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/mebEC90.jpg" /></p>
<p>​ 我们发现，C对b求导与正则化项无关，C对w求导得到得结果与正则化项有关。</p>
<p>​ 最终所反映到梯度下降优化参数上，就是如下图所示得情况：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/qM83geg.jpg" /></p>
<p>​ <strong>如果没有正则化项，那么更新得w前得系数应当为1，而现在由于因为η、λ、n都是正的，所以 1−ηλ/n小于1，它的效果是减小w，这也就是权重衰减（weight decay）的由来</strong>。</p>
<p>​ <strong>到此为止，我们发现L2正则化项，其能够使得减小w。（其实这一点比较直观的也能看出来，因为Loss函数中加入了一项<span class="math inline">\(w^2\)</span>的求和，也就是说如果权重值w过大，Loss函数值会上升，这就意味着这一个正则化项惩罚了权值矩阵使其不能取太大值。）</strong></p>
<p>​ 那么，关键问题是，<strong>为什么权重矩阵w小，能够防止过拟合呢？</strong></p>
<h4 id="较高的权重w往往意味着过拟合的函数">2、较高的权重w往往意味着过拟合的函数：</h4>
<p>​ 我们会发现：<strong>过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。</strong></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/RsR5cOK.png" /></p>
<p>​</p>
<p>​ 更进一步的解释：当权重系数很小的时候，容易出现像左图一样，高偏差拟合能力很差的情况，随着权重系数逐渐增大，就会像右侧的图进行发展。如果权重系数很大，往往意味着在某些很小的区间中，函数值的变化会非常的剧烈，导致一些高方差的结果，也就是函数对于训练数据过度拟合了。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220121122935964.png" /></p>
<p>​</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>OverFitting</tag>
        <tag>Regularization</tag>
      </tags>
  </entry>
  <entry>
    <title>《DG-Font Deformable Generative Networks for Unsupervised Font Generation》</title>
    <url>/2022/01/20/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B04%E2%80%94%E2%80%94%E3%80%8ADG-Font%20Deformable%20Generative%20Networks%20for%20Unsupervised%20Font%20Generation%E3%80%8B/</url>
    <content><![CDATA[<h4 id="论文名称dg-font-deformable-generative-networks-for-unsupervised-font-generationcvpr2021">论文名称：《DG-Font: Deformable Generative Networks for Unsupervised Font Generation》CVPR2021</h4>
<h4 id="论文地址-httpsopenaccess.thecvf.comcontentcvpr2021htmlxie_dg-font_deformable_generative_networks_for_unsupervised_font_generation_cvpr_2021_paper.html">论文地址： https://openaccess.thecvf.com/content/CVPR2021/html/Xie_DG-Font_Deformable_Generative_Networks_for_Unsupervised_Font_Generation_CVPR_2021_paper.html</h4>
<h2 id="关键词">1、关键词：</h2>
<p>​ Font-Generation、Deformable Convolution Skip Connection、Unsupervised Learning</p>
<h2 id="领域背景">2、领域背景：</h2>
<p>​ 字体生成是一个具有挑战的任务，现存的大部分方法都是通过<strong>有监督学习</strong>的方法进行字体的生成，他们需要大量的<strong>paired data</strong>（例如对应风格的字符图像），然而大量的这些数据需要花费非常昂贵的代价去进行收集。</p>
<p>​ 字体生成目标是自动的能够生成某种特定字体的字符，并且创造一个字体字符集。</p>
<p>​ 在传统的字符集的创造方式中，严重依赖于专家设计者，独立的去绘制每一个字体的图像，这对于一些基于语标的语言（比如中文、日文、韩文）很不友好。专家们需要有大量的工作量去进行设计，字体生成领域就是为了解决这样一个问题，使用神经网络去学习从一种风格到另一种风格的映射关系，从而生成特定语言的某种风格的一个字符集。</p>
<p>​ 与字体生成最相关的<strong>图像生成领域 （Image-To-Image Translation）</strong>，在通常的image-to-image的转换模型中通常将<strong>style</strong>定义为<strong>纹理和颜色</strong>，而字体的<strong>style</strong>往往是指<strong>字体的格式几何形状、笔画粗细、笔尖和连笔书写的模式等内容。（geometric transformation, stroke thickness, tips, and joined-up writing pattern）</strong>，故而没法直接应用到字体的生成上。同时在图像中往往采用的<strong>AdaIN-based</strong>方法（这种方法是在统计学上对齐特征来迁移图像的纹理和颜色特征）对于字体这种变换局部的特征模式的任务是不合适的。</p>
<p>​ 同时，对于image-to-image的生成任务而言，一系列的<strong>无监督</strong>的方法，都是使用对抗<strong>训练结合Consistent Contrains</strong>来进行的。如果使用image-to-image的方法直接应用到字体生成任务中的话，即使Consistent Constraints会帮助我们<strong>保留一个字符图片内容的结构</strong>，但是他们仍然会导致<strong>诸如模糊、丢失笔画</strong>等问题。</p>
<h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2>
<h4 id="image-to-image-translation">1) Image-To-Image Translation</h4>
<p>​ <strong>image-to-image迁移的任务，就是学习一个从source domain到target domain的映射关系。其可以用于艺术风格迁移、语义分割、图像动画等等。</strong></p>
<ul>
<li>Pix2Pix是基于Conditional GAN的第一个做Image-to-Image的迁移任务。</li>
<li>Cycle-GAN通过Cycle Consistency 做到了无监督学习。</li>
</ul>
<p>​ <strong>这类工作无法直接应用至Font-Generation中，原因在Part2的领域背景最后已经做了简略的描述</strong></p>
<h4 id="font-generation">2) Font-Generation</h4>
<p>​ <strong>字体生成目标是自动的能够生成某种特定字体的字符，并且创造一个字体字符集。</strong></p>
<p>​ <strong>一般而言，从前的方法有两大条路径：</strong></p>
<ul>
<li><strong>基于paired data进行训练</strong>
<ul>
<li>EMD和SAVAE设计的神经网络，分割开了字体的内容和风格（content &amp; style）的表示，可以生成新的风格的字符内容。</li>
<li>zi2zi和rewrite这两篇论文，通过上千对匹配的字符，基于GAN进行了有监督学习。其之后，很多文章基于zi2zi进行了生成质量的改进。</li>
</ul></li>
<li><strong>基于辅助标识（例如笔画、部首等内容）</strong>
<ul>
<li><strong>这类方法往往都依赖于先验知识，并且只能应用到特定的书写系统中去。 并且这些方法仍旧需要数千匹配的数据以及辅助注释</strong></li>
<li>《Scfont: Structure-guided chinese font generation via deep stacked networks.》给每个笔画打上标签，来通过书写轨迹的合成，生成字的图像</li>
<li>DM-FONT 提出解纠缠策略来解纠缠复杂的字形结构，这有助于在富文本设计中捕获局部细节</li>
<li>CalliGAN 进一步将字符分解为组件，并提供包括笔画顺序在内的低级结构信息来指导生成过程。</li>
<li>《RD-GAN: few/zero-shot chinese character styletransfer via radical decomposition and rendering》 使用对字符偏旁部首的分解，来达到字体的生成</li>
<li>其他一些的方法也通过提取字符的骨架和笔画的算法来进行生成</li>
</ul></li>
</ul>
<p>​ <strong>相比之下，该篇论文提出的DG-FONT不需要其他的辅助标识，并且是无监督的形式进行的</strong></p>
<h4 id="deformable-convolution">3) Deformable Convolution</h4>
<p>​ <strong>介绍链接：https://blog.slks.xyz/2022/01/07/basic5/</strong></p>
<p>​ <strong>可变形卷积Deformable Convolution</strong>，<strong>其加强了CNN的变换建模能力</strong>，它通过额外的偏移量增加了模块中的空间采样位置。 可变形卷积已被应用于解决几个高级视觉任务，例如对象检测、视频对象检测采样、语义分割和人体姿态估计。</p>
<p>​ 最近，一些方法尝试在图像生成任务中应用可变形卷积。 TDAN[48] 通过使用可变形卷积对齐两个连续帧并输出高分辨率帧来解决视频超分辨率任务。</p>
<p>​ <strong>在我们提出的 DG-Font 中，可变形卷积的偏移量是通过 learned latent style code来进行估计的。</strong>（具体内容见后细节）</p>
<h2 id="主要设计思想">4、主要设计思想：</h2>
<p>​ 作者提出了可变形生成网络（Deformable Generative Networks）来做<strong>非监督的字体生成</strong>。其利用提供的目标字体图像（<strong>style image input</strong>）来将一种字体的字符变形和转换为另一种字体。</p>
<p>​ DG-FONT 分离了字体的style和content，然后再将两个domain的表示进行融合，生成新的字体的字符。</p>
<p>​ 其核心模块为一个叫做<strong>FDSC（feature deformation skip connection）的东西</strong>，可以用来预测一个位移映射map然后使用位移映射map去对low-level的特征图做变形卷积。然后FDSC的输出被送入一个混合器，然后生成最终的结果。</p>
<p>​ FDSC模块将会<strong>对内容图像的低层级特征进行变换，其能够保留文字本身的模式</strong>，比如笔画和偏旁部首信息。因为对于内容相同的两种不同风格的字体，<strong>它们的每一笔画通常都是对应的</strong>（如下图所示）。利用字体的空间关系，利用FDSC进行空间变形，<strong>有效地保证了生成的字符具有完整的结构</strong>。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116172624229.png" /></p>
<p>​</p>
<p>​ 同时，为了区分不同的风格，模型还使用了一个多任务标识符判别器Multi-Task Discriminator，以保证每个风格都可以被独立判定。</p>
<h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116160758876.png" /></p>
<h3 id="整体架构overall-pipeline">1) 整体架构（Overall Pipeline）</h3>
<ul>
<li><p><strong>概述</strong>：如上图所示，整个网络包含一个 Style Encoder、一个Content Encoder、一个Mixer、和两个FDSC模块。</p>
<ul>
<li><p><strong>Style Encoder</strong> 从输入图像中学习Style Representation。具体而言，其将一个Style Image作为输入，将其映射至一个Style Latent Vector <span class="math inline">\(Z_S\)</span>。</p></li>
<li><p><strong>Content Encoder</strong> 提取Content Images的结构特征，将其 映射到一个空间特征图 <span class="math inline">\(Z_C\)</span></p></li>
<li><p><strong>Deformable Convolution</strong>能够使得<em>Content Encoder</em>去识别到相同内容的字中Style-Invariant的特征</p></li>
<li><p><strong>Mixer</strong> 通过混合<span class="math inline">\(Z_C\)</span>和<span class="math inline">\(Z_S\)</span>来生成输出字符。其使用<strong>AdaIN</strong>方法将style特征注入Mixer中。</p></li>
<li><p><strong>FDSC</strong> 模块能够将变形的<em>Low-Level</em>特征从<em>Content Encoder</em>中传输到Mixer中</p></li>
<li><p><strong>Multi-Task Discriminator</strong> ：当字符图像从生成网络生成后，该判别器用来对每个单独的 Style 同时执行判断任务。对于每一个style来说， Discriminator的输出是一个二元分类器， 判断其是真实图片还是生成图片。同时，因为在训练集中，有多种不同的字体风格，所以判别器的输出是一个数组，它的长度是所有风格的数量，数组里的每个元素是一个二元向量【例如，假设总共有5个风格，最终判别器输出的应当是如下的一个向量：</p>
<p>​ [ [ 1 0 ] , [ 0 0 ] , [ 0 0 ] , [ 0 0 ] , [ 0 0 ] ]</p>
<p>】</p></li>
</ul></li>
<li><p><strong>输入</strong>：<strong>Style Image</strong>【风格A，汉字1】、<strong>Content Image</strong>【风格B，汉字2】</p></li>
<li><p><strong>输出</strong>：<strong>Output Image</strong>【风格A，汉字2】</p></li>
</ul>
<h3 id="style-encoder">2) Style Encoder</h3>
<ul>
<li><strong>概述</strong>：从输入图像中学习 Style Representation。具体而言，其将一个Style Image作为输入，将其映射至一个Style Latent Vector <span class="math inline">\(Z_S\)</span>。</li>
<li><strong>输入</strong>：Style Image <span class="math inline">\(I_s \in R^{H*W}\)</span></li>
<li><strong>输出</strong>：Style Latent Vector <span class="math inline">\(Z_s \in R\)</span></li>
<li><strong>网络结构</strong>：如下所示
<ul>
<li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220119151744473.png" /></li>
</ul></li>
<li><strong>公式表达</strong>：</li>
<li><strong>实现细节（官方代码）</strong>
<ul>
<li>其在源代码中，其为GuidingNet文件部分，可供选择的项有Vgg11，Vgg13，Vgg16，Vgg19等。这个StyleEncoder网络将输入的StyleImage进行特征的提取以后，得到特征向量，在Decoder中使用AdaIn融合之前，源代码中还经过了一个MLP模块。然后，利用MLP模块输出的内容，对AdaIN2d里面的weight和bias进行初始化，最终达到</li>
</ul></li>
</ul>
<h3 id="content-encoder">3) Content Encoder</h3>
<ul>
<li><p><strong>概述</strong>：提取Content Images的结构特征，将其 映射到一个空间特征图 <span class="math inline">\(Z_C\)</span></p></li>
<li><p><strong>输入</strong>：Content Image <span class="math inline">\(I_c \in R^{H*W}\)</span></p></li>
<li><p><strong>输出</strong>：Content Latent Vector <span class="math inline">\(Z_c \in R\)</span></p></li>
<li><p><strong>网络结构细节</strong>：</p>
<ul>
<li><p>in_channel = 3,out_channel=64,kernel_size=7 x 7,stride = 1,padding = 3 的变形卷积层 + IN(64) 归一化 + Activation(ReLu)</p></li>
<li><p>得到FDSC-1模块的输入 skip1 [32, 64, 80, 80]</p></li>
<li><p>in_channel = 64,out_channel=128,kernel_size =4 x 4,stride=2,padding=1 的变形卷积层 + IN(128) 归一化 + Activation(ReLu)</p></li>
<li><p>得到FDSC-2模块的输入 skip2 [30,128,40,40]</p></li>
<li><p>in_channel =128,out_channel=256,kernel_size =4 x 4,stride=2,padding=1 的变形卷积层 + IN(256) 归一化 + Activation(ReLu)</p></li>
<li><p>N个ResBlock</p>
<ul>
<li><p>每个ResBlock如下：</p></li>
<li><p>Input---&gt;Conv2D---&gt;Conv2D---&gt; + ---&gt; output</p>
<p><span class="math inline">\(\downarrow\)</span>-------------------------------------------<span class="math inline">\(\uparrow\)</span></p></li>
<li><p>Conv2D,in_dim=256,out_dim=256,kernel_size=3x3,stride=1,padding=1 普通卷积层</p></li>
</ul></li>
</ul></li>
<li><p><strong>举例</strong>：</p>
<ul>
<li>假设输入图像为 80 * 80 ，Batch_Size = 32 , 那么：skip1,skip2和最终的output <span class="math inline">\(Z_c\)</span>的大小分别如下：</li>
<li>skip1.shape: torch.Size([32, 64, 80, 80])</li>
<li>skip2.shape: torch.Size([32, 128, 40, 40])</li>
<li>ouput.shape: torch.Size([32, 256, 20, 20])</li>
</ul></li>
<li><p><strong>意义</strong>：提取Content Images的结构特征，应用变形卷积层能够保持字体笔画结构</p></li>
</ul>
<h3 id="feature-deformation-skip-connection-fdsc-module">4) Feature Deformation Skip Connection ( FDSC Module)</h3>
<ul>
<li><p><strong>概述</strong>：其由一个变形卷积层组成，具体的作用作者在书写时写在了Mixer的卷积层后的内容里</p></li>
<li><p><strong>实现细节</strong></p>
<ul>
<li><p>首先，其输入来自于Content Encoder，也就是文中提及的skip1和skip2，我们以skip1为例继续讲解：</p></li>
<li><p>其次，它会将Skip1和Mixer中经过了Conv以后的内容A，Concat一起，然后将这个Concat完的东西放入变形卷积模块中，得到一个新计算的Concat_Pre,最后将这个Concat_Pre和A再Concat到一起，得到最终的输出。</p></li>
<li><p>这就是为什么在论文的示意图中：这个FDSC模块有来回的箭头表示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_19_15_31_59_728.png" /></p></li>
</ul></li>
</ul>
<h3 id="mixer">5) Mixer</h3>
<ul>
<li><p><strong>概述</strong>：通过混合<span class="math inline">\(Z_C\)</span>和<span class="math inline">\(Z_S\)</span>来生成输出字符。其使用<strong>AdaIN</strong>方法将style特征注入Mixer中。</p></li>
<li><p><strong>输入</strong>： <span class="math inline">\(Z_c \in R\)</span>即ContentEncoder的输出，256通道的特征图 [32,256,20,20]</p></li>
<li><p><strong>输出</strong>：</p></li>
<li><p><strong>网络结构及细节</strong>：</p>
<ul>
<li>2个ResBlock，输出仍然为256通道的特征图 [32,256,20,20]</li>
<li>Upsample上采样 ，特征图大小变为 40 x 40</li>
<li>Conv2dBlock，in_channel = 256,out_channel=128,kernel_size =5 x 5,stride=1,padding=2 的卷积层 + AdIN(128) 归一化 + Activation(ReLu), 经过该层后，计算得到的大小应当为 [32,128,40,40]
<ul>
<li>然后，需要将此层输出的output和skip2在Channel通道Concat起来，得到deformable_concat [32,256,40,40]</li>
<li>然后将deformable_concat与skip2 输入 FDSC的变形卷积模块中，得到 concat_pre [32,128,40,40] 和 offset2 [32,18,40,40]</li>
<li>最后将cancat_pre和最开始的output Concat起来，得到该步的最终输出，大小为 [32，256，40，40]</li>
</ul></li>
<li>Upsample上采样，特征图大小变为80 x 80</li>
<li>Conv2dBlock，in_channel = 256,out_channel=64,kernel_size =5 x 5,stride=1,padding=2 的卷积层 + AdIN(64) 归一化 + Activation(ReLu), 经过该层后，计算得到的大小应当为 [32,64,80,80]
<ul>
<li>然后，需要将此层输出的output和skip1在Channel通道Concat起来，得到deformable_concat [32,128,80,80]</li>
<li>然后将deformable_concat与skip1 输入 变形卷积模块中，得到 Concat_pre [32,128,40,40] 和 offset2 [32,18,40,40]</li>
<li>最后将cancat_pre和最开始的outputConcat起来，得到该步的最终输出，大小为 [32，256，40，40]</li>
</ul></li>
<li>Conv2dBlock，in_channel = 128,out_channel=3,kernel_size =7 x 7,stride=1,padding=3 的卷积层 + Activation(Tanh), 经过该层后，计算得到的大小应当为 [32,3,80,80]</li>
</ul></li>
</ul>
<h3 id="multi-task-discriminator">6) Multi-Task Discriminator</h3>
<ul>
<li><p><strong>概述</strong>：当字符图像从生成网络生成后，该判别器用来对每个单独的 Style 同时执行判断任务。对于每一个style来说， Discriminator的输出是一个二元分类器， 判断其是真实图片还是生成图片。</p></li>
<li><p><strong>输入</strong>：</p>
<ul>
<li>- x: images of shape (batch, 3, image_size, image_size).，例如为 4个 3 * 64 * 64的图像</li>
<li>- y: domain indices of shape (batch). 例如 y_in 为 4个需要其判断的 domain的标号</li>
</ul></li>
<li><p><strong>输出</strong>：各个需要判断的得分情况</p></li>
<li><p><strong>示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">D = Discriminator(<span class="number">64</span>, <span class="number">4</span>)  <span class="comment"># 初始化判别器参数 64 为图像大小，4为该判别器需要判别区分的domain的数量</span></span><br><span class="line">x_in = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>)  <span class="comment"># x_in 为 2个 3*64*64的图像  y_in 为 2个需要其判断的 domain的标号</span></span><br><span class="line">y_in = torch.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">2</span>, )) <span class="comment"># 假设为[1,3] 就是要让D去判断，第一个图是否属于domain1，第二个图是否属于domain3……</span></span><br><span class="line">out, feat = D(x_in, y_in)</span><br><span class="line"><span class="built_in">print</span>(out, feat)    </span><br><span class="line"><span class="built_in">print</span>(out.shape, feat.shape)  <span class="comment"># out为Discriminator打的分数（内部简化过）,feat为没处理过的原始输出，差别见下示例</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">out。shape = 1 * 2 ,out的第一个值相当于取了feat中[1,1,1,1]因为是第一个图片，并且是判断是否属于domain1的值，即为1.7796</span></span><br><span class="line"><span class="string">out的第二个值相当于取了feat中[2,3,1,1]因为是第二个图片，并且是判断是否属于domain3的值，即为0.9986</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">out: [1.7796, 0.9986]    </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">feat。shape = 2 * 4 * 1 * 1 ，其有2张需要判断的图像，第一张图像有4个值，分别是根据domain0-domain3打的分数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">feat:tensor([[[[-0.8181]],  </span></span><br><span class="line"><span class="string">         [[ 1.7796]],</span></span><br><span class="line"><span class="string">         [[ 3.0122]],</span></span><br><span class="line"><span class="string">         [[ 0.9614]]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[[-0.5011]],</span></span><br><span class="line"><span class="string">         [[ 0.5076]],</span></span><br><span class="line"><span class="string">         [[ 3.4774]],</span></span><br><span class="line"><span class="string">         [[ 0.9986]]]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>网络细节（官方代码）</strong></p>
<ul>
<li>与论文附录一致：</li>
<li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220119155654674.png" /></li>
</ul></li>
</ul>
<h2 id="dg-font网络的训练验证以及主函数见下一篇dg-font代码详解">6、DG-Font网络的训练、验证以及主函数（见下一篇DG-Font代码详解）：</h2>
<p><strong>注：此部分结合了官方源码的内容，讲述训练的整体过程</strong></p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>Font Generation</tag>
        <tag>Deformable Convolution Skip Connection</tag>
        <tag>Unsupervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch学习笔记10——PyTorch 中，nn与nn.functional有什么区别？（搬运）</title>
    <url>/2022/01/18/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010%E2%80%94%E2%80%94nn.Functional%E5%92%8Cnn%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h1 id="pytorch-中nn-与-nn.functional-有什么区别">PyTorch 中，nn 与 nn.functional 有什么区别？</h1>
<p>​ 注：在阅读代码以及Pytorch文档的时候发现，nn和nn.functional有很多相同的函数，文档中也有许多引用，故而搜索了一下有何区别，该文为搬运文，以下为引用说明，避免产生误会。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">作者：肥波喇齐</span><br><span class="line">链接：https://www.zhihu.com/question/66782101/answer/579393790</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br></pre></td></tr></table></figure>
<h4 id="两者的相同之处">1、两者的相同之处：</h4>
<ul>
<li><code>nn.Xxx</code>和<code>nn.functional.xxx</code>的实际功能是相同的，即<code>nn.Conv2d</code>和<code>nn.functional.conv2d</code> 都是进行卷积，<code>nn.Dropout</code> 和<code>nn.functional.dropout</code>都是进行dropout,………………；</li>
<li>运行效率也是近乎相同。</li>
</ul>
<p>​ <code>nn.functional.xxx</code>是函数接口，而<code>nn.Xxx</code>是<code>nn.functional.xxx</code>的类封装，并且<strong><code>nn.Xxx</code>都继承于一个共同祖先<code>nn.Module</code>。</strong>这一点导致<code>nn.Xxx</code>除了具有<code>nn.functional.xxx</code>功能之外，内部附带了<code>nn.Module</code>相关的属性和方法，例如<code>train(), eval(),load_state_dict, state_dict</code>等。</p>
<h4 id="两者的差别之处">2、两者的差别之处：</h4>
<ul>
<li><strong>两者的调用方式不同。</strong></li>
</ul>
<p><code>nn.Xxx</code> 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = torch.rand(<span class="number">64</span>, <span class="number">3</span>, <span class="number">244</span>, <span class="number">244</span>)</span><br><span class="line">conv = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">out = conv(inputs)</span><br></pre></td></tr></table></figure>
<p><code>nn.functional.xxx</code>同时传入输入数据和weight, bias等其他参数 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight = torch.rand(<span class="number">64</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">bias = torch.rand(<span class="number">64</span>) </span><br><span class="line">out = nn.functional.conv2d(inputs, weight, bias, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong><code>nn.Xxx</code>继承于<code>nn.Module</code>， 能够很好的与<code>nn.Sequential</code>结合使用， 而<code>nn.functional.xxx</code>无法与<code>nn.Sequential</code>结合使用。</strong></p></li>
<li><p><strong><code>nn.Xxx</code>不需要你自己定义和管理weight；而<code>nn.functional.xxx</code>需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。</strong></p>
<p>例如：使用<code>nn.Xxx</code>定义一个CNN</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.cnn1 = nn.Conv2d(in_channels=<span class="number">1</span>,  out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>,padding=<span class="number">0</span>)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        self.cnn2 = nn.Conv2d(in_channels=<span class="number">16</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>,  padding=<span class="number">0</span>)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">4</span> * <span class="number">4</span> * <span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.maxpool1(self.relu1(self.cnn1(x)))</span><br><span class="line">        out = self.maxpool2(self.relu2(self.cnn2(out)))</span><br><span class="line">        out = self.linear1(out.view(x.size(<span class="number">0</span>), -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>使用<code>nn.function.xxx</code>定义一个与上面相同的CNN。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.cnn1_weight = nn.Parameter(torch.rand(<span class="number">16</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        self.bias1_weight = nn.Parameter(torch.rand(<span class="number">16</span>))</span><br><span class="line">        </span><br><span class="line">        self.cnn2_weight = nn.Parameter(torch.rand(<span class="number">32</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        self.bias2_weight = nn.Parameter(torch.rand(<span class="number">32</span>))</span><br><span class="line">        </span><br><span class="line">        self.linear1_weight = nn.Parameter(torch.rand(<span class="number">4</span> * <span class="number">4</span> * <span class="number">32</span>, <span class="number">10</span>))</span><br><span class="line">        self.bias3_weight = nn.Parameter(torch.rand(<span class="number">10</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = F.conv2d(x, self.cnn1_weight, self.bias1_weight)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = F.max_pool2d(out)</span><br><span class="line">        </span><br><span class="line">        out = F.conv2d(x, self.cnn2_weight, self.bias2_weight)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = F.max_pool2d(out)</span><br><span class="line">        </span><br><span class="line">        out = F.linear(x, self.linear1_weight, self.bias3_weight)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></li>
</ul>
<p>​ 上面两种定义方式得到CNN功能都是相同的，但PyTorch官方推荐：具有学习参数的（例如，conv2d, linear, batch_norm)采用<code>nn.Xxx</code>方式，没有学习参数的（例如，maxpool, loss func, activation func）等根据个人选择使用<code>nn.functional.xxx</code>或者<code>nn.Xxx</code>方式。</p>
<p>​ 但关于<strong>dropout</strong>，个人强烈推荐使用<code>nn.Xxx</code>方式，因为一般情况下只有训练阶段才进行dropout，在eval阶段都不会进行dropout。使用<code>nn.Xxx</code>方式定义dropout，在调用<code>model.eval()</code>之后，model中所有的dropout layer都关闭，但以<code>nn.function.dropout</code>方式定义dropout，在调用<code>model.eval</code>之后并不能关闭dropout。</p>
]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记9——归一化方法FRN(Filter Response Normalization)</title>
    <url>/2022/01/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B09%E2%80%94FRN/</url>
    <content><![CDATA[<h4 id="一简介">一、简介</h4>
<p>​ <strong>在先前的文章中，链接：https://blog.slks.xyz/2022/01/16/basic7/，讲解了BN、LN、IN、CIN、GN等多种不同的Normalization，由于在DG-Font的阅读过程中，又发现了一个新的归一化方法叫做FRN，故而在此篇略做记录。</strong></p>
<p>​ <strong>FRN</strong>是谷歌提出的一种新的归一化方法，和GN一样不依赖batch，故而FRN层不仅消除了模型训练过程中对batch的依赖，而且当batch size较大时性能优于BN。</p>
<p>​ <strong>原论文名称：</strong>《Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks》</p>
<p>​ <strong>原论文地址：</strong>https://arxiv.org/abs/1911.09737</p>
<h4 id="二结构">二、结构：</h4>
<p>​ 如下所示，FRN层整体结构包括归一化层FRN（Filter Response Normalization）和激活层TLU（Thresholded Linear Unit）。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220118102128474.png" /></p>
<p>​ 整个结构并不复杂，就是通过一个公式计算，最后经过一个阈值限制输出</p>
<p>​ 其中FRN的操作是对每个样例的每个channel单独进行归一化的，这里<span class="math inline">\(x\)</span>就是一个N（=HxW）维度的向量，所以FRN没有BN层对batch依赖的问题。BN层采用归一化方法是减去均值然后除以标准差，而FRN却不同，这里没有减去均值操作，公式中的<span class="math inline">\(v^2\)</span>是<span class="math inline">\({x}\)</span>的二次范数的平均值。这种归一化方式类似BN可以用来消除中间操作（卷积和非线性激活）带来的尺度问题，有助于模型训练。 公式里的<span class="math inline">\({\epsilon}\)</span>是一个很小的正常量，一般为<span class="math inline">\(1^{-6}\)</span>以防止除0。</p>
<p>​ 一般情况下网络的特征图大小N(=HxW)较大，但是有时候可能会出现1x1的特征图的情况，比如InceptionV3和VGG网络，此时就<span class="math inline">\({\epsilon}\)</span>比较关键，</p>
<p>​ 归一化之后同样需要进行缩放和平移变换，这里的<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>也是可学习的参数（参数为长度是C的向量, 即为特征数目，也就是通道数):</p>
<p>​ <span class="math display">\[{y = \gamma \hat{x} + \beta}\]</span></p>
<p>​ FRN缺少去均值的操作，这可能使得归一化的结果任意地偏移0，如果FRN之后是ReLU激活层，可能产生很多0值，这对于模型训练和性能是不利的。为了解决这个问题，FRN之后采用的阈值化的ReLU，即TLU：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_18_10_32_35_986.png" /></p>
<p>​ 这里的<span class="math inline">\(\tau\)</span>是一个可学习的参数。原论文中发现FRN之后采用TLU对于提升性能是至关重要的。</p>
<h4 id="三代码实现coding">三、代码实现Coding</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FRN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_features, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FRN, self).__init__()</span><br><span class="line">        self.tau = nn.Parameter(torch.zeros(<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment">## input x  shape [B,C,H,W] (batch_size , channel , height , width)</span></span><br><span class="line">        x1 = torch.mean(x**<span class="number">2</span>, dim=[<span class="number">2</span>, <span class="number">3</span>], keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">## x1       shape [B,C,1,1] (batch_size , channel , 1 , 1)</span></span><br><span class="line">        x2 = x1 + self.eps</span><br><span class="line">        <span class="comment">## x2 	    shape [B,C,1,1] (batch_size , channel , 1 , 1),  【+ 为逐元素相加，不改变维度】</span></span><br><span class="line">        x3 = torch.rsqrt(x2)</span><br><span class="line">        <span class="comment">## x3       shape [B,C,1,1] (batch_size , channel , 1 , 1),  【sqrt 为逐元素操作，不改变为维度】</span></span><br><span class="line">        x4 = x * x3</span><br><span class="line">        <span class="comment">## x4       shape [B,C,H,W] (batch_size , channel , height , width)  </span></span><br><span class="line">        <span class="comment">##          广播机制,逐元素相乘  [B,C,H,W] * [B,C,1,1] = [B,C,H,W]</span></span><br><span class="line">        output = torch.<span class="built_in">max</span>(self.gamma * x4 + self.beta, self.tau)</span><br><span class="line">        <span class="comment">## output   shape [B,C,H,W] (batch_size , channel , height , width)  torch.max 逐元素操作</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment">## 相关注解：</span></span><br><span class="line"><span class="comment">## 输入的 x.shape = [B,C,H,W] ( Batch_Size \ Channel \ Height \ Width )</span></span><br><span class="line"><span class="comment">## torch.rsqrt()  对每个元素取平方根后再取倒数，并不会影响如维度等因素。</span></span><br><span class="line"><span class="comment">## torch.mean() 其中dim=[2,3] 代表按第2、3维求平均值, 即在单个实例、单个Channel上求平均</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>Torch/Numpy的广播机制介绍</title>
    <url>/2022/01/18/Python%E7%AC%94%E8%AE%B0/python_import_module/</url>
    <content><![CDATA[<p>Numpy以及Tensor的广播机制介绍（以Torch为例，两者一致）</p>
<h3 id="广播机制在何处会出现">1、广播机制在何处会出现？</h3>
<p>​ 广播针对的运算是element wise类型的运算，即元素对元素类型的运算</p>
<p>​ <strong>Element-wise的计算符号包括如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">算数运算：+, -, *, /, //, %, divmod(), ** or pow(), &lt;&lt;, &gt;&gt;, &amp;, ^, |</span><br><span class="line"></span><br><span class="line">比较运算：==, &lt;, &gt;, &lt;=, &gt;=, !=</span><br></pre></td></tr></table></figure>
<h3 id="广播机制的规则与出现的原因">2、广播机制的规则与出现的原因</h3>
<p>​ 正常来说，两个做Element wise类型运算的变量，其相应维度的长度要相等，如下所示：这种形式做Element-Wise的运算是非常简单且易理解的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">b = torch.rand(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">c = a * b</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">a:tensor([[0.9912, 0.3736],</span></span><br><span class="line"><span class="string">        [0.0708, 0.6939]])</span></span><br><span class="line"><span class="string">b:tensor([[0.5788, 0.6296],</span></span><br><span class="line"><span class="string">        [0.9746, 0.8540]])</span></span><br><span class="line"><span class="string">c:tensor([[0.5737, 0.2352],</span></span><br><span class="line"><span class="string">        [0.0690, 0.5925]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>​ 那么当两个Tensor的对应维度不对齐的时候，<strong>为了避免用户使用代码for循环来操作填补数组，导致一些低效率的行为出现</strong>，所以其提供了一种广播机制，其实质就是一种处理规则，在不对齐的维度上，长度较短的自动做值复制来扩充长度，从而使得两个Tensor在该维度上一致。</p>
<h3 id="广播机制起效的情况">3、广播机制起效的情况</h3>
<p>​ 但是，实际上，并不是所有的不同维度的Tensor相乘时，都会触发广播机制。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">b = torch.rand(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>​ 那么，广播机制在何时才会起效呢？其只对如下情况起作用：</p>
<p>​ <strong>两个Tensor，它们如果在某一个维度上长度不同的话，必定有一个Tensor在这个维度上的长度为1，广播机制才会起效。即 1 vs M 的情况</strong></p>
<p>​ <strong>具体计算规则：</strong>长度为1的Tensor在维度上会复制该元素并扩充至长度为M，当这个维度完成对齐，接着重复检查上一层维度，如此反复，直至所有维度都检查完。</p>
<p>​ <strong>示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.rand(<span class="number">1</span>,<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.rand(<span class="number">3</span>,<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">c = a * b</span><br><span class="line"><span class="built_in">print</span>(c.shape)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	torch.Size([3, 5, 3])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>​ <strong>示意图</strong>：</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/20200308204131296.png" alt="20200308204131296" /><figcaption aria-hidden="true">20200308204131296</figcaption>
</figure>
]]></content>
      <categories>
        <category>python基础扩充笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>Auto BroadCasting</tag>
      </tags>
  </entry>
  <entry>
    <title>Python导入自定义模块（同目录、子目录、跨目录）</title>
    <url>/2022/01/18/Python%E7%AC%94%E8%AE%B0/python_broadcast_module/</url>
    <content><![CDATA[<h3 id="基本格式">1、基本格式：</h3>
<p>​ <strong>from 文件名 import 类名</strong></p>
<h3 id="分情形讨论引入方式">2、分情形讨论引入方式：</h3>
<p>​ 假设现有如下目录结构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── model0.py</span><br><span class="line">├── main.py</span><br><span class="line">├── model1/</span><br><span class="line">│   └── model1_main.py</span><br><span class="line">└── model2/</span><br><span class="line">    └── model2_main.py</span><br></pre></td></tr></table></figure>
<h4 id="同级目录引入">1）同级目录引入</h4>
<p><strong>main.py</strong> 中需要导入 <strong>model0.py</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> model0</span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line"><span class="keyword">from</span> model0 <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<p>两者都是可以的，同级目录下引入十分简便，直接import即可</p>
<h4 id="子目录引入">2）子目录引入</h4>
<p><strong>main.py</strong> 中需要导入 <strong>model1_main.py</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">我们首先需要在model1/下建立__init__.py空文件，让编译器认为这是一个模块。</span><br></pre></td></tr></table></figure>
<p>建立后，<strong>目录结构应当如下所示：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── model0.py</span><br><span class="line">├── main.py</span><br><span class="line">├── model1/</span><br><span class="line">│   └── model1_main.py</span><br><span class="line">│   └── __init__.py</span><br><span class="line">└── model2/</span><br><span class="line">    └── model2_main.py</span><br></pre></td></tr></table></figure>
<p>然后进行引入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> model1.model1_main</span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line"><span class="keyword">from</span> model1.model1_main <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<h4 id="跨目录引入">3）跨目录引入</h4>
<p><strong>model1_main.py</strong>导入<strong>model2/model2_main.py</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">同理我们首先需要在model2/下建立__init__.py空文件，让编译器认为这是一个模块。</span><br></pre></td></tr></table></figure>
<p>建立后，<strong>目录结构应当如下所示：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── model0.py</span><br><span class="line">├── main.py</span><br><span class="line">├── model1/</span><br><span class="line">│   └── model1_main.py</span><br><span class="line">│   └── __init__.py</span><br><span class="line">└── model2/</span><br><span class="line">│   └── model2_main.py</span><br><span class="line">│   └── __init__.py</span><br></pre></td></tr></table></figure>
<p>然后在model1_main文件中进行引入</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> model2.model2_main</span><br></pre></td></tr></table></figure>
<h4 id="更为一般的介绍">4）更为一般的介绍：</h4>
<p>sys模块是python内置的，我们导入跨自定义模块的步骤一般如下：</p>
<p>​ 首先要确保被导入的模块文件夹内有 __init__.py文件，确保其被识别为一个模块，然后再执行下面步骤：</p>
<ol type="1">
<li><strong>先导入sys模块</strong></li>
<li>然后通过<code>sys.path.append(path)</code> 函数来导入自定义模块所在的目录</li>
<li><strong>导入自定义模块</strong>。</li>
</ol>
<p>​ 只不过在同级目录以及子目录下进行引入，不需要这么复杂，可以有更简单的方法，而在跨目录引入时，就需要采用这种方案。</p>
]]></content>
      <categories>
        <category>python基础扩充笔记</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Module Import</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记8——图像风格迁移方法AdaIN</title>
    <url>/2022/01/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B08%E2%80%94%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E7%AE%97%E6%B3%95%20AdaIN%20/</url>
    <content><![CDATA[<p><strong>AdaIN</strong>是一种经典的图片风格迁移算法，在 2017 年ICCV中提出。主要用于将一张图片(风格图) 中的风格、纹理迁移到另一张图片 (内容图)，同时要保留内容图的主体结构。如下所示：</p>
<p>​ <strong>论文名称：《Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization》</strong></p>
<p>​ <strong>论文链接：https://arxiv.org/abs/1703.06868</strong></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/b219ebc4b74543a95e9180d3cb15748ab9011436.jpeg" /></p>
<h2 id="一adain简介">一、AdaIN简介</h2>
<p>​ 这篇论文的主要目标是实现<strong>实时的、任意风格的风格迁移（style transfer）</strong>，核心方法就是其提出的自适应实例标准化（<strong>Adaptive Instance Normalization，AdaIN</strong>），通过<strong>将内容图像（content image）特征的均值和方差对齐到风格图像（style image）的均值和方差</strong>来实现风格迁移。</p>
<p>​ 此外，这个方法还给用户非常多的控制权，包括如下：</p>
<ul>
<li>内容和风格的折中（trade off）
<ul>
<li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116183935766.png" /></li>
</ul></li>
<li>风格插值（混合风格迁移）
<ul>
<li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116183943127.png" /></li>
</ul></li>
<li>是否保留颜色
<ul>
<li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116183918123.png" /></li>
</ul></li>
<li>对图像的特定区域进行风格迁移
<ul>
<li><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116183906461.png" /></li>
</ul></li>
</ul>
<h2 id="二前备知识">二、前备知识：</h2>
<p>​ 你需要熟悉Batch Normaliztion（BN）、Layer Norm（LN）、Instance Norm（IN）、Group Norm（GN）、Conditional Instance Norm（CIN）等概念。</p>
<p>​ 下图为特征图张量，可以直观看出BN，LN，IN，GN等规范化方法的区别。N为样本维度，C为通道维度，H为height，W即width，代表特征图的尺寸。</p>
<p><img src="https://img-blog.csdnimg.cn/2019061216413530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNTU3Njg4MQ==,size_16,color_FFFFFF,t_70" /></p>
<p>​ 具体每种Normalization的方法介绍可以参见我的这一篇Blog：</p>
<p>​ https://blog.slks.xyz/2022/01/16/basic7/</p>
<p>​</p>
<h2 id="三adain具体介绍">三、AdaIN具体介绍</h2>
<h5 id="输入content-input-x-style-input-y">输入：Content Input x &amp;&amp; Style Input y</h5>
<p>​ AdaIN 简单地将 x 的通道均值和方差对齐以匹配 y 的均值和方差。 不像BN、IN或CIN，AdaIN没有需要从网络中进行学习的仿射变换参数，它能够自适应的从style input中计算得到仿射变换的参数。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117103448955.png" /></p>
<p>​ 我们简单地用 σ(y) 缩放归一化的Content Input，并用 µ(y) 移动它。 与 IN 类似，这些统计数据是跨空间位置计算的。也就是说，其实对于单个实例的单个特征通道计算的均值和方差数据。</p>
<p>​ 直观地说，让我们考虑一个检测特定风格笔触的特征通道。</p>
<p>​ 具有这种风格的图像会对它的特征部分产生较高的平均激活。AdaIN 产生的输出将对该特征具有相同的高平均激活，同时其也保留内容图像的空间结构。 然后我们可以使用前馈解码器将风格特征转换回图像空间。同时，这个特征通道的方差可以编码更细微的风格信息，这些信息也传递到 AdaIN 输出和最终输出图像。</p>
<p>​ 简而言之，AdaIN 通过传输特征统计数据（特别是通道均值和方差）在特征空间中执行风格迁移。我们的 AdaIN 层就像一个 IN 层一样简单，几乎不增加计算成本。</p>
<h2 id="四网络结构">四、网络结构</h2>
<p>​ 在论文中，其使用VGG-19来编码内容和风格，在浅层空间将特征图通过AdaIN层，进行上述仿射变换，解码器根据变换后的特征图试图重建图像，通过反向传播训练解码器，使得解码器输出越来越真实的图像。整体架构如下所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/2019061217185282.png" /></p>
<p>​ 更为具体的代码可见以下链接地址：</p>
<p>​ https://github.com/xunhuang1995/AdaIN-style</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>Style Transfer</tag>
        <tag>AdaIN</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记7——各种归一化Normalization方法（含BN、LN、IN、CIN、GN）</title>
    <url>/2022/01/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B07%E2%80%94BN%E3%80%81LN%E3%80%81IN%E3%80%81CIN%E3%80%81GN/</url>
    <content><![CDATA[<p><strong>本文介绍了常见的几种Normalization方式，如BatchNorm、LayerNorm、InstanceNorm、Conditional Instance Norm、Group Norm。 各种Normalization在理论上都能够起到平滑损失函数平面的效果，加速函数的收敛效果，但是它们在机器学习的各个领域上，各有偏重与优势。</strong></p>
<h3 id="目录概述">目录概述：</h3>
<ul>
<li><strong>BatchNorm</strong>：batch方向做归一化，算N * H * W的均值, 常用于CNN等视觉识别领域，如果当Batch的尺寸比较小或是在一些动态网络中时不适用。</li>
<li><strong>LayerNorm</strong>：channel方向做归一化，算C * H * W的均值，LN不适用于CNN等视觉识别领域，但是可在BN无法使用的领域如RNN和Batch Size较小时进行使用。</li>
<li><strong>InstanceNorm</strong>：一个channel一个实例内做归一化，算H * W的均值，其适用于批量较小且单独考虑每个像素点的场景中，如GAN生成网络，但在MLP或RNN或Feature Map较小的时候不适用。</li>
<li><strong>GroupNorm</strong>：将channel方向分group，然后每个group内做归一化，算(C  G) * H * W的均值</li>
</ul>
<h3 id="batch-normbn">1、Batch Norm（BN）</h3>
<p>​ Ioffe 和 Szegedy 的开创性工作引入了批量归一化（BN）层，通过归一化特征统计显示简化了前馈网络的训练。 BN 层最初旨在加速判别网络的训练，但也被发现在生成图像建模中有效。</p>
<h5 id="输入"><strong>输入</strong>：</h5>
<p>​ <strong>An input batch</strong> <span class="math inline">\(x \in R^{N \times C \times H \times W}\)</span></p>
<h5 id="说明"><strong>说明</strong>：</h5>
<p>​ Batch Normalization就是对一个Batch中的数据进行标准化，就是每一个值减去batch的均值，除以batch的标准差，计算公式如下：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116184553370.png" /></p>
<ul>
<li><p><span class="math inline">\(\gamma , \beta \in R^{C}\)</span>是从数据中训练得到的参数。</p></li>
<li><p><span class="math inline">\(\mu(x) , \sigma(x) \in R^{C}\)</span>是均值和方差，为每个特征通道（C）独立计算批量大小（N）和空间维度（H * W）</p></li>
<li><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116185246851.png" /></p></li>
</ul>
<h5 id="卷积网络中的bn"><strong>卷积网络中的BN</strong></h5>
<p>​ BN除了可以应用在MLP上，其在CNN网络中的表现也非常好，<strong>卷积网络和MLP的不同点是卷积网络中每个样本的隐层节点的输出是三维（宽度，高度，维度）的</strong>，而MLP是一维的。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_33_31_622.png" /></p>
<p>​ 在上图中，假设一个批量有 <span class="math inline">\(m\)</span>个样本，Feature Map的尺寸是 <span class="math inline">\({p \times q}\)</span>，通道数是<span class="math inline">\(d\)</span>。<strong>在卷积网络中，BN的操作是以Feature Map为单位的</strong>，因此一个BN要统计的数据个数为 <span class="math inline">\({m \times p \times q}\)</span>，每个Feature Map使用一组<span class="math inline">\(\gamma\)</span>和<span class="math inline">\({\beta}\)</span>.</p>
<h5 id="类比"><strong>类比</strong>：</h5>
<p>​ 如果把输入<span class="math inline">\(x \in R^{N \times C \times H \times W}\)</span>类比为一摞书，这摞书总共有 N 本，每本有 C 页，每页有 H 行，每行 W 个字符。BN 求均值时，相当于把这些书按页码一一对应地加起来（例如第1本书第36页，第2本书第36页......），再除以每个页码下的字符总数：N<strong>×</strong>H<strong>×</strong>W，因此可以把 <strong>BN 看成求“平均书”</strong>的操作（注意这个“平均书”每页只有一个字）。</p>
<h5 id="总结"><strong>总结：</strong></h5>
<p>​ BN是深度学习调参中非常好用的策略之一（另外一个可能就是Dropout），当你的模型发生<strong>梯度消失/爆炸或者损失值震荡比较严重</strong>的时候，在BN中加入网络往往能取得非常好的效果，因为BN能够起到平滑损失平面的作用。</p>
<p>​ BN也有一些不是非常适用的场景，在遇见这些场景时要谨慎的使用BN：</p>
<ul>
<li>受制于硬件限制，每个Batch的尺寸比较小，这时候谨慎使用BN；</li>
<li>在类似于RNN的<strong>动态网络</strong>中谨慎使用BN；</li>
<li>训练数据集和测试数据集方差较大的时候。</li>
</ul>
<h3 id="layer-normln">2、Layer Norm（LN）</h3>
<p>​ Layer Normalization（LN）的提出有效的解决了BN的这两个问题（一个是不适用于动态网络，一个是batch尺寸较小的时候）。LN和BN不同点是归一化的维度是互相垂直的。如下图所示。在图1中<span class="math inline">\(N\)</span>表示样本轴， <span class="math inline">\(C\)</span>表示通道轴， <span class="math inline">\(F\)</span>是每个通道的特征数量( W*H )。BN 如右侧所示，它是取不同样本的同一个通道的特征做归一化；LN则是如左侧所示，它取的是同一个样本的不同通道做归一化。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_33_37_355.png" /></p>
<h5 id="mlp中的ln"><strong>MLP中的LN</strong></h5>
<p>​ BN的两个缺点的产生原因均是因为<strong>计算归一化统计量时计算的样本数太少</strong>。LN是一个独立于batch size的算法，所以无论样本数多少都不会影响参与LN计算的数据量，从而解决BN的两个问题。</p>
<p>​ 先看MLP中的LN。设<span class="math inline">\(H\)</span>是一层中隐层节点的数量， <span class="math inline">\(l\)</span>是MLP的层数，我们可以计算LN的归一化统计量<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_24_28_295.png" /></p>
<p>​ 注意上面<strong>统计量的计算是和样本数量没有关系</strong>的，它的<strong>数量只取决于隐层节点的数量</strong>，所以只要隐层节点的数量足够多，我们就能保证LN的归一化统计量足够具有代表性。</p>
<p>​ 通过<span class="math inline">\(\mu ^{l}\)</span>和<span class="math inline">\(\sigma ^{l}\)</span>可以计算得到归一化后的值：<span class="math inline">\(\hat{a}^l\)</span></p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_26_22_177.png" /></p>
<p>​ BN的文章中介绍过几乎所有的归一化方法都能起到平滑损失平面的作用。<strong>所以从原理上讲，LN能加速收敛速度的。</strong>但我们发现，将LN添加到CNN后，实验结果表明LN破坏了卷积层学习到的特征，使得模型无法收敛，所以在CNN之后使用BN是一个较好的选择。</p>
<h5 id="类比-1"><strong>类比</strong>：</h5>
<p>​ LN 求均值时，相当于把每一本书的所有字加起来，再除以这本书的字符总数：C<strong>×</strong>H<strong>×</strong>W，即求整本书的“平均字”，求标准差时也是同理。</p>
<h5 id="总结-1"><strong>总结</strong></h5>
<p>​ 总体而言，LN是和BN非常近似的一种归一化方法，不同的是<strong>BN取的是不同样本的同一个特征，而LN取的是同一个样本的不同特征</strong>。在BN和LN都能使用的场景中，<strong>BN的效果一般优于LN，原因是基于不同数据，同一特征得到的归一化特征更不容易损失信息。</strong></p>
<p>​ 但是有些场景是不能使用BN的，例如batchsize较小或者在RNN中，这时候可以选择使用LN，LN得到的模型更稳定且起到正则化的作用。RNN能应用到小批量和RNN中是因为LN的归一化统计量的计算是和batchsize没有关系的</p>
<h3 id="instance-normin">3、Instance Norm（IN）</h3>
<p>​ 对于图像风格迁移这类<strong>注重每个像素的任务</strong>来说，每个样本的每个像素点的信息都是非常重要的，于是像Batch Normlization这种每个批量的所有样本都做归一化的算法就不太适用了，因为BN计算归一化统计量时考虑了一个批量中所有图片的内容，从而<strong>造成了每个样本独特细节的丢失</strong>。同理对于LayerNormalization这类需要考虑一个样本所有通道的算法来说可能忽略了不同通道的差异，也不太适用于图像风格迁移这类应用。</p>
<p>​ 所以一篇论文提出了Instance Normalization（IN），一种更适合对单个像素有更高要求的场景的归一化算法（IST，GAN等）。IN的算法非常简单，<strong>计算归一化统计量时考虑单个样本，单个通道的所有元素</strong>。IN（右）和BN（中）以及LN（左）的不同从图1中可以非常明显的看出。<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_33_44_819.png" /></p>
<p>​ IN方法计算公式如下：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117093553291.png" /></p>
<p>​ 不同于BN(x),这边的<span class="math inline">\(\mu(x) , \sigma(x) \in R^{C}\)</span>是均值和方差，是为每个实例的每个特征通道（C）独立计算的。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117093715421.png" /></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117093735940.png" /></p>
<p>​ IN在计算归一化统计量时并没有像BN那样跨样本、单通道，也没有像LN那样单样本、跨通道。它是取的单通道，单样本上的数据进行计算。所以对比BN的公式，它只需要它只需要去掉批量维的求和即可。</p>
<h5 id="类比-2"><strong>类比</strong>：</h5>
<p>​ IN 求均值时，相当于把一页书中所有字加起来，再除以该页的总字数：H<strong>×</strong>W，即求每页书的“平均字”，求标准差时也是同理。</p>
<h5 id="总结-2"><strong>总结</strong>：</h5>
<p>​ IN本身是一个非常简单的算法，<strong>尤其适用于批量较小且单独考虑每个像素点的场景中</strong>，因为其计算归一化统计量时没有混合批量和通道之间的数据，对于这种场景下的应用，我们可以考虑使用IN。</p>
<p>​ 另外需要注意的一点是在图像这类应用中，每个通道上的值是比较大的，因此也能够取得比较合适的归一化统计量。但是有两个场景建议不要使用IN:</p>
<ol type="1">
<li>MLP或者RNN中：因为在MLP或者RNN中，每个通道上只有一个数据，这时会自然不能使用IN；</li>
<li>Feature Map比较小时：因为此时IN的采样数据非常少，得到的归一化统计量将不再具有代表性。</li>
</ol>
<h3 id="group-normgn">4、Group Norm（GN）</h3>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/n6ck05dvpj.png" /></p>
<p>​ GN 把通道分为组，并计算每一组之内的均值和方差，以进行归一化。GN 的计算与批量大小无关，其精度也在各种批量大小下保持稳定。可以看到，GN和LN很像。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/1496926-9e0fd762d02d26c1.webp" /></p>
<h5 id="类比-3"><strong>类比</strong>：</h5>
<p>​ GN 相当于把一本 C 页的书平均分成 G 份，每份成为有 C/G 页的小册子，求每个小册子的“平均字”和字的“标准差”。</p>
<h5 id="总体"><strong>总体</strong>：</h5>
<p>​ <strong>LN 和 IN 在视觉识别上的成功率都是很有限的</strong>，对于<strong>训练序列模型（RNN/LSTM）或生成模型（GAN）</strong>很有效。所以，<strong>在视觉识别领域，BN用的比较多，GN就是为了改善BN的不足而来的。</strong></p>
<p>​ <strong>GN适用于占用显存比较大的任务，例如图像分割</strong>。对这类任务，可能 batchsize 只能是个位数，再大显存就不够用了。而当 batchsize 是个位数时，BN 的表现很差，因为没办法通过几个样本的数据量，来近似总体的均值和标准差。GN 是独立于 batch 的,所以可以适用。</p>
<h3 id="conditional-instance-normcin">5、Conditional Instance Norm（CIN）</h3>
<p>​ Dumoulin等人在进行风格迁移任务，使用IN的时候，用不同的<span class="math inline">\(\gamma\)</span>和 <span class="math inline">\(\beta\)</span>即可生成出风格不同的图像，于是提出了Conditional Instance Normalization(CIN)。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117102111791.png" /></p>
<p>​ 其中s代表风格，<span class="math inline">\(\gamma ^s\)</span>和 <span class="math inline">\(\beta ^s\)</span>是学出来的，一组 <span class="math inline">\((\gamma ^s,\beta ^s)\)</span>对应一种风格。Dumoulin等人的方法迁移有限种的风格，想迁移新的的风格则需要训练新的模型。</p>
<p><strong>部分内容参考链接：</strong></p>
<p>https://zhuanlan.zhihu.com/p/54530247</p>
<p>https://zhuanlan.zhihu.com/p/56542480</p>
<p>https://www.jianshu.com/p/f15fcdf13438</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>《Deep Residual Fourier Transformation for Single Image Deblurring》论文笔记</title>
    <url>/2022/01/14/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E3%80%8ADeep%20Residual%20Fourier%20Transformation%20for%20Single%20Image%20Deblurring%E3%80%8B/</url>
    <content><![CDATA[<h4 id="论文名称deep-residual-fourier-transformation-for-single-image-deblurring">论文名称：《Deep Residual Fourier Transformation for Single Image Deblurring》</h4>
<h4 id="论文地址-httpsarxiv.orgabs2111.11745">论文地址： https://arxiv.org/abs/2111.11745</h4>
<h2 id="关键词">1、关键词：</h2>
<p>​ Image Deblurring、FFT、ResBlock</p>
<h2 id="领域背景">2、领域背景：</h2>
<p>​ Image Deblurring 图像去模糊，往往指由非常规相机或物体移动、光学虚焦等因素引起的一种现象，他们会导致看上去低质量的图像。</p>
<h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2>
<p>​ 在先前，DeepDeblur使用CNN配合ResBlock，构建了多尺度的一个结构，使用Residual Block来聚焦学习模糊的图像和清晰的图像对之间的差距。并且取得了非常好的效果。但是，它也有一定的局限性：</p>
<ul>
<li><p>ResBlock通常在CNN中进行，其感知域容易受到限制（尤其是在比较前面的层），所以ResBlock的机制往往会无法对全局的信息进行建模（这些信息往往在从一个模糊图像重建一个清晰图像的时候较为有用）</p></li>
<li><p>先前的方法很少从频域的角度去关注模糊的图像和清晰的图像之间的关系，而我们发现，相较于模糊图像，清晰的图像往往包含更少的低频信息以及更多的高频信息。</p></li>
<li><p>CNN在捕获可见的特征的时候很厉害，但是对于频域的特征较弱，并且ResBlock 可能具备良好的高频信息的学习，但是对于低频信息的学习较弱一些。</p></li>
<li><p>论文贡献</p>
<ul>
<li>这篇论文提出的 <em>Residual Fast Fourier Transform with Convolution Block (Res FFT-Conv Block)</em>既能够捕获长距离信息，也能捕获短距离信息，同时也有能力考虑整个高频和低频的信息。通过使用这个Block还提出了一个框架，可以应用于图像去模糊领域 <em>Residual Fourier Transformation (DeepRFT) framework</em></li>
</ul></li>
</ul>
<h2 id="主要设计思想">4、主要设计思想：</h2>
<p>​ 提出了<strong>Residual Fast Fourier Transform with Convolution Block（Res FFT-Conv Block）</strong>模块，这是一个即插即用的模块。设计思想如下所示：</p>
<ul>
<li>Res FFT-Conv模块 将图像从空间域转至频域，然后使用<span class="math inline">\(1 \times 1\)</span>的卷积层进行卷积。由于FFT的特性，就能够使得在很early的层上卷积的感知域就能包含整个图像。其能更好的捕获模糊图像和清晰图像之间全局的差异。</li>
</ul>
<p>​ 提出了<strong>Deep Residual Fourier Transformation (DeepRFT) 框架</strong>，主要操作如下：</p>
<ul>
<li>DeepRFT框架通过将Res FFT-Conv模块插入进MIMO-UNet这个网络结构中，来进行图像去模糊的任务同时，使用DepthWise Over-parameterized Conolution以加速网络训练，达到很好的效果。</li>
</ul>
<h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2>
<h3 id="resblocklook-back">1) ResBlock（Look Back）</h3>
<ul>
<li><p><strong>网络结构</strong>：包含两个<span class="math inline">\(3 \times 3\)</span>卷积层 以及 一个 RELU激活函数层</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114155111786.png" alt="image-20220114155111786" style="zoom: 50%;" /></p></li>
<li><p><strong>意义</strong>：能够训练更深的网络结构、拥有更大的感知域、加快训练时的收敛速度。卷积操作能够很好的学习到一些图像中的高频信息，因为其经常能够从图像的edges中捕获信息。</p></li>
<li><p><strong>缺陷</strong>：</p>
<ul>
<li>缺少对低频信息的建模能力</li>
<li>虽然我们能够通过堆叠模块来加大感知域，但是堆叠会带来巨大的计算复杂度。且在前几层网络中，感知域大小还是非常局部的，缺少全局的信息。</li>
</ul></li>
</ul>
<h3 id="residual-fast-fourier-transform-blockcurrent-substitute">2）Residual Fast Fourier Transform Block（Current Substitute）</h3>
<ul>
<li><p><strong>输入</strong>：<span class="math inline">\(Z \in R^{H \times W \times C}\)</span></p></li>
<li><p><strong>网络结构</strong>：两条残差流：</p>
<ul>
<li>与ResBlock一致的空间域信息残差流</li>
<li>基于Channel-Wise FFT的频域信息流，用于在频域中捕获图像的全局信息。</li>
</ul>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114155802644.png" alt="image-20220114155802644" style="zoom: 50%;" /></p></li>
<li><p><strong>核心实现-公式表达</strong>（最左侧的频域信息流）</p>
<ol type="1">
<li><span class="math inline">\(F(Z) \in C^{H \times W/2 \times C}\)</span>,对输入的特征Z，计算 2D real FFT ,由于傅立叶变换两侧对称，所以宽方向可以仅保留一半</li>
<li><span class="math inline">\(\widetilde{Z} = R(F(Z)) \odot_C I((F(Z))) \in R^{H \times W/2 \times 2C}\)</span>, 将傅立叶变换后的实数部分和虚数部分在Channel层面Concatenate起来</li>
<li>经过2个<span class="math inline">\(1 \times 1\)</span>的卷积层和1个ReLU激活函数层</li>
<li><span class="math inline">\(Y^{fft} = F^{-1}(f^{real} + jf^{img}) \in R^{H \times W \times C}\)</span>应用逆向2D real FFT来将 f <span class="math inline">\((f^{real} \odot_C f^{img} )\)</span>变换回空间域</li>
<li><span class="math inline">\(Y = Y^{fft} + Y^{res} + Z\)</span>，最终输出使用三个相加的形式得到。</li>
</ol></li>
</ul>
<h3 id="deep-residual-fourier-transform-framework">3） Deep Residual Fourier Transform Framework</h3>
<ul>
<li><p><strong>简介</strong>：基于MIMO-UNet进行的设计，MIMO-UNet是一个用于做多尺度图像去模糊的多输入多输出的U-Net架构。将MIMO-UNet中的所有ResBlocks用该论文提出的Res FFT-Conv Blocks进行替换。同时，我们额外将所有的<span class="math inline">\(1 \times 1\)</span>卷积层用我们提出的DO-Conv替换掉了。</p></li>
<li><p>MIMO-UNet 详细可以参见这篇论文：《Rethinking Coarse-to-Fine Approach in Single Image Deblurring》</p></li>
<li><p><strong>网络结构</strong>：</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114170635133.png" alt="image-20220114170635133" style="zoom:50%;" /></p></li>
</ul>
<h3 id="depthwise-over-parameterized-convolution">4）<strong>Depthwise over-parameterized convolution</strong></h3>
<ul>
<li><p><strong>简介</strong>：DO-Conv已经在许多高级别的视觉任务中显示出了它的潜力，它加速了训练并且通过使用深度卷积增强卷积层来获得更好的性能。DO-Conv 是两个相邻的线性运算，在操作时可以组合成传统的卷积运算。</p></li>
<li><p>具体可以参见这篇论文《DO-Conv: Depthwise Over-parameterized Convolutional》</p></li>
<li><p>论文链接：https://arxiv.org/pdf/2006.12030.pdf</p></li>
</ul>
<h3 id="loss-function">5） Loss Function</h3>
<ul>
<li><p><span class="math inline">\(Let\)</span><span class="math inline">\({k \in {0,……,K-1}}\)</span>为 DeepRFT的第k个层级</p></li>
<li><p><span class="math inline">\({\hat{S_k}}\)</span>为 <span class="math inline">\(k_{th}\)</span>重建图像</p></li>
<li><p><span class="math inline">\(S_k 为\)</span> <span class="math inline">\(k_{th}\)</span>GroundTruth清晰的图像</p></li>
<li><p>考虑3种，不同类型的Loss Function：</p>
<ul>
<li><p>Multi-Scale Charbonnier loss：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114171903475.png" alt="image-20220114171903475" style="zoom:50%;" /></p></li>
<li><p>Multi-Scale Edge loss：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114171944558.png" alt="image-20220114171944558" style="zoom:50%;" /></p></li>
<li><p>Multi-Scale Frequency Reconstruction (MSFR) 在频域计算，FT代表FFT操作</p></li>
</ul></li>
</ul>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114172909832.png" alt="image-20220114172909832" style="zoom: 50%;" /></p>
<ul>
<li>最终Loss函数：<span class="math inline">\(L = L_{msc} + \alpha_1L_{msed}+ \alpha_2L_{msfr}\)</span></li>
<li><span class="math inline">\(\alpha_1\)</span>、<span class="math inline">\(\alpha_2\)</span>为tradeoff参数，通常为0.05 和 0.01</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>FFT</tag>
        <tag>Image Deblurring</tag>
        <tag>Residual Block</tag>
      </tags>
  </entry>
  <entry>
    <title>《SwinIR- Image Restoration Using Swin Transformer》论文笔记</title>
    <url>/2022/01/13/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E3%80%8ASwinIR-%20Image%20Restoration%20Using%20Swin%20Transformer%E3%80%8B/</url>
    <content><![CDATA[<h3 id="论文名称swinir-image-restoration-using-swin-transformer">论文名称：《SwinIR: Image Restoration Using Swin Transformer》</h3>
<h3 id="论文链接httpsopenaccess.thecvf.comcontenticcv2021waimhtmlliang_swinir_image_restoration_using_swin_transformer_iccvw_2021_paper.html">论文链接：https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html</h3>
<h2 id="关键词">1、关键词：</h2>
<p>​ 图像修复（Image Restoration）、Transformer</p>
<h2 id="领域背景图像修复">2、领域背景—图像修复：</h2>
<p>​ 图像修复是一个经典问题，一般而言其目标为从低分辨率的图像中恢复出高分辨率的图像。通常可以用于超分辨率、图像去噪，以及JPEG压缩鬼影去除等应用。此篇文章将Swin Transformer应用于图像修复领域中。</p>
<h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2>
<p>​ 和CNN比起来，Transformer设计了Self-Attention机制来捕获全局内容之间的信息交互，也在一系列的任务重得到了比较好的结果。但是ViT用于图像恢复的话，因为Vit要划分patch，所以就会不可避免的导致恢复的图像在patch和patch之间有边界感，同时每一个patch邻近边界的像素也会因为缺少信息而没法做出更好的恢复效果。</p>
<p>​ <strong>先前方法的问题</strong>：使用基于CNN的方法进行图像修复：会存在两个源于卷积层本身带来的基本的问题：</p>
<p>1、图像和卷积核之间是内容无关的，我们用同一个卷积核去修复不同的图像区域，可能并不是一个好的选择</p>
<p>2、由于CNN的local processing方案，卷积层是在局部邻域（ local neighborhood ）内建立像素关系，其长距离的依赖关系（long-range dependency）主要通过深度叠加卷积层来进行建模，有的时候可能并不是很有效</p>
<p>​ 而Swin Transformer，结合了两者的优势。既能够像CNN那样处理大尺度的图像，也能弥补CNN在long-range dependency上的不足（使用SW-MSA机制）</p>
<h2 id="主要设计思想">4、主要设计思想：</h2>
<p>​ SwinIR由三部分组成。首先浅层特征提取部分是由卷积层组成的，输出结果将直接传输到重建模块中，为了保持图像本身的低频信息，而深层特征提取模块主要由RSTB（Residual Swin Transformer blocks）组成。同时，其在每一个block的后面还加了一层卷积层，来做特征加强，以及使用残差网络来为特征聚合提供捷径。最终，浅层和深层特征被输送到重建模块，进行高质量的图像重建。总体流程概括如下：</p>
<ul>
<li>浅层特征提取：<strong>低质量图像</strong> <span class="math inline">\(\to\)</span><strong>浅层特征图</strong></li>
<li>深层特征提取：<strong>浅层特征图</strong><span class="math inline">\(\to\)</span><strong>深层特征图</strong></li>
<li>高质量图片生成：<strong>浅层特征图</strong>+<strong>深层特征图</strong><span class="math inline">\(\to\)</span><strong>高质量图像</strong></li>
</ul>
<h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/network_swinir.png" /></p>
<p>​ 首先，对于这一整个架构而言，分为3大块，分别是浅层特征提取、深层特征提取以及图像重建，<strong>对于不同的任务而言，我们使用相同的特征提取模块，但是会使用不同的图像重建模块</strong>。</p>
<h3 id="浅层特征提取shallow-feature-extraction">1）浅层特征提取(Shallow Feature Extraction)</h3>
<ul>
<li><strong>概述</strong>：使用卷积网络从<strong>低质量图像</strong>中提取<strong>浅层特征图</strong></li>
<li><strong>输入</strong>：<strong>低质量图像(LQ)</strong> <span class="math inline">\(I_{LQ}\in R^{H\times W\times C_{in}}\)</span></li>
<li><strong>输出</strong>：<strong>浅层特征图</strong> <span class="math inline">\(F_{0}\in R^{H\times W\times C_{embed}}\)</span>，<span class="math inline">\(C_{embed}\)</span>为特征通道数目</li>
<li><strong>网络结构</strong>：单层<span class="math inline">\(3\times3\)</span>卷积网络</li>
<li><strong>公式表达</strong>：<span class="math inline">\(F_{0}=H_{SF}(I_{LQ})\)</span>，其中<span class="math inline">\(H_{SF}\)</span>为卷积网络</li>
<li><strong>实现细节（官方代码）</strong>
<ul>
<li><span class="math inline">\(H_{SF}\)</span>为单层，输入图像通道<span class="math inline">\(C_{in}=3\)</span>，输出特征通道<span class="math inline">\(C=96\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络</li>
</ul></li>
<li><strong>意义</strong>：卷积层在前期的视觉处理中往往能起到较好的效果，能够导致一个较为稳定的结果，同时也提供了一个从图像空间映射到高维特征空间的手段。</li>
</ul>
<h3 id="深层特征提取deep-feature-extraction">2）深层特征提取(Deep Feature Extraction)：</h3>
<ul>
<li><p><strong>输入</strong>：<strong>浅层特征图</strong><span class="math inline">\(F_{0}\in R^{H\times W\times C_{embed}}\)</span></p></li>
<li><p><strong>输出</strong>：<strong>深层特征图</strong><span class="math inline">\(F_{DF}\in R^{H\times W\times C_{embed}}\)</span></p></li>
<li><p><strong>网络结构：</strong></p>
<ul>
<li><span class="math inline">\(K\)</span>个串联的Residual Swin Transformer Block（RSTB）和 <span class="math inline">\(1\)</span>个卷积层 构成</li>
<li>每个RSTB（Residual Swin Transformer Block）内部由<span class="math inline">\(L\)</span>个串联的 Swin Transformer Layer（STL）和 一个卷积层构成（如上图a）</li>
<li>STL（Swin Transformer Layer）结构为Layer Norm<span class="math inline">\(\to\)</span>MSA<span class="math inline">\(\to\)</span>Layer Norm<span class="math inline">\(\to\)</span>MLP 且 MSA和MLP后有残差连接（如上图b）</li>
</ul></li>
<li><p><strong>公式表达</strong>：</p>
<ul>
<li><strong>整体结构</strong>：
<ul>
<li><span class="math inline">\(F_i=H_{RSTB_i}(F_{i-1}),\ i=1,2,...,K\)</span></li>
<li><span class="math inline">\(F_{DF}=H_{CONV}(F_{K})\)</span></li>
</ul></li>
<li><strong>RSTB</strong>：
<ul>
<li><span class="math inline">\(F_{i,j}=H_{Swin_{i,j}}(F_{i,j-1}),\ j=1,2,...,L\)</span></li>
<li><span class="math inline">\(F_i=H_{CONV_i}(F_{i,L})\)</span></li>
</ul></li>
<li><strong>STL</strong>：
<ul>
<li><span class="math inline">\(H_{Swin_{i,j}}(X)=STL_2(STL_1(X))\)</span></li>
<li><span class="math inline">\(STL_1(X)=MSA(LN(X))+X\)</span></li>
<li><span class="math inline">\(STL_2(X)=MLP(LN(X))+X\)</span></li>
</ul></li>
</ul></li>
<li><p><strong>实现细节（官方代码）</strong></p>
<ul>
<li><p><strong>PatchEmbed和PatchUnEmbed操作</strong></p>
<ul>
<li>代码中使用PatchEmbed操作将<span class="math inline">\(224\times224\)</span>的特征图拆分为<span class="math inline">\(16\times16\)</span>的Patch，并且有可选的LayerNorm操作</li>
<li>对应的代码中使用PatchUnEmbed操作将<span class="math inline">\(16\times16\)</span>的Patch还原为<span class="math inline">\(224\times224\)</span>的特征图</li>
<li>PatchEmbed和PatchUnEmbed操作的执行逻辑如下
<ul>
<li>每一个RSTB的开头，执行PatchEmbed操作</li>
<li>每一个RSTB的卷积操作前，执行PatchUnEmbed操作</li>
<li>只在第一个RSTB开头的PatchEmbed操作中，使用LayerNorm</li>
</ul></li>
</ul></li>
<li><p><strong>RSTB的串联数量（<span class="math inline">\(K\)</span>的取值）</strong></p>
<ul>
<li>虽然论文图片中的RSTB有6个，但在代码中只有4个，即<span class="math inline">\(K=4\)</span></li>
</ul></li>
<li><p><strong>STL的串联数量（<span class="math inline">\(L\)</span>的取值）</strong></p>
<ul>
<li>代码中每个RSTB内部的STL的数目为6个，与论文图片中的一致，即<span class="math inline">\(L=6\)</span></li>
</ul></li>
<li><p><strong>配对的STL</strong></p>
<ul>
<li>如前述所说，每个RSTB中STL的数目为6个，其中每两个STL构成一组，第一个STL内的MSA为Swin中的W-MSA，第二个为Swin中的SW-MSA，这一操作与原版Swin中的一致</li>
</ul></li>
<li><p><strong>卷积操作</strong></p>
<ul>
<li>对于整个深层特征提取模块末尾和RSTB内部的卷积，代码根据任务不同分为两种
<ol type="1">
<li>对于小模型任务，如一般图片、轻量图片的SR、图片降噪和JPEG格式压缩图像修复，采用的是单层，输入输出维度为<span class="math inline">\(C_{embed}=96\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络</li>
<li>对于大模型任务，如真实世界图片SR，采用的是一个多层卷积网络
<ul>
<li>第一层为输入维度为<span class="math inline">\(C_{embed}=96\)</span>，输出维度为<span class="math inline">\(C_{embed}/4\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对输出进行LeakyReLu</li>
<li>第二层为输入维度为<span class="math inline">\(C_{embed}/4\)</span>，输出维度为<span class="math inline">\(C_{embed}/4\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对输出进行LeakyReLu</li>
<li>第三层为输入维度为<span class="math inline">\(C_{embed}/4\)</span>，输出维度为<span class="math inline">\(C_{embed}\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络</li>
</ul></li>
</ol></li>
</ul></li>
</ul></li>
<li><p><strong>意义</strong>：</p>
<ul>
<li>作者认为在所有RSTB后面，再加一层Conv层，能够将卷积操作的归纳偏置(inductive bias)带入这类基于Transformer骨架的网络中，为后续浅层和深层特征的融合打好基础。</li>
<li><!--具体的作用需要通过实验验证，这个Conv层是否是必要的。--></li>
<li>对于单个RSTB内部的卷积操作，论文认为卷积核在空间上的不变性可以增强提取出特征的平移不变性</li>
<li>残差连接为不同的RSTB模块提供了一个到后续图像重建模块的短连接，允许不同级别的特征在最后一个重建模块中更好的进行聚合。</li>
<li><!--同样，具体的作用需要通过实验验证，这个Conv层是否是必要的。--></li>
</ul></li>
</ul>
<h3 id="高质量图像修复-hq-image-reconstruction">3）高质量图像修复 (HQ Image Reconstruction)：</h3>
<ul>
<li><strong>概述</strong>：根据<strong>浅层与深层特征图</strong>生成<strong>高质量图像</strong>，浅层特征负责包含低频信息，深层特征聚焦于恢复丢失的高频信息。</li>
<li><strong>输入</strong>：<strong>浅层特征图</strong> <span class="math inline">\(F_{0}\in R^{H\times W\times C_{embed}}\)</span>和 <strong>深层特征图</strong> <span class="math inline">\(F_{DF}\in R^{H\times W\times C_{embed}}\)</span></li>
<li><strong>输出</strong>：<strong>高质量重建图像(RHQ)</strong> <span class="math inline">\(I_{RHQ}\in R^{H&#39;\times W&#39;\times C_{out}}\)</span></li>
<li><strong>网络结构</strong>：卷积网络</li>
<li><strong>公式表达</strong>：<span class="math inline">\(I_{RHQ}=H_{REC}(F_{0}+F_{DF})\)</span>，其中<span class="math inline">\(H_{REC}\)</span>为重建模块函数，其实在实现上就是一个卷积模块</li>
<li><strong>实现细节（官方代码）</strong>
<ul>
<li><span class="math inline">\(H_{REC}(F_{0}+F_{DF})\)</span>中的“+”号就是数学意义上的相加，本质上是残差连接</li>
<li><strong>卷积操作</strong>
<ul>
<li>根据任务不同分为四种卷积操作：
<ol type="1">
<li>对于图像去噪和JPEG格式压缩图像修复，采用的是单层，输入维度为<span class="math inline">\(C_{embed}=96\)</span>，输出维度为<span class="math inline">\(C_{out}=3\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，同时卷积输出与输入使用残差连接，即<span class="math inline">\(H_{REC}(X)=H_{CONV}(X)+X\)</span></li>
<li>对于一般图像SR，卷积分为三层
<ul>
<li>第一层进行特征降维，为输入维度为<span class="math inline">\(C_{embed}=96\)</span>，输出维度为<span class="math inline">\(C_{feat}=64\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对卷积输出进行LeakyReLU</li>
<li>第二层进行上采样，先通过输入维度为<span class="math inline">\(C_{feat}=64\)</span>，输出维度为<span class="math inline">\(4\times C_{feat}\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，然后对卷积输出进行PixelShuffle</li>
<li>第三层为输入维度为<span class="math inline">\(C_{feat}=64\)</span>，输出维度为<span class="math inline">\(C_{out}=3\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络</li>
</ul></li>
<li>对于轻量图像SR，为了减少参数，采用的是单层，输入维度为<span class="math inline">\(C_{feat}=64\)</span>，输出维度为<span class="math inline">\(Scale^2\times C_{out}\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，然后对卷积输出进行PixelShuffle</li>
<li>对于真实世界图像SR，使用多层卷积
<ul>
<li>第一层进行特征降维，为输入维度为<span class="math inline">\(C_{embed}=96\)</span>，输出维度为<span class="math inline">\(C_{feat}=64\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对卷积输出进行LeakyReLu</li>
<li>第二、三层进行上采样，先使用torch.nn.functional.interpolate函数进行指定<span class="math inline">\(Scale\)</span>的最近邻上采样，然后通过输入输出维度为<span class="math inline">\(C_{feat}=64\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对卷积输出进行LeakyReLU</li>
<li>第四层为输入输出维度为<span class="math inline">\(C_{feat}=64\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络，并对卷积输出进行LeakyReLU</li>
<li>第五层为输入维度为<span class="math inline">\(C_{feat}=64\)</span>，输出维度为<span class="math inline">\(C_{out}=3\)</span>，步长<span class="math inline">\(stride=1\)</span>，填充<span class="math inline">\(pading=1\)</span>的<span class="math inline">\(3\times3\)</span>卷积网络</li>
</ul></li>
</ol></li>
</ul></li>
</ul></li>
</ul>
<h3 id="其他细节">4） 其他细节：</h3>
<p>​ 上述总的结构中看到的<strong>Skip-Connection</strong>，是能够将浅层特征直接输入到重建模块中，让深层特征提取模块专注于高频信息的提取以及能够获得更稳定的训练。</p>
<h3 id="使用的损失函数根据任务情况略有不同">5） 使用的损失函数：（根据任务情况略有不同）</h3>
<ul>
<li><p>对于一般图像以及轻量图像SR，使用<strong>L1 Loss</strong>,<span class="math inline">\(I_{RHQ}\)</span>为重建的高质量图像，<span class="math inline">\(I_{HQ}\)</span>为Ground_Truth高质量图像</p>
<ul>
<li><span class="math inline">\(L = \sqrt{||I_{RHQ} - I_{HQ} ||_1}\)</span></li>
</ul></li>
<li><p>对于真实世界图像SR，会结合<strong>Pixel Loss</strong>和<strong>GAN Loss</strong>以及<strong>Percepture Loss</strong>来提升生成质量</p></li>
<li><p>对于去噪和JPEG压缩任务而言，我们使用<strong>Charbonnier Loss</strong>进行优化</p>
<ul>
<li><span class="math inline">\(\sqrt{||I_{RHQ}-I_{HQ}||^2 + \varepsilon^2 }\)</span>, <span class="math inline">\(\varepsilon\)</span>是个常数，通常被设置为<span class="math inline">\(10^{-3}\)</span></li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>Swin Transformer</tag>
        <tag>Image Restoration</tag>
      </tags>
  </entry>
  <entry>
    <title>《Uformer A General U-Shaped Transformer for Image Restoration》论文笔记</title>
    <url>/2022/01/13/Paper%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94%E3%80%8AUformer%20A%20General%20U-Shaped%20Transformer%20for%20Image%20Restoration%E3%80%8B/</url>
    <content><![CDATA[<h3 id="论文名称uformer-a-general-u-shaped-transformer-for-image-restoration">论文名称：《Uformer: A General U-Shaped Transformer for Image Restoration》</h3>
<h3 id="论文地址-httpsarxiv.orgabs2106.03106">论文地址： https://arxiv.org/abs/2106.03106</h3>
<h2 id="关键词">1、关键词：</h2>
<p>​ 图像修复（Image Restoration）、类UNet结构、Transformer</p>
<h2 id="领域背景图像修复">2、领域背景—图像修复：</h2>
<p>​ 图像修复是一个经典问题，一般而言其目标为从低分辨率的图像中恢复出高分辨率的图像。通常可以用于超分辨率、图像去噪，以及JPEG压缩鬼影去除等应用。此篇文章将Swin Transformer应用于图像修复领域中。</p>
<h2 id="先前工作描述与比较">3、先前工作描述与比较：</h2>
<p>​ 近年来图像修复很好结果的都是基于卷积的网络，但是卷积网络在long-range dependencies上依旧存在局限性。同时，最近也有很多文章使用transofmer对低分辨率的特征图进行处理（限于self-attention的计算复杂度）。</p>
<h2 id="主要设计思想">4、主要设计思想：</h2>
<p>​ 论文旨在 在多尺度分辨率下，去恢复更多的图像细节。Uformer基于UNet，只不过将所有的卷积层替换为了Encoder-Decoder结构，同时保留了整体的Encoder-Decoder架构以及skip-connections。总体来说就是使用TransformerBlock 构建了一个层次化的encoder-decoder网络。</p>
<p>​ <strong>2个核心设计</strong></p>
<p>​ 1、<strong>LeWin Transformer Block</strong>( locally-enhanced window Transformer block ),使用分块的self-attetiond代替全局的self-attention，在捕获高分辨率图像的局部特征时，减少了计算量。</p>
<p>​ 2、提出了<strong>可学习的多尺度恢复调制器</strong>，以多尺度 <strong>spatial bias</strong> 的形式去在decoder的不同层上调节特征。来处理不同的图像退化问题（比如说虚焦、运动模糊等。这个模块本身由一个多尺度的spatial bias组成，用来在decoder的不同层级上调整特征。更具体一点的话，就是一个可学习的基于窗口的tensor张量，和特征直接相加，从而来调整特征能够恢复更多的细节。整体而言，这个调制器对于恢复图像细节而言有非常强的能力，同时只带来了一点点额外的参数和计算代价。</p>
<h2 id="具体方法与网络架构">5、具体方法与网络架构：</h2>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220112112308268.png" /></p>
<h3 id="整体架构overall-pipeline">1) 整体架构（Overall Pipeline）</h3>
<ul>
<li><strong>概述</strong>：U型带Skip-Connection的Encoder-Decoder网络，基于UNet结构，但是将UNet中的所有卷积层都替换成了LeWinBlocks。</li>
<li><strong>输入</strong>：退化图像 <span class="math inline">\(I \in R^{3\times H\times W}\)</span></li>
<li><strong>输出</strong>：修复图像 <span class="math inline">\(I^{&#39;} \in R^{3\times H\times W}\)</span>
<ul>
<li><span class="math inline">\(Residual \in R^{3\times H\times W }\)</span></li>
<li><span class="math inline">\(I^{&#39;} = I + R e sidual\)</span></li>
</ul></li>
</ul>
<h3 id="input-projection">2) Input Projection</h3>
<ul>
<li><strong>概述</strong>：用于从退化图像中提取低层级信息</li>
<li><strong>输入</strong>：退化图像 <span class="math inline">\(I \in R^{3\times H\times W}\)</span></li>
<li><strong>输出</strong>：low-level 特征图 <span class="math inline">\(X_0\)</span></li>
<li><strong>网络结构</strong>：
<ul>
<li><span class="math inline">\(3\times 3\)</span>卷积层 + LeakyReLU激活函数</li>
</ul></li>
</ul>
<h3 id="encoder---one-stage-encoder中总共k个stage">3) Encoder - One Stage （ Encoder中总共K个Stage ）</h3>
<ul>
<li><strong>概述</strong>：用于从退化图像中提取低层级信息</li>
<li><strong>输入</strong>：特征图 <span class="math inline">\(X_{l-1}\)</span></li>
<li><strong>输出</strong>：第<span class="math inline">\(l\)</span>个阶段的输出特征图 <span class="math inline">\(X_l \in R^{2^lC \times \frac{H}{2^l} \times \frac{W}{2^l}}\)</span></li>
<li><strong>网络结构</strong>：
<ul>
<li><span class="math inline">\({N_l}\)</span>个 LeWinBlocks的叠加，以及一个下采样层（$4  $卷积 $stride$2）</li>
</ul></li>
<li><strong>实现细节（官方代码）</strong>
<ul>
<li>LeWinBlocks
<ul>
<li>将2D空间特征图先进行patch划分，然后展平，具体见后</li>
</ul></li>
<li>Down Sampling：（channel 翻倍，特征图宽高 减半）
<ul>
<li>Reshape 展平的特征 至 2D的空间特征图，然后使用 <span class="math inline">\(4 \times 4\)</span>卷积 $stride$2，对其进行卷积</li>
</ul></li>
</ul></li>
<li><strong>意义</strong>：在一遍遍的下采样与Lewin Transformer Blocks中提取特征信息，Lewin Transformer Block可以更好的提取远距离的相关性依赖信息，并且有效的降低计算复杂度</li>
</ul>
<h3 id="bottleneck-stage">4) BottleNeck Stage</h3>
<ul>
<li><strong>概述</strong>：用于提取更长距离的信息，甚至是全局的信息</li>
<li><strong>输入</strong>：特征图 <span class="math inline">\(X_{K}\)</span></li>
<li><strong>输出</strong>：序列向量 V</li>
<li><strong>网络结构</strong>：
<ul>
<li><span class="math inline">\({N_?}\)</span>个 LeWinBlocks的叠加</li>
</ul></li>
<li><strong>实现细节（官方代码）</strong>
<ul>
<li>LeWinBlocks
<ul>
<li>将2D空间特征图先进行patch划分，然后展平，具体见后</li>
</ul></li>
</ul></li>
<li><strong>意义</strong>：由于前面已经进行了许多次的特征提取以及下采样了，所以在此处再增加一个LeWinBlocks的模块，能够捕获到更长距离的信息，甚至是能够捕获全局的信息。</li>
</ul>
<p><!-- 但是这个模块难道不可以用Encoder阶段更大的K 来弥补吗？需要实验 --></p>
<h3 id="decoder---one-stage-decoder中总共k个stage与encoder一致">5）Decoder - One Stage （Decoder中总共K个Stage，与Encoder一致 ）</h3>
<ul>
<li><strong>概述</strong>：用于小尺寸的特征图中逐渐重建恢复特征信息</li>
<li><strong>输入</strong>：1维特征序列 <span class="math inline">\(V_{in}\)</span></li>
<li><strong>输出</strong>：1维特征输出序列 <span class="math inline">\(V_{out}\)</span></li>
<li><strong>网络结构</strong>：
<ul>
<li>一个上采样层（$2  $反卷积 $stride$2）以及 <span class="math inline">\({N_l}\)</span>个 LeWinBlocks的叠加</li>
</ul></li>
<li><strong>实现细节（官方代码）</strong>
<ul>
<li>Up Sampling：（channel 减半，特征图宽高 翻倍）
<ul>
<li>先将输入的特征序列 Reshape 成 2D特征图，然后进行上采样，得到新的2D特征图</li>
</ul></li>
<li>Residual Module：
<ul>
<li>Up Sampling 完以后得到的2D特征图需要和对应Encoder-Stage中 输出的2D特征图进行Concat，然后得到一个新的2D特征图。</li>
</ul></li>
<li>LeWinBlocks
<ul>
<li>将残差Concat得到的2D空间特征图先进行patch划分，然后展平，具体见后</li>
</ul></li>
</ul></li>
<li><strong>意义</strong>：在一遍遍的上采样与Lewin Transformer Blocks中重建特征信息。</li>
</ul>
<h3 id="output-projection">6) Output Projection</h3>
<ul>
<li><strong>概述</strong>：用于将输出的序列，变换成2D特征图后，再 映射至 3通道的残差图像，准备和原图像叠加。</li>
<li><strong>输入</strong>：1维特征输出序列 <span class="math inline">\(V_{out}\)</span></li>
<li><strong>输出</strong>：残差图像 <span class="math inline">\(Residual \in R^{3\times H\times W }\)</span></li>
<li><strong>网络结构</strong>：
<ul>
<li><span class="math inline">\(3\times 3\)</span>卷积层</li>
</ul></li>
<li><strong>实现细节（官方代码）</strong>：
<ul>
<li>先将 最后一个Decoder Stage输出的 序列 <span class="math inline">\(V_{out}\)</span>Reshape成2D的特征图，然后应用<span class="math inline">\(3 \times 3\)</span>的卷积层，来获得一个残差图像</li>
</ul></li>
</ul>
<h3 id="最终输出-loss函数">7）最终输出 &amp; Loss函数</h3>
<ul>
<li><strong>概述</strong>：将OutPut Projection 得到的Residual Image 和 原来的输入图像相加，得到 最终的修复图像。</li>
<li><strong>Loss</strong>：使用 <strong>Charbonnier Loss</strong>，<span class="math inline">\(I^{&#39;}\)</span>是 ground-truth 图像
<ul>
<li><span class="math display">\[l(I^{&#39;},\hat I) = \sqrt{||I^{&#39;} - \hat I||^2 + \varepsilon^2}\]</span></li>
</ul></li>
</ul>
<h3 id="lewin-transformer-block">8）LeWin Transformer Block</h3>
<ul>
<li><p><strong>概述</strong>：由W-MSA 以及LeFF两个模块组成</p></li>
<li><p><strong>输入</strong>：2D特征图</p></li>
<li><p><strong>输出</strong>：2D特征图</p></li>
<li><p><strong>网络结构</strong>：公式表达：</p>
<ul>
<li><span class="math inline">\(X_{l}^{&#39;} = W-MSA(LN(X_{l-1})) + X_{l-1}\)</span><strong>W-MSA 模块的输出</strong></li>
<li><span class="math inline">\(X_l = LeFF(LN(X_l^{&#39;})) + X_l^{&#39;}\)</span>**LeFF模块的输出*</li>
</ul></li>
<li><p><strong>实现细节（官方代码）</strong>：</p>
<ul>
<li><p>W-MSA 与 Vision Transformer中一致</p></li>
<li><p>作者论文中说尝试了移动窗口，但是带来的结果好坏增长微乎其微，说明：在图像修复领域，窗口与窗口之间的信息交互并不是很重要。图像修复领域比较注重局部的信息交互？</p></li>
<li><p>LeFF模块结构：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/截屏2022-01-12%20下午3.31.12.png" /></p>
<ul>
<li>1、对于每个Token，应用一个Linear Projection，增加特征channel</li>
<li>2、将tokens Reshape 成 2D 特征图，用 <span class="math inline">\(3 \times 3\)</span>depth-wise convolutional 捕获局部信息</li>
<li>3、再将2D特征图展平回tokens，再应用一个Linear Projection，缩减特征channel</li>
<li>在每一个Linear Projection / Convolution 后都会应用 GELU激活函数。</li>
</ul></li>
</ul></li>
<li><p><strong>意义</strong>：解决了如下两个问题 1）全局计算self-attention复杂度太高 2）在捕捉局部的依赖关系时有限制。</p></li>
</ul>
<h3 id="multi-scale-restoration-modulator">9）Multi-Scale Restoration Modulator</h3>
<ul>
<li><strong>概述</strong>：因为不同的图像有不同的混乱残差形式，比如模糊、噪音、下雨等等，为了更好的应对各种不同的修复任务，提出了这个light-weight multi-scale的恢复模块。来calibrate特征以及更好的修复细节。<br />
</li>
<li><strong>网络结构</strong>：
<ul>
<li>在每一个LeWin Transformer Block中，其为一个<span class="math inline">\(M \times M \times C\)</span>大小的张量，M为window_size，C为特征图通道数。</li>
<li>在一个LeWin Transformer Block中，对于所有其分割出来的windows，这个 调制器模块的参数都是共享的。</li>
</ul></li>
<li><strong>形式</strong>：在Self-Attention计算前，将其直接加到每一个窗口的像素值上。</li>
<li><strong>意义</strong>：在Image Deblurring 和 Image Denoising里面，作者证实了该模块的重要性，能够更好的修复细节。一种可能的解释是添加 在解码器的每个阶段的modulator可以对特征图进行更为灵活的调整，从而提高恢复的细节的性能表现。这个跟先前的StyleGAN的某个模块一致。</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>Swin Transformer</tag>
        <tag>Image Restoration</tag>
        <tag>UNet</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记6——全卷积网络FCN &amp; U-Net结构</title>
    <url>/2022/01/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B06%E2%80%94%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9CFCN&amp;U-Net%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<p>本文讲解FCN与U-Net相关的知识，在此之前你需要了解CNN是什么</p>
<h3 id="一fcn全卷积网络">一、FCN全卷积网络</h3>
<p>​ 首先，我们还是要回顾一下CNN的整体网络架构与优势：CNN网络最后输出的是类别的概率值。CNN 的强大之处在于它的多层卷积结构能自动学习特征，并且可以学习到多个层次的特征：</p>
<p>​ 较浅的卷积层感知域较小，学习到一些局部区域的特征。</p>
<p>​ 而较深的卷积层具有较大的感知域，能够学习到更加抽象一些的特征。这些抽象特征对物体的大小、位置和方向等敏感性更低，从而有助于识别性能的提高。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114212245766.png" /></p>
<p>​ 而FCN相较于CNN来说，其将CNN最后几个用于输出概率的全连接层都改成了卷积层，从而使得模型网络中所有的层都是卷积层，最终输出一张已经label好的图像，故称为全卷积网络。全卷积神经网络主要使用了三种技术：</p>
<p>​ 1、卷积化（Convolutional）</p>
<p>​ 2、上采样（Upsample）</p>
<p>​ 3、跳跃结构（Skip Layer）</p>
<p>​ 整个FCN网络基本原理如图5（只是原理示意图）：</p>
<p>​ 1、image经过多个卷积和+一个max pooling变为pool1 feature，宽高变为1/2</p>
<p>​ 2、pool1 feature再经过多个conv+一个max pooling变为pool2 feature，宽高变为1/4</p>
<p>​ 3、pool2 feature再经过多个conv+一个max pooling变为pool3 feature，宽高变为1/8</p>
<p>​ 4、......</p>
<p>​ 5、直到pool5 feature，宽高变为1/32。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/dsawqeasd.png" /></p>
<p>那么，对于三种不同规格参数的FCN，后续还原操作也不太一样，如下所示：</p>
<p>​ 1、对于FCN-32s，直接对pool5 feature进行32倍上采样获得32x upsampled feature，再对32x upsampled feature每个点做softmax prediction获得32x upsampled feature prediction（即语义分割图）。</p>
<p>​ 2、对于FCN-16s，首先对pool5 feature进行2倍上采样获得2x upsampled feature，再把pool4 feature和2x upsampled feature逐点相加，然后对相加的feature进行16倍上采样，并softmax prediction，获得16x upsampled feature prediction。</p>
<p>​ 3、对于FCN-8s，首先进行pool4+2x upsampled feature逐点相加，然后又进行pool3+2x upsampled逐点相加，即进行更多次特征融合。具体过程与16s类似，不再赘述。</p>
<p>​ 在上述处理过程中，我们发现FCN-16s和FCN-8s都引入了skip connection，将pool3或是pool4 feature与pool5上采样后的feature逐像素相加，进行多次特征融合，这样处理的原因在于：</p>
<p>​ FCN模型虽然通过卷积和反卷积我们基本能定位到目标区域，但是，我们会发现模型前期是通过卷积、池化、非线性激活函数等作用输出了特征权重图像，我们经过反卷积等操作输出的<strong>图像实际是很粗糙的</strong>，毕竟丢了很多细节。因此我们需要找到一种方式填补丢失的细节数据，所以就有了跳跃结构。</p>
<p>​ 作者在原文种给出3种网络结果对比，明显可以看出效果：FCN-32s &lt; FCN-16s &lt; FCN-8s，即使用多层feature融合有利于提高分割准确性。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114212421618.png" /></p>
<h4 id="fcn优点">FCN优点：</h4>
<p>​ 与传统用CNN进行图像分割的方法相比，FCN有两大明显的优点：一是可以接受任意大小的输入图像，而不用要求所有的训练图像和测试图像具有同样的尺寸。二是更加高效，因为避免了由于使用像素块而带来的重复存储和计算卷积的问题。</p>
<h4 id="fcn缺点">FCN缺点：</h4>
<p>1、分割的结果不够精细。图像过于模糊或平滑，没有分割出目标图像的细节</p>
<p>2、因为模型是基于CNN改进而来，即便是用卷积替换了全连接，但是依然是独立像素进行分类，没有充分考虑像素与像素之间的关系</p>
<h2 id="二关于卷积网络中降采样与上采样以及特征提取阶段的理解">二、关于卷积网络中降采样与上采样以及特征提取阶段的理解：</h2>
<p>​ 1、降采样的理论意义是，它可以增加对输入图像的一些小扰动的鲁棒性，比如图像平移，旋转等，减少过拟合的风险，降低运算量，增加感受野的大小。</p>
<p>​ 2、上采样的最大的作用其实就是把抽象的特征再还原解码到原图的尺寸，最终得到分割结果。但很容易得到模糊或过于平滑的结果，无法还原细节部分。</p>
<p>​ 3、对于特征提取阶段，浅层结构可以抓取图像的一些简单的特征，比如边界，颜色，而深层结构因为感受野大了，而且经过的卷积操作多了，能抓取到图像的一些抽象特征。</p>
<h3 id="三unet网络">三、UNet网络</h3>
<p>​ UNet网络整体结构如下：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114212533302.png" /></p>
<p>​ 蓝色箭头代表3*3的卷积层+ReLU激活函数层</p>
<p>​ 红色箭头代表2*2的最大池化层</p>
<p>​ 绿色箭头代表2*2的上采样层（通常采用反卷积）</p>
<p>​ 浅蓝色箭头代表1*1的卷积层，在最后用于调整输出通道数目</p>
<p>​ 在整一个过程中，值得注意的是灰色箭头，我们会注意到灰色箭头代表的是“copy and crop”，即复制和剪切。我们以第一层的灰色箭头举例来看：其左侧大小为64 * 568 * 568，右侧大小为128 * 392 * 392。其中，右侧有64个通道的数据来源于左侧，64个通道的数据来源于上一层的上采样。那么灰色箭头应该就是把左侧的内容复制到了右侧，并且concat在了原先上采样后得到的64 * 392 * 392的数据上，形成了128 * 392 * 392的数据。</p>
<p>​ 但是问题在于，568 * 568和392 * 392还有较大的尺寸差别，这就需要利用crop来完成，由于在每个卷积中都会丢失边界像素，因此裁剪crop是必要的</p>
<p>​ 在上述网络形式中，最重要的结构就是其中的<strong>skip-connection</strong>。UNet中<strong>Concat</strong>形式的skip-connection的好处是，<strong>对于分割这个任务，空间域信息非常重要</strong>。而网络的encoder部分，通过各个pooling层已经把特征图分辨率降得非常小了，这一点不利于精确的分割mask生成，通过skip-connection可以把较浅的卷积层特征引过来，那些特征分辨率较高，且层数浅，会含有比较丰富的low-level信息，更利于生成分割mask。</p>
<p>​ 总体来说，就是把对应尺度上的特征信息引入到上采样或反卷积过程，为后期图像分割提供多尺度多层次的信息，由此可以得到更精细的分割效果，如U-Net论文描述的分割结果一样。这比单纯用编解码器框架要好，纯粹的编解码器框架，在编码过程中压缩和丢失了大量细节信息，而这些信息很可能会有助于后期的图像分割。</p>
<p>​ 同时，需要注意的一点是：此处的skip-connection与ResNet中直接相加形式的skip-connection不同，ResNet中的跳跃连接可以有效的减少梯度消失和网络退化问题，使训练更容易。直观上理解可以认为BP的时候，深层的梯度可以更容易的传回浅层，因为这种结构的存在，对神经网络层数的设定可以更随意一些。</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>UNet</tag>
        <tag>FCN</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记5——1x1卷积 &amp; 部分卷积 &amp; 空洞卷积 &amp; 可变形卷积</title>
    <url>/2022/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B05%E2%80%941x1%E5%8D%B7%E7%A7%AF&amp;%E9%83%A8%E5%88%86%E5%8D%B7%E7%A7%AF&amp;%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF&amp;%E5%8F%AF%E5%8F%98%E6%80%A7%E5%8D%B7%E7%A7%AF/</url>
    <content><![CDATA[<h3 id="一1-x-1-卷积">一、1 X 1 卷积</h3>
<h4 id="实现特征的升降维">1、实现特征的升降维</h4>
<p>​ 当1*1卷积出现时，在大多数情况下它作用是升/降特征的维度，这里的维度指的是通道数（厚度），而不改变图片的宽和高。</p>
<p>​ 因为1*1的卷积核，并不会改变图像的宽和高，但是其能够通过卷积核的数量，来调节输出的feature map的通道数。同时，其对不同通道上的像素点进行线性组合，有助于通道间信息的交互和整合过程</p>
<h4 id="减少模型参数量">2、减少模型参数量</h4>
<p>​ 这一想法最早在GoogleNet中被提出，比如说如下两个Inception模块：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114211201756.png" /></p>
<p>​ 从直观上来看，感觉像是只有3*3卷积的模块会拥有更少的训练数据，实则不然，我们可以计算一下训练数据个数：</p>
<p><strong>情况1:</strong></p>
<ol type="1">
<li><p>1 * 1卷积部分： 96 * 32 * 1 * 1= 3072个</p></li>
<li><p>3 * 3卷积部分： 32 * 48 * 3 * 3= 13824个</p>
<p>总计16896个训练参数</p></li>
</ol>
<p><strong>情况2:</strong> 总计 96<em>48</em>3*3 = 41472个训练参数</p>
<p>​ 从实际上来看，带1 * 1卷积的模型参数量更少，本质原因是因为1*1的卷积对数据的特征向量进行了降维的处理，使得特征通道数目先有了一定的减少。</p>
<p>​ 如下所示：在ResNet中的残差模块使用1*1的卷积核的意义也在于此：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114211323250.png" /></p>
<h3 id="二空洞卷积dilated-convolution">二、空洞卷积（Dilated Convolution）</h3>
<p>​ 顾名思义，其做法就是在卷积map中加入空洞的部分，以此增加感受域。如下图所示，</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114211417135.png" /></p>
<p>​ 右侧为普通卷积，左侧为空洞卷积。</p>
<p>​ 这个做法起源于最开始的语义分割领域，主要问题在于CNN本身可能存在一些致命性的缺陷，比如up-sampling和pooling layer的设计。问题主要在于：</p>
<p>​ 1、上采样和池化层是人为设计好的，是不可学习的。</p>
<p>​ 2、容易丢失内部数据结构与空间的层次化信息</p>
<p>​ 3、小物体的信息无法重建，假设有多个pooling layer，那么从理论上来讲在一定小的范围内的像素的物体是无法被重建的。</p>
<p>​ 所以有人提出了空洞卷积的思想。其本身避免了一部分上述讲到的2、3的问题，但是其也存在一些潜在问题，具体可以参考：https://www.zhihu.com/question/54149221</p>
<p>​ 最终，图森组提出了一个HDC的设计结构，来满足整个基于空洞卷积的设计。较为具体的内容也可以参考上述的文章。</p>
<h3 id="三部分卷积partial-convolution">三、部分卷积（Partial Convolution）</h3>
<p>​ 部分卷积在这篇论文中被提出，用于进行图像补全的任务，《Image Inpainting for Irregular Holes Using Partial Convolutions》</p>
<p>​ 在图像补全领域中，这篇论文前通常使用卷积层对有效像素和mask像素无差别的做卷积，容易导致颜色差异和伪影。而且先前的工作都是聚焦于一个图像中心的规则区域，并且都需要很昂贵的后处理机制。</p>
<p>​ 相比于人们以前使用的非神经网路的方法（PatchMatch等），使用神经网络的方法容易学习到语义的优点和有意义的隐藏表示信息，对于图像补全有很大的意义。但是这些网络使用卷积核的时候，使用一个固定的值将输入图像中被mask的地方代替，然后去卷积，这样就很容易导致孔洞区域内的纹理缺失，或者是边缘出现伪影。（就比如说使用经典卷积的U-Net架构。</p>
<p>​ 具体一点，如下图所示：先使用固定值m替换掉mask为0的区域，然后进行卷积。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114211456573.png" /></p>
<p>​ 而在这篇论文中提出了部分卷积的概念，只针对于有有效像素的部分进行卷积，并且能够自动的更新mask。</p>
<h4 id="部分卷积公式">1、部分卷积公式：</h4>
<p>​ 首先如下所示为部分卷积的公式：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114211521396.png" /></p>
<p>​ 仍然以如下所示为例：红色代表图像数据，蓝色代表mask数据，绿色代表卷积核。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114211532929.png" /></p>
<p>我们选择其中红色图像中的两个蓝色框起来的部分做卷积，作为示例：</p>
<p>首先公式中各个运算符号的含义如下英文描述所示，总体的步骤主要就是：</p>
<p>​ 1）将被卷积的区域先展开，形成X</p>
<p>​ 2）将mask对应的区域展开，形成M</p>
<p>​ 3）X和M做点对点乘积⊙，然后再乘上一个缩放因子，来调整有效输入的变化量。</p>
<p>​ 4）SUM(1)代表区域大小和卷积核一致，但值都是1的区域，所以在此处SUM(1)为9，而SUM(M)为7，因为该对应区域的mask中有7个1.</p>
<p>​ 5）最后再将卷积核展开形成W，两个向量相乘就完成了该区域的卷积。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/图片2.png" /></p>
<p>​ 另外一个区域，由于mask内的值都是0，所以最终输出的值直接为0.</p>
<h4 id="自动更新mask规则">2、自动更新Mask规则</h4>
<p>​ 具体更新规则描述如下：如果卷积能够根据至少一个有效输入值来计算输出输出，那么我们在下一步中，就把此像素点的mask标记为1.再讲的通俗一点，其实就是该范围的mask内只要有1，那就在下一层的卷积中，更新mask在此点的值至1.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114211755555.png" /></p>
<p>​ 具体的示例如下所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/图片3.png" /></p>
<p>​ 所以你会发现，在部分卷积的自动更新mask的过程中，只要部分卷积层叠加的足够多，最后任何mask都会逐渐变为所有都是1的形式，从而计算出需填补区域的内容。</p>
<p>​ 所以最终该论文在图像边界使用带有适当掩码的部分卷积来代替典型的填充。这确保了图像mask边缘处的修复内容不会受到图像外部无效值的影响。</p>
<h3 id="四可变形卷积deformable-convolution">四、可变形卷积（Deformable Convolution）</h3>
<p>​ 普通的卷积在应对一些物体复杂形变的场景的时候，往往无法得到好的结果。这个时候就有人提出了可变性卷积。</p>
<p>​ 简单而言，Deformable Conv 在感受野中引入了偏移量，而且这偏移量是可学习的，这样可以使得感受野不再是单一的方形，而是能够尽可能与物体的实际形状贴近，于是卷积区域便始终覆盖在物体形状周围，无论物体如何形变，加入可学习的偏移量后都可以搞定。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114211950684.png" alt="image-20220114211950684" style="zoom:150%;" /></p>
<p>​</p>
<p>​ 一个二维的卷积通常由两步组成，第一步是使用一个grid R去到输入的特征图上进行采样。第二步是根据卷积核的权重，对所有采样的点进行加权求和。比如说像下述这样一个就定义了一个3*3的方形的被卷积区域。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114212003778.png" /></p>
<p>​ 那么对于一个普通的卷积而言，可以用如下公式来进行表示，p0就是当前点，pn是R中的值。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114212010083.png" /></p>
<p>​ 在可变形卷积中，如下所示，R被加强了，还多了一个offset。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114212018393.png" /></p>
<p>​ 这样的话呢，采样就在一个不规则的并且带有偏移的位置上进行了。通常而言这个offset ∆pn 是小数，所以通常需要通过线性插值来计算像素的值，公式如下所示：其实就是x(p)的值要由其周围整数像素x(q)加权得到。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114212038364.png" /></p>
<p>​ 那么，关键的问题来了：这个offset怎么得到呢？这个offset是通过另一个在相同的输入图像上的卷积得到的。这个卷积的卷积核与当前的可变形卷积的卷积核有相同的空间分辨率。同时，经过这个卷积后得到的offest的输出域，和输入也有同样的空间分辨率。输出维度为2N，分别对应N个2维的offsets。</p>
<p>​ 在训练过程中，用来生成输出特征的卷积核和用来计算offsets的卷积核是同时进行学习的。下图是一个3*3的可变形卷积的示意图。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114212046759.png" /></p>
<p>​ 至于更为详细的可变形卷积思想以及实验，可以更深入的精读《Deformable Convolutional Networks》这篇论文。</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Convolution</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记4——Depth-wise卷积 &amp; Point-wise卷积</title>
    <url>/2022/01/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B04%E2%80%94Depth-wise%E5%8D%B7%E7%A7%AF&amp;Point-wise%E5%8D%B7%E7%A7%AF/</url>
    <content><![CDATA[<p><strong>提示：阅读本文前需要你掌握卷积相关操作概念</strong></p>
<h3 id="normal-convolution">1、Normal Convolution</h3>
<p>​ 输入一张3通道的图像，我们有4个3<strong>3</strong>3的卷积核，卷完以后，会生成4个特征图。特征图的数量等于卷积核的数量。每个卷积核的通道数与图像的通道数一致，一一对应相乘。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114211015420.png" /></p>
<h3 id="depth-wise-convolution">2、Depth-Wise Convolution</h3>
<p>​ Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积。上面所提到的常规卷积每个卷积核是同时操作输入图片的每个通道。</p>
<p>​ 同样是对于一张三通道输入图像，Depthwise Convolution不同于上面的常规卷积，其完全是在二维平面内进行。卷积核的数量应当与输入的图像的通道数相同（因为通道和卷积核是一一对应的）。所以一个三通道的图像经过运算后生成了3个Feature Map。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114211022892.png" /></p>
<p>​ 总结而言：Depthwise Convolution完成后的Feature map数量与输入的通道数相同，无法扩展。且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的特征信息。</p>
<p>​ 但是，Depthwise通过深度以及广度的操作能很好的保留各个通道信息的同时，降低了计算开销。而这一思想也逐渐应用到了移动端神经网络。</p>
<h3 id="point-wise-convolution1x1卷积">3、Point-Wise Convolution(1x1卷积)</h3>
<p>​ Pointwise Convolution的运算的卷积核的尺寸为 1×1×M，M为输入的图像的通道数。如下图所示：输入图像为3通道，经过4个1x1x3的卷积核以后，生成4个特征图，特征图大小并不会变化，故而这里的卷积运算其实就是将输入的图像在深度方向上进行加权组合，生成新的Feature map。</p>
<p>​ 详见：https://blog.slks.xyz/2022/01/07/basic/basic5/</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114211035827.png" /></p>
<h3 id="depth-wise-separable-convolution">4、Depth-Wise Separable Convolution</h3>
<p>​ Depthwise Separable Convolution是将一个完整的卷积运算分解为两步进行，即上面所述的Depthwise Convolution与Pointwise Convolution。</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Convolution</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记3——反卷积Deconvolution</title>
    <url>/2022/01/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B03%E2%80%94%E5%8F%8D%E5%8D%B7%E7%A7%AFDeconvolution/</url>
    <content><![CDATA[<p><strong>提示：在观看此篇文章前，需要你掌握卷积操作。</strong></p>
<p>​ Deconvolution反卷积通常用于进行图像的上采样，恢复分辨率。又称transposed convolution或是Fractionally-strided convolution。首先，我们先聊一聊为什么需要反卷积来进行上采样？</p>
<p>​ 在反卷积前，人们上采样通常采用插值的方法去进行，但是插值的方法完全是由人工定义规则，并不能够对于不同图像的插值有特定的优化，也没有可调整参数的空间。此时，我们想要让网络能够学出一种最优化的上采样方案，就是我们的反卷积操作。</p>
<p>​ 众所周知如下所示是一个普通的卷积操作，我们拿一个3<em>3的卷积核去卷一个4</em>4的矩阵，padding=0，stride=1，得到了一个2*2的矩阵。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210644864.png" /></p>
<p>​ 从实质的角度去理解卷积这一件事情，其实它就是建立了输入与输出图像的像素对应关系，比如说按照上图所示的内容，输入图像中每9个像素，通过卷积核，建立起了和输出图像中1个像素的映射关系。所以Convolution建立的是一个“多对一”的像素映射关系。那么，顾名思义，Deconvolution本身应该要做的就是从output到input之间，建立“一对多”的像素映射关系。也就是说从output的一个值中计算出input的9个值。看到这里也许会有些懵，实际上来说，卷积这个操作是没法逆向进行的，我们这边说的反卷积只是一种形式，因为其比较像而已。</p>
<p>​ 那么反卷积是如何进行的呢？我们先再来看一看卷积的另一种计算形式（除了移动卷积核遍历以外）—— 使用矩阵相乘的形式进行计算，然后能够很好的推出得到反卷积的计算形式。</p>
<p>仍然以上面的例子为例，我们如果想要一次性计算整个ouput中的值，可以如下操作：</p>
<p>​ 1、将input展平，成为一个16 * 1的矩阵A。</p>
<p>​ 2、将kernel按照一定规则扩充，扩充成为一个4 * 16的矩阵B</p>
<p>​ 3、将矩阵A和矩阵B计算矩阵乘法，得到一个1 * 4的向量C，将其reshape成2 * 2即output。</p>
<p>那么在步骤2中的“按照一定规则填充”是指什么呢？</p>
<p>​ 其实就是重新排列一下kernel矩阵, 使得我们通过一次矩阵乘法就能计算出卷积操作后的矩阵。以上述的卷积核为例，应当如下所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210739770.png" /></p>
<p>然后我们发现，原来卷积操作，可以写成两个矩阵相乘的形式，在上述例子中就是：</p>
<p>B（4 * 16）· A（16 * 1） = C（4 * 1）--》reshape --》 output（2 * 2）</p>
<p>​ 那么，现在反卷积要干的事情其实就是让C（4<em>1）·BT（16</em>4）就可以得到一个16<em>1的矩阵，然后进行reshape就可以变成一个4</em>4的图像，其分辨率为最开始输入的2*2的图像的两倍，就可以完成反卷积。</p>
<p>​ 其实，按照上述的做法，我们就建立了一个1对多的映射关系，请注意，在实际操作中，其实并不需要去计算反卷积的矩阵B内部到底是什么信息，B内部的参数是可以让网络进行学习而得到的，网络学习得到的参数往往就是针对于这一组训练集或者针对于特定任务而言，最好的一个上采样的方案。</p>
<p>​ 所以严格来说，反卷积并不算卷积，只是另一种形式罢了。</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Deconvolution</tag>
      </tags>
  </entry>
  <entry>
    <title>ML2021课程系列笔记4——海森矩阵计算（课程作业）</title>
    <url>/2021/12/23/ML2021%E7%AC%94%E8%AE%B0/ML2021%E8%AF%BE%E7%A8%8B%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B04%E2%80%94%E2%80%94%E6%B5%B7%E6%A3%AE%E7%9F%A9%E9%98%B5%E7%9A%84%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<h3 id="hung-yi-lee-李宏毅-machine-learning-2021-spring课程-hw2.2笔记">Hung-yi Lee (李宏毅) MACHINE LEARNING 2021 SPRING课程 HW2.2笔记</h3>
<p>​ https://link.zhihu.com/?target=https%3A//speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html</p>
<p>​ 先给出课程中助教提供的代码链接：</p>
<p>​ https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW02/HW02-2.ipynb</p>
<p>​ 先前在系列笔记2中——机器学习任务攻略笔记中，已经提及了在数学理论上，当我们训练不下去的时候，如何分辨是达到了一个Local Minimal还是一个Saddle Point。</p>
<p>​ https://zhuanlan.zhihu.com/p/447280599</p>
<p>​ 此篇笔记从实际的角度进行记录，通过一个实例，讲解如何去进行分辨，整理出的这个框架在今后如果遇到了训练不下去的时候，可以采用此计算，分辨到底是碰到了何种情况：</p>
<p>​ 首先，在实际的训练过程中，我们很难找到梯度为0的点，并且也很难找到一个真正的Local Minimal（也就是海森矩阵特征值全大于0），所以我们首先需要做一些近似：</p>
<ul>
<li><p>我们将gradient小于1^-3即视为近似为0</p></li>
<li><p>如果minimum ratio 大于0.5 且 gradient 小于 1^-3 ，我们即认为其为一个Local Minimal。</p></li>
<li><p>附： Minimum ratio = 海森矩阵的所有特征值中正的特征值的数量 / 海森矩阵的所有特征值的数量</p></li>
</ul>
<p>​ 然后就开始通过代码来实现计算过程：首先我们需要有一个用来作为示例的网络和训练点,这个在助教给的代码中已经准备好了，我此处把重要的东西抽出来记录</p>
<h4 id="step1-定义一个简单的模型">Step1: 定义一个简单的模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MathRegressor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_hidden=<span class="number">128</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.regressor = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">1</span>, num_hidden),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(num_hidden, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.regressor(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="step2-加载属于该模型的预先准备好的某训练点以及用于训练的数据和标签">Step2: 加载属于该模型的预先准备好的某训练点以及用于训练的数据和标签</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = MathRegressor()</span><br><span class="line">model.load_state_dict(<span class="string">&#x27;xxxx&#x27;</span>)  <span class="comment"># xxxx为事先准备好的训练到某一个点的模型文件路径</span></span><br><span class="line">train = Tensor([[<span class="number">0.2400</span>],  <span class="comment"># 然后feature和target(label)也加载进来</span></span><br><span class="line">        [<span class="number">0.2500</span>],</span><br><span class="line">        [<span class="number">0.2600</span>],</span><br><span class="line">        [<span class="number">0.2700</span>],</span><br><span class="line">        [<span class="number">0.2800</span>],</span><br><span class="line">        [<span class="number">0.2900</span>]])</span><br><span class="line">target = Tensor([[-<span class="number">0.1559</span>],</span><br><span class="line">        [-<span class="number">0.1801</span>],</span><br><span class="line">        [-<span class="number">0.1981</span>],</span><br><span class="line">        [-<span class="number">0.2101</span>],</span><br><span class="line">        [-<span class="number">0.2162</span>],</span><br><span class="line">        [-<span class="number">0.2168</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="step3-定义计算该训练进度当前所在位置loss函数的梯度">Step3: 定义计算该训练进度当前所在位置Loss函数的梯度</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># function to compute gradient norm</span></span><br><span class="line"><span class="comment"># model 代表模型实例 ，例如为 MathRegressor()</span></span><br><span class="line"><span class="comment"># criterion为损失函数,例如为 nn.MSELoss()</span></span><br><span class="line"><span class="comment"># train 为训练数据feature 见上</span></span><br><span class="line"><span class="comment"># target为训练数据label 见上</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_gradient_norm</span>(<span class="params">model, criterion, train, target</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    model.zero_grad()</span><br><span class="line">    output = model(train)</span><br><span class="line">    loss = criterion(output, target)</span><br><span class="line">    loss.backward()  <span class="comment"># 通过反向传播计算该点梯度</span></span><br><span class="line"></span><br><span class="line">    grads = []</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.regressor.children():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(p, nn.Linear):</span><br><span class="line">            param_norm = p.weight.grad.norm(<span class="number">2</span>).item()</span><br><span class="line">            grads.append(param_norm)</span><br><span class="line">		</span><br><span class="line">    grad_mean = np.mean(grads) <span class="comment"># compute mean of gradient norms</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad_mean</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">对于内容的一些注解：</span></span><br><span class="line"><span class="string">	前半部分比较好理解，就是做了前向计算以后，然后通过反向传播计算梯度</span></span><br><span class="line"><span class="string">	后半部分，其实是对所有的层的参数的梯度做了一个平均的计算：</span></span><br><span class="line"><span class="string">	1、model.regressor 输出的内容如下</span></span><br><span class="line"><span class="string">    Sequential(</span></span><br><span class="line"><span class="string">      (0): Linear(in_features=1, out_features=128, bias=True)</span></span><br><span class="line"><span class="string">      (1): ReLU()</span></span><br><span class="line"><span class="string">      (2): Linear(in_features=128, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">	2、代码中的p在三次循环中分别为：</span></span><br><span class="line"><span class="string">    第一遍循环：Linear(in_features=1, out_features=128, bias=True)</span></span><br><span class="line"><span class="string">    第二遍循环：ReLU()</span></span><br><span class="line"><span class="string">    第三遍循环：Linear(in_features=128, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">    故而我们知道 for p 那个循环其实就是在： 如果该层是全连接层，那么我们就要计算所有权重参数的梯度，如果不是（言外之意是如果是ReLU层，因为ReLU层是激活函数，无权重参数，所以我们不需要考虑）。所以需要注意，如果我们需要计算的模型是一个含有除了Linear以外的 带权重参数的网络模型，这个地方就要有所更改了</span></span><br><span class="line"><span class="string">  3、如果p是Linear层，应当如何计算：</span></span><br><span class="line"><span class="string">  	p.weight 是一个Tensor，为所有的权重参数</span></span><br><span class="line"><span class="string">  	p.weight.grad 是一个Tensor，为所有的权重参数对应当前点的导数（梯度）</span></span><br><span class="line"><span class="string">  	p.weight.grad.norm(2) 是一个仅有一个元素的Tensor，计算了先前的Tensor的二阶范式，其实就是平方和开更号。</span></span><br><span class="line"><span class="string">    p.weight.grad.norm(2).item() 是一个数值，即从Tensor中取出数值来。</span></span><br><span class="line"><span class="string">    最后将该层的 梯度的二阶范式 append 至 grads数组中</span></span><br><span class="line"><span class="string">  4、最后返回值是什么？</span></span><br><span class="line"><span class="string">  	最后grads中有 n个元素，n为所有带权重参数的层的个数（示例中的网络为2个），然后在用np.mean计算即得到最后的返回值，即所有层的梯度的平均值。</span></span><br><span class="line"><span class="string">  	</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h4 id="step4-计算hession-矩阵">Step4: 计算Hession 矩阵</h4>
<p>https://github.com/cybertronai/autograd-lib</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们使用autograd-lib 这个python库计算Hession矩阵</span></span><br><span class="line">pip install autograd-lib</span><br><span class="line"><span class="comment"># 在github的链接中，作者给出了Hessian矩阵的计算示例，其使用的是高斯牛顿法，我们在此不进行赘述，有兴趣的可以看github链接的计算源码。</span></span><br><span class="line"><span class="comment"># 以下是两个依赖函数，下面用得到，也是github源码中给出的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_activations</span>(<span class="params">layer, A, _</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    A is the input of the layer, we use batch size of 6 here</span></span><br><span class="line"><span class="string">    layer 1: A has size of (6, 1)</span></span><br><span class="line"><span class="string">    layer 2: A has size of (6, 128)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    activations[layer] = A</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper function to compute Hessian matrix</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_hess</span>(<span class="params">layer, _, B</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    B is the backprop value of the layer</span></span><br><span class="line"><span class="string">    layer 1: B has size of (6, 128)</span></span><br><span class="line"><span class="string">    layer 2: B ahs size of (6, 1)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    A = activations[layer]</span><br><span class="line">    BA = torch.einsum(<span class="string">&#x27;nl,ni-&gt;nli&#x27;</span>, B, A) <span class="comment"># do batch-wise outer product</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># full Hessian</span></span><br><span class="line">    hess[layer] += torch.einsum(<span class="string">&#x27;nli,nkj-&gt;likj&#x27;</span>, BA, BA) <span class="comment"># do batch-wise outer product, then sum over the batch</span></span><br></pre></td></tr></table></figure>
<h4 id="step5-计算minimal-ratio">Step5: 计算Minimal Ratio</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># function to compute the minimum ratio</span></span><br><span class="line"><span class="comment"># model 代表模型实例 ，例如为 MathRegressor()</span></span><br><span class="line"><span class="comment"># criterion为损失函数,例如为 nn.MSELoss()</span></span><br><span class="line"><span class="comment"># train 为训练数据feature 见上</span></span><br><span class="line"><span class="comment"># target为训练数据label 见上</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_minimum_ratio</span>(<span class="params">model, criterion, train, target</span>):</span></span><br><span class="line">    model.zero_grad() </span><br><span class="line">    <span class="comment"># 1、计算Hessian矩阵</span></span><br><span class="line">    <span class="keyword">with</span> autograd_lib.module_hook(save_activations):</span><br><span class="line">        output = model(train)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line">    <span class="keyword">with</span> autograd_lib.module_hook(compute_hess):</span><br><span class="line">        autograd_lib.backward_hessian(output, loss=<span class="string">&#x27;LeastSquares&#x27;</span>)</span><br><span class="line">		</span><br><span class="line">    layer_hess = <span class="built_in">list</span>(hess.values())</span><br><span class="line">    <span class="comment"># 2、计算minimum_ratio</span></span><br><span class="line">    minimum_ratio = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> layer_hess:</span><br><span class="line">        size = h.shape[<span class="number">0</span>] * h.shape[<span class="number">1</span>]</span><br><span class="line">        h = h.reshape(size, size)</span><br><span class="line">        h_eig = torch.symeig(h).eigenvalues </span><br><span class="line">        <span class="comment"># torch.symeig() returns eigenvalues and eigenvectors of a real symmetric matrix</span></span><br><span class="line">        num_greater = torch.<span class="built_in">sum</span>(h_eig &gt; <span class="number">0</span>).item()</span><br><span class="line">        minimum_ratio.append(num_greater / <span class="built_in">len</span>(h_eig))</span><br><span class="line"></span><br><span class="line">    ratio_mean = np.mean(minimum_ratio) <span class="comment"># compute mean of minimum ratio</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ratio_mean</span><br><span class="line">  </span><br><span class="line">activations = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">hess = defaultdict(<span class="built_in">float</span>)</span><br><span class="line">gradient_norm = compute_gradient_norm(model, criterion, train, target)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	在上述代码中，有些许部分与理论不同。代码实现中是分层计算海森矩阵的，即针对于神经网络的每个层计算海森矩阵，而后再求解特征值。然后将每层的结果做一个平均。而理论上，应当计算一个所有参数的大的海森矩阵，对其求解特征值。</span></span><br><span class="line"><span class="string">	即在上述的案例中，我们原本应该得到一个256*256的海森矩阵，但实际上得到了2个128*128的海森矩阵，并且应当是理论上256*256海森矩阵的左上角部分与右下角部分。即下述所示的A和B，剩下的两块的？是L对两层参数交叉求微分的部分，没有计算。</span></span><br><span class="line"><span class="string">	                              [ A   ? ]</span></span><br><span class="line"><span class="string">	                              [ ?   B ]</span></span><br><span class="line"><span class="string">	如此做的原因我思索了很久也没有得出一个结论，有两个合理的可能猜测：</span></span><br><span class="line"><span class="string">		可能1:完整的海森矩阵计算难度过大，故而我们单独计算L对每一层参数的海森矩阵，来作为近似。</span></span><br><span class="line"><span class="string">		可能2:由于某些性质，剩下的没有计算的两块的值，不会影响最终判定海森矩阵是否正定的结果，但是我并没有实际进行证实。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ML2021课程系列笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ML2021课程系列笔记3——卷积神经网络CNN</title>
    <url>/2021/12/20/ML2021%E7%AC%94%E8%AE%B0/ML2021%E8%AF%BE%E7%A8%8B%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</url>
    <content><![CDATA[<h3 id="hung-yi-lee-李宏毅-machine-learning-2021-spring课程-week3笔记">Hung-yi Lee (李宏毅) MACHINE LEARNING 2021 SPRING课程 Week3笔记</h3>
<p>### 课程链接：https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html</p>
<p>​ 该篇记录了一个用于影像处理的非常经典的网络架构：CNN网络，其中也提及了如何设计网络架构能够更好的为我们的机器学习任务目标进行服务。</p>
<h2 id="一从neuron-version角度来介绍cnn">一、从Neuron Version角度来介绍CNN</h2>
<p>​ 我们如果使用全连接网络来进行影像处理的话，如下图所示，我们先将一张100*100的3通道RGB图像展平，然后将展平后的向量作为全连接层的输入，假设第一个隐藏层有1000个神经å元，那么第一层的参数就需要有<span class="math inline">\(310^7\)</span>个，非常的多。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114729143.png" alt="image-20220114114729143" style="zoom:20%;" /></p>
<p>​ 所以考虑到影像本身的特性，其实我们并不需要让每一个neural和每一个像素之间都有权重的联系关系。所以我们需要基于影像的特点，提出一个适用于影像处理的架构CNN：</p>
<p>​ 首先我们提出如下的观察：</p>
<h3 id="观察1">观察1：</h3>
<p>​ 我们辨识影响往往是通过影响中的一些重要的部分（Pattern）来做出判断的，比如说，我们要判断一张图片中是不是含有鸟，我们往往可能会判断说有没有鸟嘴、有没有鸟眼睛、有没有鸟爪子等等重要的鸟的组成部分。这样的话呢，我们其实会发现，我们只要训练网络去观察这些重要的组成部分就可以了，而这些组成部分往往是图片的一小部分，所以并不需要每一个神经元都去观察整张图片的信息（FC网络由于每个神经元都和所有的输入像素相连接，所以每个神经元中都会有全局的信息），只需要让网络侦测一些重要的Pattern有没有出现即可。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114815294.png" alt="image-20220114114815294" style="zoom:25%;" /></p>
<p>​ 基于上述观察1，我们就可以提出一些机制（简化1）：Receptive Field感知域机制，如下所示：本来的话，在FC中，每一个Neural都会和整张图片的所有像素都有一个weight相连，但是我们现在可以设置一个3<em>3</em>3的感知域，比如图中左上角的这个红色感知域，然后某一个Neural仅和这个感知域内所有的像素相连，也就是这个神经元仅有3<em>3</em>3 = 27 个weight，再加上bias的参数，就可以得到神经元的输出作为下一层的输入。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114832378.png" alt="image-20220114114832378" style="zoom:25%;" /></p>
<p>​ 那么，以此类推，我们可以有很多很多的感知域，对应不同的Neural，每一个不同的Neural负责不同的感知域部分（如图中所示）。这个地方其实会存在比较多的疑问，比如说：同一个感知域可以连接两个不同的神经元吗？感知域之间可以重叠吗？我们一定要3通道的感知域吗？可以有不同尺度的感知域吗？感知域一定要是相邻的吗？感知域可以是非正方形吗？答案都是可以的，感知域的设计是一个非常自由的选项，其归根结底是需要看你对于问题的理解。一般而言，我们说感知域最好是相邻的，是因为我们认为在影响检测中，如果想要检测一个重要的Pattern的话，这个Pattern应该是会在图像任意位置都有可能出现的。如果有一个特殊的任务，其内容都在左上角，那么你可以设计感知域都重叠在左上角。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114850796.png" alt="image-20220114114850796" style="zoom:25%;" /></p>
<p>​ 那么最经典的Receipt Field应该是怎么样的呢？我们一般都会考虑所有的通道，而不是单个通道的考虑，这样的话我们只需要确定图像平面的感知域大小就可以，将之称为kernal size，一般的kernel size就是3<em>3，5</em>5和7*7已经属于较大的核大小了。然后对于一个receipt field来说，往往不会只有一个neural与之对应，而是会有一组neuron( 64或128个)去和这个感知域相连。</p>
<p>​ 各个不同的receipt field一般都是相邻的，有重叠的，相邻距离叫做stride。同时边界的填充大小叫做padding。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220121172659518.png" /></p>
<h3 id="观察2">2、观察2</h3>
<p>​ 我们发现，影像中同一个模式的部分，可能出现在图像的任何一个地方，比如说在不同的图像上，鸟嘴可能出现在蓝色方框处，也可能出现在红色方框处。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114115003492.png" alt="image-20220114115003492" style="zoom:25%;" /></p>
<p>​ 基于上述的观察，虽然我们在简化1中提到了，对于不同的区域，我们都会有对应的神经元去连接这一块区域（感知域），那么由于我们是要检测同一个模式，所以用于检测同一个模式的感知域的神经元应该是可以共享参数的。如下图所示，用于检测左上角的某模式的一个神经元核检测中部的同一模式的神经元应当共享训练参数。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114115020687.png" alt="image-20220114115020687" style="zoom:25%;" /></p>
<p>​ 那么一般而言，我们是如何去共享参数的呢？我们先前说过，一般而言，每一个感知域都会对应连接64个或128个不同的神经元，所有的感知域所对应的相同序号的神经元全都是共享参数的。以下图中两个红色方框框起的感知域为例，每个感知域都对应了64个神经元，即64组参数，它们各自的第1个神经元的参数（filter1）间参数是共享的，后续同理。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114115047789.png" alt="image-20220114115047789" style="zoom:25%;" /></p>
<p>​ 所以我们发现，Convolutional Layer其实就是人们针对于影像的两大特性，提出了感知域和参数共享这两个简化，从而使得FC进化成了CNN，所以CNN一般情况下我们说仅用于影像处理，或者说用于具有上述我们提及的两大特性的问题领域，才会有比较好的效果。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114115102350.png" alt="image-20220114115102350" style="zoom:25%;" /></p>
<h2 id="二从filter-vision角度来介绍cnn">二、从Filter Vision角度来介绍CNN</h2>
<p>​ 上述的那个角度介绍CNN其实是令我耳目一新的，它非常好的阐明了CNN这个架构之所以这样子设计的原因。那么其实在最开始的时候我学习的是接下来要说的这个从Filter的版本进行的介绍。</p>
<p>​ 首先你需要知道卷积的概念，在卷积层中，其实我们就是用许多个不同的卷积核Filter去卷积我们输入的图像。假设我们的图像是3通道的，那么我们一般的卷积核大小就是一个<span class="math inline">\(3 \times 3 \times 3(width*height*channel)\)</span>的三维Tensor。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114115134255.png" alt="image-20220114115134255" style="zoom:25%;" /></p>
<p>​ 每一个Filter卷积过原图像以后，都会生成一张新的图像，被我们称之为特征图，我们通常使用64个或128个卷积核去对图像卷积，在第一层的卷积层过后，就会生成一张64或128通道的图像，每一个通道都是一个Feature Map。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114115246463.png" alt="image-20220114115246463" style="zoom:20%;" /></p>
<p>那么在多层的卷积层网络中，这样子经过第一层卷积层后得到的一个特征图组成的64通道的图像，将会再次被视为一张图片，经过下一个卷积层。下一层的每个卷积核应当是<span class="math inline">\(3 \times 3 \times 64\)</span>的大小。然后又会根据卷积核的数目，决定该层输出的特征图的数量</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114115306217.png" alt="image-20220114115306217" style="zoom:25%;" /></p>
<h2 id="三-两个version的介绍中的相通点">三、 两个Version的介绍中的相通点</h2>
<p>​ 其实第二个版本中的Filter就是第一个版本中的某个感知域所对应的神经元的参数，也就对应着我们的观察1.如下所示：比如说一个对应着<span class="math inline">\(3 \times 3 \times 3\)</span>的感知域的神经元，其拥有27个参数，这27个参数其实就是我们在第二个版本中所说的Filter的大小。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114115437195.png" alt="image-20220114115437195" style="zoom:25%;" /></p>
<p>​ 同时，我们在卷积的过程中，需要拿卷积核遍历一遍图像，还会有stride和padding，在拿卷积核遍历卷积图像的时候，卷积核的参数不会变化，这也就意味着，对于同一个卷积核（神经元）来说，当其对应去计算不同的感知域时，参数是共享的。对应了我们的观察2.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114115455473.png" alt="image-20220114115455473" style="zoom:25%;" /></p>
<p>​ 所以其实来说两个版本的介绍是完全一致的，从两个方向来介绍能够更好的加深我们对于CNN的理解。</p>
<h2 id="四一些问题与pooling层的作用">四、一些问题与Pooling层的作用</h2>
<p>​ 1、我们一般采用<span class="math inline">\(3 \times 3\)</span>的filter，那么需要担心检测不到比较大的Pattern吗？因为有些Pattern肯定不会只在<span class="math inline">\(3 \times 3\)</span>这么小的一个区域内出现。</p>
<p>​ 答案是不需要担心，因为在多层的卷积神经网络中，我们第一层使用<span class="math inline">\(3 \times 3\)</span>的filter，得到的特征图，在第二层卷积的时候，第二层如果采用的是<span class="math inline">\(3 \times 3\)</span>的filter，我们可以知道，第二层卷积的特征图中的一个像素，其实代表的是原图像中一个<span class="math inline">\(3 \times 3\)</span>的范围，所以第二层的<span class="math inline">\(3 \times 3\)</span>的filter所感知到的源图像的范围其实是比<span class="math inline">\(3 \times 3\)</span>要大的，那就是<span class="math inline">\(9 \times 9\)</span>吗？也不一定，因为考虑到感知域之间会有所重叠。如下图所示，比较好理解，上面的矩阵是原图，下面的矩阵是第一层卷完以后的特征图，在特征图上做<span class="math inline">\(3 \times 3\)</span>的蓝色框卷积，其实就相当于感知到了原图的<span class="math inline">\(5 \times 5\)</span>的一个蓝色方框区域。所以卷积神经网络叠的越深，其能侦测到的特征的尺度上限越大。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114115621526.png" alt="image-20220114115621526" style="zoom:25%;" /></p>
<p>​ 2、Pooling层：</p>
<p>​ 池化层基于这样一个观察：我们发现，影像如果做了下采样，并不会影响整体显示的内容。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114115654899.png" alt="image-20220114115654899" style="zoom:25%;" /></p>
<p>​ 所以在传统的CNN中，Pooling最主要的作用是减少运算量，减少参数。人们往往把Pooling层和卷积层交替叠加，形成卷积神经网络。但其实在现阶段而言，我们的GPU运算能力上来了，池化层在有些时候可有可无，尤其是当我们需要执行一些识别细小Pattern的任务的时候，池化层所进行的下采样很可能让我们丢失一些信息，导致侦测不到一些细小的Pattern。</p>
<p>​ 故而我们在使用一个网络架构的时候，一定要想清楚这个网络架构的某些部分到底是干什么用的，针对什么特性来进行设计的，才能获得比较好的效果以及应用。</p>
<p>3、最后CNN无法处理图像的放缩与旋转，所以在做影像辨识的时候我们往往要做Data Augmentation（先前提及过的方法），来让我们的训练集尽量多元化，避免这个问题。当然也有专门做了相应处理的网络层，叫做Spatial Transformer Layer，感兴趣可以学习。 https://youtu.be/SoCywZ1hZak</p>
<h2 id="五cnn应用-playing-go">五、CNN应用： Playing GO</h2>
<h4 id="如何将下围棋变成一个分类问题">1、如何将下围棋变成一个分类问题？</h4>
<p>​ 我们把下围棋去当作一个分类的问题，假设棋盘是19*19的大小，网络的输入就是一个19*19的向量，每一个位置可能用一个值来代表这个位置当前的状态，（Alpha Go原论文中，每个位置是由一个48维的向量进行表示的），经过网络以后，我们要的输出结果是下一步棋会下在哪里？</p>
<p>​ 这就是一个 19*19个类别的分类问题，网络输出在各个地方的落子概率，挑选落子概率最大的那一个位置下子就可以了。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_21_17_35_31_200.png" /></p>
<h4 id="为什么cnn可以用在下围棋上呢">2、为什么CNN可以用在下围棋上呢？</h4>
<p>​ 1、一些pattern比整个棋局要小，并且可以识别出来。如下所示，围棋中也有一个个小的pattern，通过一个个小的pattern就可以去进行一些的判断。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_21_17_32_54_32.png" /></p>
<p>​ 2、同一个pattern可能出现在棋盘上的任何位置，可能可以出现在左下角，也可以出现在右上角。围棋中也是这个样子。</p>
<h4 id="那么pooling层在围棋中也会适用吗">3、那么Pooling层在围棋中也会适用吗？</h4>
<p>​ ！完全不适用！并且不可以使用！如果我们在棋盘中使用MaxPooling层的话，移除了偶数行和奇数行以后，会导致很多围棋的细节丢失。所以Pooling层虽然适合做影像处理，但在下围棋这个任务中，显然不是很合适。</p>
<p>​ 所以Alpha Go的原论文中，在附件中阐述的网络结构中也提到了，其设计的网络结构中，已经没有Pooling层了，是非常合理的。</p>
]]></content>
      <categories>
        <category>ML2021课程系列笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>ML2021课程系列笔记2——机器学习训练任务攻略</title>
    <url>/2021/12/19/ML2021%E7%AC%94%E8%AE%B0/ML2021%E8%AF%BE%E7%A8%8B%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1%E6%94%BB%E7%95%A5/</url>
    <content><![CDATA[<h3 id="hung-yi-lee-李宏毅-machine-learning-2021-spring课程-week2笔记">Hung-yi Lee (李宏毅) MACHINE LEARNING 2021 SPRING课程 Week2笔记</h3>
<h3 id="课程链接httpsspeech.ee.ntu.edu.twhyleeml2021-spring.html">课程链接：https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html</h3>
<p>​ 本篇将会介绍一些机器学习深度学习训练过程中，容易发生的一些问题，以及经常使用的一些基础的Tricks。上篇已经阐明了机器学习任务的训练分为以下三个阶段。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114110835052.png" alt="image-20220114110835052" style="zoom:25%;" /></p>
<p>​ 同时也会有对应的测试集数据进行验证的部分。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114110905969.png" alt="image-20220114110905969" style="zoom: 25%;" /></p>
<p>​ 那么在训练与测试的过程中，可能会出现一些问题，下面的图就是一个整体的Guidance，基本阐述了当在训练时我们遇到不同的现象时，大概是何种问题所致。</p>
<p>​ 此篇将会顺着以下这个分支树进行前序遍历，一条条往下进行讲解。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114110925311.png" alt="image-20220114110925311" style="zoom:25%;" /></p>
<h2 id="一model-bias问题">一、Model Bias问题</h2>
<p>​ 当在训练数据上得到的Loss函数值较大的话，其中一种可能就是Model Bias.Model Bias一般指模型不够复杂，无法很好的拟合数据。从形式化的角度来讲就是，能够拟合数据的函数，不在你所寻找的函数空间中。举个例子来说：你在第一步中设定的拟合函数为Y = a + bx，然后使用梯度下降想要寻找使得Loss最低的一组a和b，但是你会发现，即使你找到了使得Loss最低的一组a和b，这组a和b计算得到的Loss还是很高。原因可能是，你需要拟合的数据其实是一个二次函数，或是更复杂的函数，而你却想要使用线性函数去进行拟合。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220129151628264.png" /></p>
<h2 id="二optimization">二、Optimization</h2>
<p>​ 当在训练数据上得到的Loss函数值较大的话，另外一种可能就是Optimization导致的。此种情况与第一种不同，其模型是有能力拟合数据的，但是由于优化方法的限制，我们没法找到能够使得Loss函数值最小的那一组未知参数。我们先前使用的Gradient Descent用于求解优化的方法，还是存在比较多的限制的。如下图为例，假设Loss函数的图像如下图所示，我们设定的θ的起点在左侧，那么经过一段时间的梯度下降后，其会到达图中蓝线所示的Local Minimum，也就是局部最优，但是其永远也无法找到右侧的全局最优。往往优化算法无法很好的执行是由于模型过于复杂，或不同的特征数据的尺度相差过大等原因导致。</p>
<p>​ 此处仅先简单的介绍Optimization是怎样一种问题，后续第八章中会继续探讨这类问题的产生原因以及在第九章中讲如何解决优化这一问题（如何逃出Shaddle Point）。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114111434996.png" alt="image-20220114111434996" style="zoom: 25%;" /></p>
<h2 id="三如何区分是model-bias还是optimization导致的train-loss过大">三、如何区分是Model Bias还是Optimization导致的Train Loss过大？</h2>
<p>​ 我们先前说，当在训练数据上得到的Loss函数值较大的话，这两种问题都有可能，那么一般而言如何区分呢？一般来说，我们需要通过模型与模型的比较来获得一些见解。如下所示，在右侧的Training Data显示的图像中，20层的网络已经能够将Training Error降至比较低的水准了，那么56层的这个网络Training Error还处于一个较高的水准，那么肯定是Optimizaiton问题，而不是Model Bias的问题。为何这么说呢？因为56层的网络肯定比20层的网路复杂，如果20层的网络已经能够很好的拟合训练数据了，那么56层的网络肯定是具有拟合训练数据的能力的，但是现在Training Error还居高不下，说明是优化的时候出现了问题，或者是进入了一个局部最优点中，导致其找不到最好的那一组参数。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114111512628.png" alt="image-20220114111512628" style="zoom:25%;" /></p>
<h2 id="四训练时能够避免上述情况发生的一些tips">四、训练时能够避免上述情况发生的一些Tips？</h2>
<p>​ 一般新执行一个训练任务的时候，从较浅的网络开始训练，因为一般较浅的网络不会出现优化的问题。如果发现较浅的网络复杂度不够拟合数据，再逐渐加深网络。</p>
<p>​ 如果更深的网络没有获得更小的损失训练数据，则存在优化问题。</p>
<h2 id="五overfitting问题">五、OverFitting问题</h2>
<p>​ 当Training Loss已经降至较低水平，但是在Testing集上的Loss值仍然较高，其中一种可能就是出现了Overfitting过拟合的问题。如下图所示，很好的表现了过拟合问题。由于我们的模型非常有弹性，所以可能在训练集上（蓝色点）时非常好的拟合了，但是在测试集（橙色）上的表现就会比较差。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114111606224.png" alt="image-20220114111606224" style="zoom:25%;" /></p>
<p>​ 更为极端的情况就是训练出如下图所示的函数：在训练集上的数据，都是完全拟合的，但是对于不在训练集上的数据就是一个随机值。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114111622606.png" alt="image-20220114111622606" style="zoom: 15%;" /></p>
<h2 id="六如何解决overfitting问题">六、如何解决OverFitting问题</h2>
<p>​ 下述是一个随着模型逐渐变得复杂，Training Loss和Testing Loss 的值变化曲线。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114111700084.png" alt="image-20220114111700084" style="zoom:16%;" /></p>
<p>​ 对于Overfitting我们有如下的几种解决方案：</p>
<ul>
<li><p>1、更多的训练数据</p>
<ul>
<li><p>如果我们能够收集到更多的训练数据，就可以一定程度上减轻过拟合的问题。比较常用的增加训练数据的方法，叫做Data Augmentation. 如下图所示，比如说在一个图像检测的任务中，我们最开始数据集里的图片只有最左侧一张图片，我们可以将其左右翻转、或者进行放缩，就能够形成新的一些训练数据，来扩充我们的数据集。然而，上下翻转形成的新的图像，一般不会被我们放入数据集中，因为它并不是正常形成的图片，如果加入数据集中很可能让机器学到奇怪的内容。因此，Augmentation也不是随意的对数据进行变换，是需要有一定依据的。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114111733247.png" alt="image-20220114111733247" /><figcaption aria-hidden="true">image-20220114111733247</figcaption>
</figure></li>
</ul></li>
<li><p>2、减少未知参数或共享部分参数：</p>
<ul>
<li>减少模型的复杂度，或者使得模型中某些层共享一些参数，能够一定程度上解决Overfitting的问题。</li>
</ul></li>
<li><p>3、使用更少的特征</p>
<ul>
<li>将一些对于结果影响较为不显著的特征进行丢弃，只保留一些关键重要的特征，也可以一定程度上解决Overfitting。例如：我们要训练一个模型预测车的价格，根据车的如下特征：车大小、车颜色、车品牌、车动力等。其实，在这么多特征中，车颜色应该是对价格影响较少的，所以我们在训练时可以将这个特征舍去，不参与训练。</li>
</ul></li>
<li><p>4、Early Stopping机制（在Week1的Hw中已经提及）</p></li>
<li><p>5、Regularization正则化技术（见后）</p></li>
<li><p>6、DropOut技术（见后）</p></li>
</ul>
<h2 id="七如何避免在training-data上表现很好但是testing-data上表现很差的问题cross-validation交叉验证技术">七、如何避免在Training Data上表现很好但是Testing Data上表现很差的问题——Cross Validation交叉验证技术？</h2>
<p>​ 我们将Training Set分割为Training Set和Validation Set两个集合，一般而言比例为9：1。我们使用划分后的Training Set进行训练，在每个Epoch结束后使用训练期间机器没有见到过的Validation进行验证，依据验证集得到的Loss值来进行模型好坏的衡量。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114111856862.png" alt="image-20220114111856862" style="zoom:15%;" /></p>
<p>​ 那么该如何进行划分呢？一般来说随即划分即可。但随即划分也可能带来一些问题，比如说可能训练集中的数据都偏向于某一类，而验证集的数据偏向于另一类，就可能导致Validation出现问题.故而又出现了N-Fold Cross Validation.如下图所示,我们将Training Set分为N个集合(示例中为3个),其中N-1个集合用于训练,1个集合用于验证,然后每轮Epoch中,都执行N遍,每一遍都拿不同的集合用于训练与验证,然后计算一遍Loss值,最终选取平均Loss最小的那一组参数进行模型的更新.</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114111918585.png" alt="image-20220114111918585" style="zoom: 25%;" /></p>
<h2 id="八optimization问题产生的原因small-gradient">八、Optimization问题产生的原因：Small Gradient</h2>
<p>​ 我们先来看一下优化无法进行的根本原因是什么？无论是训练了一段时间以后Loss无法再下降，还是一开始Loss就无法下降，其终极原因就是达到了梯度接近于0的点，我们通常称为critical point。注意，此处千万不能说是碰到了local minimum，因为这样子会非常不严谨。Critical point 不仅仅指Local Minimum，还有可能是 Saddle Point（鞍点）等情况。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112021653.png" alt="image-20220114112021653" style="zoom:15%;" /></p>
<p>​ 所以我们首先要知道，当我们遇到一个Critical Point的时候，需要区分这到底是一个Local Minimum还是一个Saddle Point。因为如果是一个局部最小值，那么其实就已经无路可走了，但是如果是一个Saddle Point的话，周边仍然是有比他低的值的。还是有办法可以解决一番.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112044178.png" alt="image-20220114112044178" style="zoom:25%;" /></p>
<p>​ <span class="math inline">\(L(\theta)\)</span>函数本身会是一个非常复杂的函数,我们没法知道它的全貌,但是我们可以动用如下的数学手段来判断某一个Critical Point到底是Local Minimum还是Saddle Point,如下所示:</p>
<p>​ 利用泰勒展开,我们可以知道<span class="math inline">\(L(\theta)\)</span>在某一个点<span class="math inline">\(\theta^{&#39;}\)</span>周围的函数的近似值: <span class="math inline">\(g\)</span>为梯度,是<span class="math inline">\(L\)</span>对<span class="math inline">\(\theta\)</span>的一阶导数组成的向量.<span class="math inline">\(H\)</span>为海森矩阵,为<span class="math inline">\(L\)</span>对<span class="math inline">\(\theta\)</span>的二阶微分组成的矩阵.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112216322.png" alt="image-20220114112216322" style="zoom:15%;" /></p>
<p>​ 我们知道,在Critical Point上,梯度为0,所以我们可以将上述公式中中间那一项划掉不看,那么L(θ)在θ’周围的函数值就可以近似为L(θ’) 加上后面这一项.</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112238183.png" alt="image-20220114112238183" style="zoom:25%;" /></p>
<p>​ 因此,我们其实可以通过后面红色方框内的这一项的正负,来判断θ’周围的值的分布到底是如下图所示的三种中的哪一种:</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112259806.png" alt="image-20220114112259806" style="zoom: 25%;" /></p>
<p>​ 为了后续的数学表示,我们先将<span class="math inline">\((\theta - \theta^{&#39;})\)</span>表示为符号<span class="math inline">\(v\)</span>,那么如下图所示: 上述近似式子中的红色方框的那项,值有如下三种情况:</p>
<p>​ 如果对于所有的<span class="math inline">\(v\)</span>来说,该项恒大于0,那就代表在<span class="math inline">\(\theta^{&#39;}\)</span>周围,所有的值都比该点大,该点即为局部最小值.但是问题在于我们没法验证所有的<span class="math inline">\(v\)</span>,那应该如何呢?</p>
<p>​ 已经有人为我们证明了, 对于所有<span class="math inline">\(v\)</span>,<span class="math inline">\(v^THv\)</span>恒大于0这件事情,和海森矩阵H是一个正定矩阵 是等价的, 也等价于 H矩阵的所有特征值都是正的这件事情.</p>
<p>​ 所以理论上我们如果想要知道<span class="math inline">\(v^THv\)</span>的情况,知道θ’周围的分布,那么其实只要计算出H矩阵的所有特征值,看一下分布即可.</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112443208.png" alt="image-20220114112443208" style="zoom:25%;" /></p>
<p>​ 那么如果我们发现这个点是一个Saddle Point的话, 应当如何去继续优化呢? 我们可以利用H进行优化, 我们找到一个矩阵H的特征向量u, 其对应的特征值为λ, λ&lt;0 ,那么如下图所示: <span class="math inline">\(u^THu\)</span>这一项就应当为 &lt; 0 的值.</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112505138.png" alt="image-20220114112505138" style="zoom:15%;" /></p>
<p>​ 故而: 我们令<span class="math inline">\((\theta - \theta^{&#39;}) = u\)</span>即可得到一个比<span class="math inline">\(L(\theta^{&#39;})\)</span>小的值,完成了L值的优化.</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112600293.png" alt="image-20220114112600293" style="zoom: 25%;" /></p>
<p>​ 从执行上来讲,其实就是找一个H矩阵的特征值为负的特征向量u,这个向量u就指明了梯度下降的方向,沿着该方向走一小步,就可以完成Loss函数的值的优化.</p>
<p>​ 到此为止,已经记录了Saddle Point和Local Minimum怎么去区分, 并且也发现Saddle Point并不可怕,可以通过计算H矩阵来进行求解.</p>
<p>​ 在实际我们的训练过程中,其实达到Local Minimum的机会是很少的,基本上都会停留在Saddle Point上,那么这是不是就意味着我们如果碰到了Critical Point也就是梯度为0的点,基本上就可以用求解H来继续训练呢? 理论上应该是可以的,但是在实际操作中,其实我们并不会去这样子做,因为计算H矩阵所需要花费的时间代价太过昂贵了.我们会有其他的训练技巧,来帮助绕过这一些Saddle Point. 这些内容在下面的第9章中会讲述。同时最后的话还需要提及一点，其实大部分的训练优化做不下去了，并不是由于到达了Critical Point，而是由于学习率的原因，在一个山谷的两侧来回震荡，这一点会在10章中进行详细的叙述。</p>
<h2 id="九optimize训练tips-batch-and-momentum">九、Optimize训练Tips: Batch and Momentum:</h2>
<h4 id="batch技术">1、Batch技术</h4>
<p>​ 先前我们说过在训练的过程中，每次进行梯度下降并不是参考所有的训练数据来计算Loss函数的，而是会像下图一样通过一个Batch一个Batch的进行计算。我们在每一轮Epoch之前，一般而言都会对数据集重新划分Batch，也就是说在不同的Epoch中，每一个Batch都是不一样的，这件事情叫做Shuffle。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112658511.png" alt="image-20220114112658511" style="zoom:25%;" /></p>
<p>​ 接下去的问题就是我们为什么要使用Batch呢？考虑如下的一个例子，左侧的Full batch就是每一次机器都要看过所有的数据集以后再更新参数，而右侧的Batch Size = 1则是会在看过每个样例以后都更新一遍参数。从直观上来看，BatchSize越大，其就更新一次参数而言所需要花费的时间越大，但是其更新更有效。那么按照这样子说的话，我们为什么还要分Batch呢？实际状况跟我们的直觉有一些不同。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112717272.png" alt="image-20220114112717272" style="zoom:25%;" /></p>
<p>​ 下面这张图显示了单次update以及一轮epoch训练的时间和BatchSize的关系，我们发现单次update的时间，对于batchsize = 1和=1000来说，竟然差别不大。导致这个的原因是我们考虑了并行计算的因素，也就是使用GPU进行运算的原因。当然，如果BatchSize再增长下去，我们的GPU也是有一定的上限的所以后面也会有较大的一个时间增长。那么，右侧的这幅图就比较好理解了，对于一轮Epoch来说，如果我们的Batch Size比较小的话，在一轮Epoch中，更新Update次数就会比较多，相对而言整体的一轮Epoch的耗时就会较长。所以随着Batch Size的增大，反而一轮Epoch的时间会变长。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112747946.png" alt="image-20220114112747946" style="zoom:25%;" /></p>
<p>​ 这样看来，BatchSize越大，反而训练速度越快。这样的话BatchSize越大，速度又快，更新又准确，那为什么还要分Batch呢？下面一张图表示了BatchSize大小对训练出的网络的准确率的影响，我们可以看到，在BatchSize较大的时候，准确率直线下降，这是为什么呢？原因在于：我们先前所说BatchSize较大的话，每次Update较为准确，没有问题。但是对于网络而言，并不是一次Update就能解决问题的。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112807695.png" alt="image-20220114112807695" style="zoom:23%;" /></p>
<p>​ 如果BatchSize过大，会导致优化问题的出现，如左图所示，原因就是因为其每次update都太准确，如果遇到 critical point或者是local minimum，那么其就会深陷其中，无法继续优化了。如过分了Batch的话，由于每次使用的是不同的Batch，所以每次的L函数都会有些许不一样，如果在L1处到达了一个critical point，在一个Batch训练的时候，也许该点又重新可以继续梯度下降训练了。所以，Batch Size较小的时候带来的单次更新的”Noisy”反而对整体的网络训练能够带来好处。那么，如何衡量就是一个需要我们手工调整的问题了。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112827827.png" alt="image-20220114112827827" style="zoom:27%;" /></p>
<p>​ 最后，先辈们还发现了一个有趣的现象，就是如果我们使用BatchSize不一样的训练方法，训练同一个网络，将Training Loss训练到一样好以后，我们将其在测试集上进行测试，往往是BatchSize小的训练出来的网络，准确率较高。大致原因如下图所示：large batch容易让网络的参数进入一些Sharp Minima峡谷中，这样子的话，一旦训练数据集和测试数据集的分布有些许不同，在训练集上处于很低点的参数应用到测试集上可能会有一个比较大的Loss。相比较而言，small batch往往会跳出这种sharp minima的峡谷，找到一些较为平坦的flat minima，对于这些flat minima的参数来说，即使训练集与分布集的数据分布不同，表现出来的loss性能也不会相差太多。有兴趣的可以研读这篇paper.《On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima 》</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112849800.png" alt="image-20220114112849800" style="zoom:25%;" /></p>
<h4 id="momentum技术">2、Momentum技术</h4>
<p>​ 除了使用Batch来帮助我们训练以外，还有一种借鉴了物理世界的Momentum技术，可以帮助我们走出一些Critical point。</p>
<p>​ 如下图所示，在梯度下降中，也许我们训练至一些critical point，参数就不会继续优化了，但是在物理世界中，如果一个小球从高处滚下，其将会靠着动量滚下saddle point，也有可能靠着动量滚出local minimum。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112915300.png" alt="image-20220114112915300" style="zoom:25%;" /></p>
<p>​ 如下图所示是我们普通梯度下降的算法示意图：从<span class="math inline">\(\theta_0\)</span>开始计算梯度，然后沿着梯度反方向移动下降，一步步进行优化。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114112942271.png" alt="image-20220114112942271" style="zoom:25%;" /></p>
<p>​ 那么在Gradient Descent + Momentum的算法如下：最开始第一步和原来的一样，从<span class="math inline">\(\theta^0\)</span>开始计算梯度，然后沿着梯度反方向移动下降，达到<span class="math inline">\(\theta^1\)</span>时，此时和原先就会发生不同了，其现在的移动会结合前一步的movement（即<span class="math inline">\(m^1\)</span>）以及当前点的梯度<span class="math inline">\(g^1\)</span>，计算出一个新的下降方向<span class="math inline">\(m^2\)</span>，然后进行更新。如图所示：<span class="math inline">\(m^2\)</span>是由 <span class="math inline">\(m^1\)</span>和 <span class="math inline">\(-g^1\)</span>两个向量相加所得到的。从公式上来讲就是$m^1 - g^1 $，两者都有自己的参数，来控制影响整个梯度下降方向的比例。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113022761.png" alt="image-20220114113022761" style="zoom:25%;" /></p>
<p>​ 其实从另一个角度来看，mi其实是先前所有的梯度的和，也就是g0 - gi-1的和。所以我们可以说Momentum是考虑了上一步的movement，也可以说Momentum是考虑了先前所有更新点的梯度值。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113348655.png" alt="image-20220114113348655" style="zoom:15%;" /></p>
<h2 id="十optimize训练tips-automatic-learning-rate">十、Optimize训练Tips: Automatic Learning Rate</h2>
<p>​ 其实，当我们训练卡住的时候，并不一定是碰到了梯度较低的地方，很有可能是在某个山谷的两侧由于学习率过大的问题来回震荡。那么这种现象是怎么产生的呢？</p>
<p>​ 在Week1的HW1笔记中，曾经记录数据预处理归一化的重要性，否则很有可能因为不同特征的数据尺度相差太大导致震荡，从而导致训练不出结果，如下图所示：</p>
<p>​ 下图是一个Error Surface,就是Loss函数的平面，我们使用两种不同大小的学习率去进行梯度下降，黄色的叉叉代表训练终点。我们发现整个Error Surface中，上下维度的梯度比较陡，左右维度的梯度比较缓。如果我们采用较大的学习率，就会产生左图这样子的黑色训练线，在竖直方向疯狂震荡。但是如果我们使用如右图所示的较小的学习率，一开始是可以沿着竖直方向的梯度慢慢更新，但是当要开始沿水平方向更新的时候，会发现学习率太小了，以至于更新了10w次，还距离我们的训练重点遥遥无期。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220121170751170.png" /></p>
<p>​ 所以，我们发现，一个固定的学习率在训练中是不太合适的，最好的话是如图所示，要对不同的参数有不同的学习率，学习率要随着梯度的变化以及参数的变化而进行变化。</p>
<p>​ 于是我们首先提出了如下想法，将原来的学习率η修改成 η/(σit), 这个σit既跟参数相关又跟训练步骤（不同点所在的梯度）相关。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220121170754841.png" /></p>
<p>​ 以下就是一种最为简单的σ的计算方式，通过计算每次更新得到的参数空间所在点的梯度的Norm值的平方的平均，即RMS来计算每步中σ。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113547513.png" alt="image-20220114113547513" style="zoom:25%;" /></p>
<p>​ 这种方法被用在Adagrad中，主要思想就是，如果Loss函数在某个方向上比较平坦,梯度比较小，那么我们希望Learning Rate比较大，快速的走过这一片平坦的区域。如果在某个方向上比较陡峭，我们希望Learning Rate比较小.</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113613777.png" alt="image-20220114113613777" style="zoom:25%;" /></p>
<p>​ 接下来一种较为进阶的做法叫做RMSProp，它在计算每一步的σ的时候，结合了上一步的σ以及该步的梯度g，同时还有一个超参数α，可以进行调整。如果我们调整α比较大的话，代表其参考当前的梯度较多，也就是说如果梯度突然产生较大变化，其就能快速的反应过来，对LearningRate进行快速的调整。相较于前RMS所有先前的梯度都平均权值考虑的做法，这一做法能够更快速的对梯度的变化进行响应。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113634056.png" alt="image-20220114113634056" style="zoom:25%;" /></p>
<p>​ 如下所示，是一个较为公式化的写法，以及一个简单的示意图。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113649974.png" alt="image-20220114113649974" style="zoom: 25%;" /></p>
<p>​ 其实按照这种方法，进行训练的话，可能出现如下图所示的训练曲线：为何训练时过一段时间就会突然上下暴走呢？因为其σ是累积平均得到的，在先前的一段时间里，上下方向的更新梯度都近乎为0，累积一段时间以后，上下的梯度就会很小，然后学习率就会变得很大，但是马上又会因为我们的机制更新学习率回来，从而只会出现这样的震荡一下的现象，而不是一直的上下乱窜。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113710215.png" alt="image-20220114113710215" style="zoom:25%;" /></p>
<p>​ 在自动修改学习率的方法中，还有一种形式叫做Learning Rate Scheduling：它在原先的基础上对η也进行了一些调整。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113737186.png" alt="image-20220114113737186" style="zoom:8%;" /></p>
<p>​ 较为常见的就是Learning Rate Decay这种，随着优化的更新深入，我们使得η逐渐变小。这种想法也是比较能够理解的。类似于越往后快要到达终点了，我们就越要慢慢的靠近，精细的调整，而一开始可以以较大的速率进行更新。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113805989.png" alt="image-20220114113805989" style="zoom:25%;" /></p>
<p>​ 也有一些网络会用到类似下面的Warming Up的黑科技：</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113828593.png" alt="image-20220114113828593" style="zoom:25%;" /></p>
<h2 id="十一综合各种训练技巧的优化器adam">十一、综合各种训练技巧的优化器：Adam</h2>
<p>​ Adam是一种综合了RMSProp和Momentum技术的优化器，其大致算法如下图所示，在此处就不多赘述。原论文网址：https://arxiv.org/pdf/1412.6980.pdf</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113903161.png" alt="image-20220114113903161" style="zoom:33%;" /></p>
<h2 id="十二改变loss函数也有可能影响训练">十二、改变Loss函数，也有可能影响训练：</h2>
<p>​ 此部分通过描述分类任务，来顺带阐明改变Loss函数的形式，也有可能影响训练这样一件事情。下图是回归与分类的任务一些差别，在分类任务中，最终的结果往往是一个类别，而不是一个数值，同时，类别往往会由One-hot向量进行表示。所以，我们分类任务所输出的内容往往是一个多个数值的向量y。然后我们需要比较y和 真实类别的标签直接的差别，尽可能的最小化这个差别即可。那么，由于标签往往都是One-hot 向量，所以我们为了比较y和标签的值，往往都会先对y做一个softmax函数进行处理，将y中的所有值归约化至0-1之间，并且所有值的和为1.</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114113948987.png" alt="image-20220114113948987" style="zoom:23%;" /></p>
<p>​ Soft-Max内部对输入进行的处理如下图所示，我们假设输入y1,y2,y3，我们先将三个输入计算exp(y1),exp(y2),exp(y3),然后求和，然后计算输出。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114013901.png" alt="image-20220114114013901" style="zoom: 25%;" /></p>
<p>​ 那么在得到归一化后的y’之后，我们如何去计算y’和label y 的差距呢？按照先前回归的想法是计算两个向量之间的MSE，也就是如下图所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114036461.png" alt="image-20220114114036461" style="zoom:25%;" /></p>
<p>​ 但其实一般而言，我们在分类任务中，都会采用Cross-Entropy作为Loss函数，即如下所示的公式：</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114056586.png" alt="image-20220114114056586" style="zoom:9%;" /></p>
<p>​</p>
<p>​ 一个简单的解释是，最小化Cross-Entropy和最大化似然概率是完全等同的一件事情（至于为什么完全等同，可以找相关资料），分类任务就是要找出计算得到的y属于哪一类的概率最大，所以我们往往采用Cross-Entropy作为Loss函数。而计算Cross-Entropy之前，我们往往就是要对网络的输出进行SoftMax函数的处理。所以往往SoftMax会和Cross-Entropy绑定在一起，有趣的是Pytorch的底部实现中，如果你的Loss函数使用了Cross-Entropy，那么你在网络结构中是不需要实现SoftMax层的，因为它会自动将SoftMax层加到你的网络的最上面。</p>
<p>​ 如下所示是一个比较直观的结果：当我们采用不同的Loss函数的时候，Error Map也会截然不同。左侧采用MSE的Error map在左上角的区域上（也就是Large Loss）的区域上，梯度就很平坦，就很难训练，而采用Cross-Entropy则会好训练很多。这也就是为什么说有的时候改变Loss函数，也可以改变训练的难度。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114125892.png" alt="image-20220114114125892" style="zoom:25%;" /></p>
<h2 id="十三feature-normalization-batch-normalization-技术">十三、Feature Normalization: Batch Normalization 技术</h2>
<p>​ 我们先前提及过，当训练数据不同维度的特征的尺度相差较大时，训练难度会较大，那么Feature Normalization就是为了使得不同特征的尺度类似，从而使得Error Surface较为平滑。如下图所示：左侧的Loss函数平面，不同特征的梯度变化不同，有Smooth和Steep两类不同的内容。右侧就是做了一个Feature Normalization以后的Error Surface，各个维度都较为平滑。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114217069.png" alt="image-20220114114217069" style="zoom:25%;" /></p>
<p>​ 那么最为平常的特征正则化，如下所示：比如说给出一组x向量，我们计算向量中每个维度的平均值与方差，然后通过如下公式进行计算，我们就可以将所有维度的数据归约化至一个mean为0，deviation为1的一个分布中，使得大致的数据尺度差不多。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114244676.png" alt="image-20220114114244676" style="zoom:25%;" /></p>
<p>​ 上述的归约化处理在HW1的代码中也已经有所体现，那么先前我们在说的是数据预处理步骤中，对于所有数据的归约化。那么对于深度学习来说，可能存在如下问题：</p>
<p>​ 如下图所示，输入的特征经过归一化以后，在第一层的训练参数是比较正常的，但是经过第一层的参数以后，输出的不同维度的数据又可能回到了不同尺度这样一种状态。这样的话，对于第二层的网络来讲，参数优化又会变得较为困难。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114310723.png" alt="image-20220114114310723" style="zoom:25%;" /></p>
<p>​ 所以我们需要在层与层之间也进行归一化的操作，如下示例来讲，就是通过z1,z2,z3计算出u和σ。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114331145.png" alt="image-20220114114331145" style="zoom:25%;" /></p>
<p>​ 然后我们再将z1,z2,z3根据公式来进行归一化的操作，得到归约化后的z1,z2,z3。这边的话其实有一个有趣的内容：原本来说，如果参数更新了，z1更新了，那么只会影响a1的改变，但是现在而言，加入了BN以后，z1更新了以后，会影响到u和σ,从而会再影响归约化后的z1,z2,z3，随后a2,a3也会受到影响。所以，当我们加入归约化的时候，就需要将整个考虑成一个大的网络。所以就把BN做成一个Batch Normalization层。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114355882.png" alt="image-20220114114355882" style="zoom:25%;" /></p>
<p>​ 并且，我们在训练过程中，由于输入数据往往是一个Batch一个Batch输入的，所以训练时计算的u和σ我们往往不会去计算所有的训练数据，而是计算该Batch内部的所有数据的u（向量）和σ（向量）。故而，这个技术叫Batch Normalization。</p>
<p>​ 所以，Batch Normalization往往需要在Batch比较大的时候才会可以使用，会比较好一些。不然的话计算得到的u和σ并不能代表训练数据的一些分布，归约化就会出现一些问题。同时，再归约化时也会有一些小的问题如下：</p>
<p>​ 因为我们归约化至了mean = 0, deviation=1的数据分布，会不会影响后续机器学习的性能，所以我们再归约化后又加入了γ和β两个参数，在训练开始时，γ 一般为1，β一般为0，帮助其更好的拟合后续的特征，同时又能较大程度的保持各个维度的数据在统一尺度下。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114421588.png" alt="image-20220114114421588" style="zoom:26%;" /></p>
<p>​ 那么先前所讲的是在训练过程中的BatchNoramlization,那么如果在Testing的过程中，</p>
<p>​ 每次输入一个数据，没有凑齐一个Batch的情况下，BatchNormalization又应当如何执行呢？此时，我们的u和σ就不来源于测试的Batch了，而是来源于训练时候每一个Batches计算得到的u的均值依据一些权重累加得到。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114114447990.png" alt="image-20220114114447990" style="zoom:25%;" /></p>
<p>​ 总而言之，Batch Normalization能让你的error surface 更加平滑，从而降低训练的难度，让网络拥有更好的性能，这样的解释无论是理论还是实验都已经被人所证实。具体可以参见这篇论文《How Does Batch Normalization Help Optimization?》。</p>
]]></content>
      <categories>
        <category>ML2021课程系列笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Training Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title>ML2021课程系列笔记1——DeepLearning基本介绍</title>
    <url>/2021/12/17/ML2021%E7%AC%94%E8%AE%B0/ML2021%E8%AF%BE%E7%A8%8B%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94DL%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h3 id="hung-yi-lee-李宏毅-machine-learning2021spring课程week1笔记">Hung-yi Lee (李宏毅) MACHINE LEARNING2021SPRING课程Week1笔记</h3>
<h3 id="课程网址httpsspeech.ee.ntu.edu.twhyleeml2021-spring.html">课程网址：https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html</h3>
<p>其实Machine Learning的过程就是以下的三个基本步骤：</p>
<p>​ 1、 Function with Unknown Parameters</p>
<p>​ 2、 Define Loss From Training Data</p>
<p>​ 3、 Optimization</p>
<p>本篇就围绕这三个步骤进行简单的讲解：</p>
<h2 id="一最简单的ml模型函数">一、最简单的ML模型函数：</h2>
<p>​ 在最简单的ML的过程中：</p>
<p>​ 我们的第一步就是要找一个有未知参数的函数，那么最简单的就是最为纯粹的线性模型：Y = wx1 + b.</p>
<p>​ 我们将x1称为feature，w，b称为未知待训练参数，在第二步中，我们定义一个损失函数Loss，这个损失函数代表该线性模型拟合数据的好坏程度。它是模型未知参数的函数，即L(w,b)。我们利用训练数据集，来计算这个Loss函数的值，从而判断该模型拟合数据的好坏。</p>
<p>​ 第三步，我们就是要找到一组最好的w和b，使得Loss函数最小，也就是使得我们的模型最能够拟合数据。在这一步中，常常会用到梯度下降的方法去进行优化。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220121170849128.png" /></p>
<p>​ 至此，一个最为简单的机器学习任务就完成了，我们用一个最简单的带两个未知参数的函数，去拟合一组数据，找到了最能够拟合这组数据的两个参数的数值。简单而言，训练的步骤就是由以下这三步组成。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114105405284.png" alt="image-20220114105405284" style="zoom: 25%;" /></p>
<h2 id="二进阶的ml模型函数">二、进阶的ML模型函数</h2>
<p>​ 然后我们在想，刚才那样是最简单的线性模型，如果我们增加features，我们可以得到如下图所示的带7个参数的模型，也可以得到有28个参数的模型……当然在第二步和第三步中过程是一样的。参数越多，我们能够拟合的features就越多。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114105522828.png" alt="image-20220114105522828" style="zoom: 8%;" /> <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114105556231.png" alt="image-20220114105556231" style="zoom:8%;" /></p>
<p>​ 但是，仅仅是线性的模型的组合，还是让我们能够拟合的函数太过有限了，所以我们还想在第一步中找到更flex一点的函数，于是乎，我们就发现了sigmoid函数：它也有b，w，c三个未知参数。。那么进行推广以后，也可以将所有的函数</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114105640184.png" alt="image-20220114105640184" style="zoom:19%;" /></p>
<p>​ 那么进行推广以后，我们可以将如图所示的红色函数，用三个sigmoid函数的叠加来进行拟合，也就是能够列出一个图片左下方的公式，这个公式共有3*3 + 1 = 10个参数。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114105705024.png" alt="image-20220114105705024" style="zoom:25%;" /></p>
<p>​</p>
<p>​ 我们将这个式子提取出来，再仔细的看一看：</p>
<p>​ 其实我们可以讲上述这个式子写成矩阵相乘的形式，我们首先看sigmoid函数内部所干的事情，sigmoid函数内部所干的事情就可以写成如下所示的矩阵表示形式，r = b + W * x</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114105732731.png" alt="image-20220114105732731" style="zoom:25%;" /></p>
<p>​ 然后我们的r向量中的每一项再分别通过sigmoid函数，得到a向量</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114105800543.png" alt="image-20220114105800543" style="zoom:25%;" /></p>
<p>​ 最后a向量[a1,a2,a3]乘上系数c1,c2,c3，再加上偏执b，得到最终的预测值y。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114105825308.png" alt="image-20220114105825308" style="zoom:25%;" /></p>
<p>​ 故而我们最终就可以将这个more flexible的函数写成如下形式，其中的W，b(绿色)，c，b(灰色)，都是未知参数，而x是features，y是tag。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114105847631.png" alt="image-20220114105847631" style="zoom:25%;" /></p>
<p>​ 随后，我们可以把所有的未知参数拼接起来，称为向量θ。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114105904095.png" alt="image-20220114105904095" style="zoom: 15%;" /></p>
<p>​ 然后，再次回到ML框架中，在第二步的寻找Loss函数的时候，与前面基础的其实相差不多。此时Loss函数应当是θ的函数，即L(θ)，最后再使用梯度下降优化的时候，我们就是要找到一个θ*来使得Loss函数值最小。</p>
<p>​ <span class="math inline">\(\theta^{*} = arg min_\theta L\)</span></p>
<p>​ 至于方法，也与前面一致：计算Loss函数对各个未知参数的倒数，然后梯度下降即可。按照下图所示，我们将L对各个未知参数的导数组成了一个向量，称为梯度。然后依据学习率来进行未知参数的更新。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114110015271.png" alt="image-20220114110015271" style="zoom:23%;" /></p>
<p>​ 在这个训练优化参数的过程中，我们需要注意，我们往往不会一次性拿所有的训练数据来计算损失函数，从而更新未知参数，而是会将训练数据分成一个batch一个batch，每次使用一个batch来计算损失函数，进而计算梯度，更新未知参数。</p>
<p>​ 所有我们需要区分以下一些名词：<strong>Update / Epoch / Batch Size / Training Data</strong></p>
<ul>
<li><p>Update: 每做一次梯度下降，更新一次参数，就叫做一次Update</p></li>
<li><p>Batch Size: 每次用来计算梯度下降更新参数用到的训练集中样本的个数，我们在训练时往往会将其分为一个Batch一个Batch，每次使用一个Batch来计算梯度，更新参数。</p></li>
<li><p>Training Data: 所有的训练数据集样本。</p></li>
<li><p>Epoch: 每当使用过一轮所有的训练数据集样本以后，叫做一个Epoch</p></li>
</ul>
<p>​ 整个的Optimization的过程如下所示：</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114110125128.png" alt="image-20220114110125128" style="zoom:23%;" /></p>
<p>​ 下面是一些帮助理解的例子：</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114110144324.png" alt="image-20220114110144324" style="zoom:25%;" /></p>
<h2 id="三ml模型最终进化deeplearning">三、ML模型最终进化——DeepLearning</h2>
<p>​ 在第二部分的function中，我们拟合了许多个sigmoid函数，sigmoid函数内部又是线性的函数。最终，形成了如下图所示的一个数据流。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114110230140.png" alt="image-20220114110230140" style="zoom:20%;" /></p>
<p>​ 其实，我们在得到[a1,a2,a3]之后，还可以将[a1,a2,a3]再次作为输入，输入到另一个类似结构中去，如下所示：这样子所形成的模型够更好的拟合数据。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114110253145.png" alt="image-20220114110253145" style="zoom:25%;" />‘</p>
<p>​ 我们将每一个sigmoid函数组成的单位叫做一个Neuron，每处理一次数据所有的Neuron组成的叫做一个Hidden Layer，由于整个架构会有很多个Hidden Layer组成，这种架构的模型函数就叫做<strong>Deep Neural Network ( DNN )</strong>，深度神经网络，</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114110326577.png" alt="image-20220114110326577" style="zoom:20%;" />‘’</p>
<p>​ 以下便是一些比较经典的DNN架构的网络：ALexNet、VGG、GoogleNet等等</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/imac/image-20220114110348644.png" alt="image-20220114110348644" style="zoom:25%;" /></p>
<h2 id="附录-hw1-回归任务作业笔记">附录： HW1 回归任务作业笔记</h2>
<h4 id="有关数据预处理中的归一化处理">1、有关数据预处理中的归一化处理</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.data[:, <span class="number">40</span>:] = (self.data[:, <span class="number">40</span>:] - self.data[:, <span class="number">40</span>:].mean(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)) / self.data[:, <span class="number">40</span>:].std(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>​ 其可以帮助你将数据的不同维度的特征数值，压到类似的尺度上，不至于出现有些维度的特征数值是km级别，有些可能是mm级别。如果不同维度的特征尺度相差太大，会导致训练时梯度下降算法非常难以执行，Lr过小的话，体现出的结果就是训练了半天Loss降不下去。Lr过大又可能导致Loss乱窜。所以归一化处理还是很重要的。</p>
<h4 id="如果模型太复杂易过拟合我们可以在loss函数中加入l2正则化项">2、如果模型太复杂易过拟合，我们可以在Loss函数中加入L2正则化项</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_loss</span>(<span class="params">self, pred, target</span>):</span></span><br><span class="line">	regularization_loss = <span class="number">0</span></span><br><span class="line">	<span class="comment"># 使用L2正则项</span></span><br><span class="line">	<span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">		regularization_loss += torch.<span class="built_in">sum</span>(param ** <span class="number">2</span>)</span><br><span class="line">	<span class="keyword">return</span> self.criterion(pred, target) + <span class="number">0.00075</span> * regularization_loss</span><br></pre></td></tr></table></figure>
<h4 id="训练模型的基本步骤以及一些tricks">3、训练模型的基本步骤以及一些tricks</h4>
<ul>
<li>1）每一个epoch中需要执行的必备步骤：切换模型至train模式，通过dataLoader迭代遍历每一个Batch，在每一个Batch中，首先将梯度调整至0，然后前向计算，然后计算Loss，然后通过Loss.backward()反向传播，最后通过优化器更新模型参数。</li>
<li>2）在每一轮epoch结束后 ，使用验证集进行验证，计算dev_mse.</li>
<li>3）在每一轮epoch结束后，如果dev_mse没有进步的话early_stop_cnt++,当early_stop_cnt达到设定值时，提早结束训练。（代表已经很多轮没有进步了）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">min_mse = <span class="number">1000.</span></span><br><span class="line">early_stop_cnt = <span class="number">0</span></span><br><span class="line">epoch = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> epoch &lt; n_epochs:</span><br><span class="line">	model.train() <span class="comment"># set model to training mode</span></span><br><span class="line">	<span class="keyword">for</span> x, y <span class="keyword">in</span> tr_set: <span class="comment"># iterate through the dataloader</span></span><br><span class="line">		optimizer.zero_grad() <span class="comment"># set gradient to zero</span></span><br><span class="line">		x, y = x.to(device), y.to(device) <span class="comment"># move data to device (cpu/cuda)</span></span><br><span class="line">		pred = model(x) <span class="comment"># forward pass (compute output)</span></span><br><span class="line">		mse_loss = model.cal_loss(pred, y) <span class="comment"># compute loss</span></span><br><span class="line">		mse_loss.backward() <span class="comment"># compute gradient (backpropagation)</span></span><br><span class="line">		optimizer.step() <span class="comment"># update model with optimizer</span></span><br><span class="line">	<span class="comment"># After each epoch, test your model on the validation (development) set.</span></span><br><span class="line">	dev_mse = dev(dv_set, model, device)</span><br><span class="line">	<span class="keyword">if</span> dev_mse &lt; min_mse:</span><br><span class="line">		<span class="comment"># Save model if your model improved</span></span><br><span class="line">		min_mse = dev_mse</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&#x27;Saving model (epoch = &#123;:4d&#125;, loss = &#123;:.4f&#125;)&#x27;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>, min_mse))</span><br><span class="line">		torch.save(model.state_dict(), config[<span class="string">&#x27;save_path&#x27;</span>]) <span class="comment"># Save model to specified path</span></span><br><span class="line">		early_stop_cnt = <span class="number">0</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		early_stop_cnt += <span class="number">1</span></span><br><span class="line">	epoch += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># Stop training if your model stops improving for &quot;config[&#x27;early_stop&#x27;]&quot; epochs.</span></span><br><span class="line">	<span class="keyword">if</span> early_stop_cnt &gt; config[<span class="string">&#x27;early_stop&#x27;</span>]:</span><br><span class="line">		<span class="keyword">break</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ML2021课程系列笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch学习笔记9——常用编码技巧（更新中）</title>
    <url>/2021/12/17/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09%E2%80%94%E2%80%94%E5%B8%B8%E7%94%A8%E7%BC%96%E7%A0%81%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<h3 id="task1">Task1：</h3>
<h4 id="任务描述完成padding填充使得h和w为2的倍数同时实现下图所示的变换">任务描述：完成padding填充，使得H和W为2的倍数同时，实现下图所示的变换：</h4>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/mac/image-20211229101356170.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输入的x，维度size为[B,L,C] ,其中L = H * W</span></span><br><span class="line">x = x.view(B, H, W, C) <span class="comment"># 首先将其重新展为 [B,H,W,C]</span></span><br><span class="line"><span class="comment"># padding 然后，如果输入feature map的H，W不是2的整数倍，需要进行padding</span></span><br><span class="line">pad_input = (H % <span class="number">2</span> == <span class="number">1</span>) <span class="keyword">or</span> (W % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> pad_input:</span><br><span class="line">   <span class="comment"># (C_front, C_back, W_left, W_right, H_top, H_bottom)</span></span><br><span class="line">   <span class="comment"># 注意这里的Tensor通道是[B, H, W, C]，所以会和官方文档有些不同</span></span><br><span class="line">   x = F.pad(x, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, W % <span class="number">2</span>, <span class="number">0</span>, H % <span class="number">2</span>)) <span class="comment"># 在右侧和底侧进行padding</span></span><br><span class="line">   </span><br><span class="line"><span class="comment"># 此时要开始进行下采样了，如何在H，W这个平面上进行下采样呢？ 非常关键的编码技巧！！</span></span><br><span class="line">x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]   ::2 代表 步长</span></span><br><span class="line">x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line"><span class="comment"># 然后将四个子矩阵在channel维度[也就是最后一个维度]上进行拼接</span></span><br><span class="line">x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># [B, H/2, W/2, 4*C]</span></span><br><span class="line"><span class="comment"># 再重新将H/2,W/2平面展成1个维度</span></span><br><span class="line">x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># [B, H/2*W/2, 4*C]</span></span><br></pre></td></tr></table></figure>
<h3 id="task2">Task2:</h3>
<h4 id="任务描述-python中的广播机制">任务描述： Python中的广播机制</h4>
<p>​ 在python中使用numpy进行<strong>按位运算</strong>的时候，有一个小技巧可以帮助减少代码量——那就是broadcasting,广播机制。简单来说，broadcasting可以这样理解：如果你有一个m * n的矩阵，让它加减乘除一个1 * n的矩阵，它会被复制m次，成为一个m * n的矩阵，然后再逐元素地进行加减乘除操作。同样地对m * 1的矩阵成立。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/mac/750.png" /></p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = numpy.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">result = A + <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	[101 102 103]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 就不再需要去书写 A + [100,100,100] 了</span></span><br><span class="line"></span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])  <span class="comment"># [3,2]</span></span><br><span class="line">result = A + [<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>]   <span class="comment"># [3,1]</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[[101 202 303]</span></span><br><span class="line"><span class="string"> [104 205 306]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>### Task3:</p>
<h4 id="section"></h4>
]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch学习笔记8——常用函数整理（更新中）</title>
    <url>/2021/12/16/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08%E2%80%94%E2%80%94%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h3 id="softmax-dim应当如何填">1、SoftMax — dim应当如何填？</h3>
<p>官方文档：https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html?highlight=softmax#torch.nn.functional.softmax</p>
<p>1）标准用法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.functional.softmax(<span class="built_in">input</span>, dim)</span><br><span class="line">（Applies a softmax function.）</span><br></pre></td></tr></table></figure>
<p>2）依赖库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure>
<p>3）简单示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.rand(<span class="number">3</span>,<span class="number">16</span>,<span class="number">20</span>) <span class="comment"># 我们看一个三维的tensor</span></span><br><span class="line">b=F.softmax(a,dim=<span class="number">0</span>)  </span><br><span class="line"><span class="comment"># dim = 0代表 输出的是在dim=0维上的概率分布，也就是说 对于任意的i,j (i是dim=1上的某个坐标，j是dim=2上的某个坐标)，通过softmax之后，要求 a[0][i][j] + a[1][i][j] + a[2][i][j] = 1 【此处仅有3项相加，原因是a向量dim=0维度 长度为3，仅有3个元素。】</span></span><br></pre></td></tr></table></figure>
<p>4）一个更为一般的示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.rand(x1,x2,x3,x4,……,xn) <span class="comment"># 我们看一个n维的tensor</span></span><br><span class="line">b=F.softmax(a,dim=m)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dim = m代表 输出的是在dim=m维上的概率分布，也就是说 通过softmax之后，对于任意的dim!=m的维度的一个定位：i1,i2,i3,……im-1,im+1,……in，需要满足如下公式 </span></span><br><span class="line"><span class="comment">#  a[i1][i2]……[im-1][0][im+1]……[in] + </span></span><br><span class="line"><span class="comment">#  a[i1][i2]……[im-1][1][im+1]……[in] +</span></span><br><span class="line"><span class="comment">#  a[i1][i2]……[im-1][2][im+1]……[in] +</span></span><br><span class="line"><span class="comment">#  …… +</span></span><br><span class="line"><span class="comment">#  a[i1][i2]……[im-1][xm][im+1]……[in] = 1 即可</span></span><br></pre></td></tr></table></figure>
<h3 id="view函数">2、View函数</h3>
<p>官方文档：https://pytorch.org/docs/stable/generated/torch.Tensor.view.html?highlight=view#torch.Tensor.view</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Tensor.view(*shape) → Tensor</span><br><span class="line">(Returns a new tensor with the same data as the `self` tensor but of a different `shape`.)</span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand(<span class="number">10</span>)  <span class="comment"># 一个1维的Tensor</span></span><br><span class="line">y = x.view(<span class="number">2</span>, <span class="number">5</span>) <span class="comment"># 将其resize成了一个2*5大小的 二维的Tensor</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">x: tensor([0.6611, 0.3041, 0.8008, 0.1733, 0.0197, 0.3914, 0.0468, 0.2380, 0.4159,</span></span><br><span class="line"><span class="string">        0.8241])</span></span><br><span class="line"><span class="string">y: tensor([[0.6611, 0.3041, 0.8008, 0.1733, 0.0197],</span></span><br><span class="line"><span class="string">        [0.3914, 0.0468, 0.2380, 0.4159, 0.8241]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>4）扩展示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand(<span class="number">20</span>)  <span class="comment"># 一个1维的Tensor</span></span><br><span class="line">y = x.view(<span class="number">2</span>,<span class="number">5</span>,-<span class="number">1</span>) <span class="comment"># 将其resize成了一个 2*5*? 大小的 三维的Tensor，？代表第三维度的大小需要推断得到，在此处的话，最后应该会形成一个2*5*2的三维Tensor。</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">x: tensor([0.5704, 0.4154, 0.0700, 0.8145, 0.1743, 0.1049, 0.1678, 0.2554, 0.9557,</span></span><br><span class="line"><span class="string">        0.0484, 0.7714, 0.5377, 0.8711, 0.6069, 0.0996, 0.6384, 0.9334, 0.2851,</span></span><br><span class="line"><span class="string">        0.5883, 0.5882])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">y: tensor([[[0.5704, 0.4154],</span></span><br><span class="line"><span class="string">         [0.0700, 0.8145],</span></span><br><span class="line"><span class="string">         [0.1743, 0.1049],</span></span><br><span class="line"><span class="string">         [0.1678, 0.2554],</span></span><br><span class="line"><span class="string">         [0.9557, 0.0484]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[0.7714, 0.5377],</span></span><br><span class="line"><span class="string">         [0.8711, 0.6069],</span></span><br><span class="line"><span class="string">         [0.0996, 0.6384],</span></span><br><span class="line"><span class="string">         [0.9334, 0.2851],</span></span><br><span class="line"><span class="string">         [0.5883, 0.5882]]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下为错误示例：</span></span><br><span class="line">x = torch.rand(<span class="number">10</span>)</span><br><span class="line">y = x.view(<span class="number">2</span>,<span class="number">3</span>)  <span class="comment"># 会报错</span></span><br><span class="line">y = x.view(<span class="number">2</span>,)   <span class="comment"># 会报错</span></span><br></pre></td></tr></table></figure>
<h3 id="permute函数">3、Permute函数</h3>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.permute(input, dims) → Tensor</span><br><span class="line">- parameters:</span><br><span class="line">  input (Tensor) – the input tensor.</span><br><span class="line">  dims (tuple of python:ints) – The desired ordering of dimensions</span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(x.size())</span><br><span class="line"><span class="comment"># torch.Size([2, 3, 5])</span></span><br><span class="line"><span class="built_in">print</span>(torch.permute(x, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)).size())</span><br><span class="line"><span class="comment"># torch.Size([5, 2, 3])</span></span><br><span class="line"><span class="comment"># 我们可以看到：  [dim0,dim1,dim2,dim3,dim4]  经过permute(2,0,1,4,3)以后：</span></span><br><span class="line"><span class="comment"># （ permute的含义：	现在的第0维应当为原来的第2维，现在的第1维应当为原来的第0维，…… ）</span></span><br><span class="line"><span class="comment"># 会变成如下形式。[dim2,dim0,dim1,dim4,dim3]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="nn.identity">4、nn.Identity()</h3>
<p>官方文档：https://pytorch.org/docs/stable/generated/torch.nn.Identity.html#torch.nn.Identity</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class torch.nn.Identity(*args, **kwargs)</span><br><span class="line">- A placeholder identity operator that is argument-insensitive.</span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.nn</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)</span><br><span class="line">&gt;&gt;&gt; input = torch.randn(128, 20)</span><br><span class="line">&gt;&gt;&gt; output = m(input)</span><br><span class="line">&gt;&gt;&gt; print(output.size())</span><br><span class="line">torch.Size([128, 20])</span><br><span class="line"></span><br><span class="line"># 经过m等于 没有发生任何变换 </span><br></pre></td></tr></table></figure>
<h3 id="nn.parameter">5、nn.Parameter</h3>
<p>官方文档：https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html?highlight=parameter#torch.nn.parameter.Parameter</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class torch.nn.Parameter(data=None, requires_grad=True)</span><br><span class="line">- A kind of Tensor that is to be considered a module parameter.</span><br><span class="line">- 创建一个可训练参数</span><br><span class="line">- when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. </span><br><span class="line">- 当它们作为模型的一个属性的时候，会被自动加入到模型的可训练参数中去</span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Layer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">	  	self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, <span class="number">762</span>))</span><br></pre></td></tr></table></figure>
<h3 id="nn.functional.pad">6、nn.functional.pad</h3>
<p>官方文档：https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html?highlight=pad#torch.nn.functional.pad</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.nn.functional.pad(input, pad, mode=&#x27;constant&#x27;, value=0.0)</span><br><span class="line">-	input (Tensor) – N-dimensional tensor</span><br><span class="line">- pad (tuple) – m-elements tuple, where m/2 ≤ input dimensions and m is even.</span><br><span class="line">- mode – &#x27;constant&#x27;, &#x27;reflect&#x27;, &#x27;replicate&#x27; or &#x27;circular&#x27;. Default: &#x27;constant&#x27;</span><br><span class="line">- value – fill value for &#x27;constant&#x27; padding. Default: 0</span><br><span class="line"></span><br><span class="line">对于padding-size的说明：</span><br><span class="line">	填充尺寸用来填充输入的某些维度，从最后一个维度开始进行描述。比如说:</span><br><span class="line">	如果想要填充最后一个维度，那就设置为(padding_left,padding_right)</span><br><span class="line">	如果想要填充最后两个维度，那就设置为(padding_left,padding_right,padding_top,padding_bottom)</span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.function <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">t2d = torch.rand(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">p1d = (<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t2d)</span><br><span class="line">out = F.pad(t2d, p1d, <span class="string">&quot;constant&quot;</span>, <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(out.size())</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[0.6359, 0.0629],</span></span><br><span class="line"><span class="string">        [0.5814, 0.5980]])</span></span><br><span class="line"><span class="string">torch.Size([2, 4])</span></span><br><span class="line"><span class="string">tensor([[0.0000, 0.6359, 0.0629, 0.0000],</span></span><br><span class="line"><span class="string">        [0.0000, 0.5814, 0.5980, 0.0000]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>4）扩展示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t4d = torch.empty(<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p3d = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>) <span class="comment"># pad by (0, 1), (2, 1), and (3, 3)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = F.pad(t4d, p3d, <span class="string">&quot;constant&quot;</span>, <span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(out.size())</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<h3 id="flatten">7、flatten</h3>
<p>官方文档：https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.flatten(input, start_dim=0, end_dim=- 1) → Tensor</span><br><span class="line">  - input (Tensor) – the input tensor.</span><br><span class="line">  - start_dim (int) – the first dim to flatten</span><br><span class="line">  - end_dim (int) – the last dim to flatten</span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">t2d = torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(t2d, t2d.size())</span><br><span class="line">out = torch.flatten(t2d, <span class="number">1</span>)  <span class="comment"># 从第1维开始，展平后续所有维度</span></span><br><span class="line"><span class="built_in">print</span>(out, out.size())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[0.4151, 0.8645],</span></span><br><span class="line"><span class="string">         [0.1289, 0.2260]],</span></span><br><span class="line"><span class="string">        [[0.3302, 0.0393],</span></span><br><span class="line"><span class="string">         [0.9822, 0.5305]]]) torch.Size([2, 2, 2])</span></span><br><span class="line"><span class="string">tensor([[0.4151, 0.8645, 0.1289, 0.2260],</span></span><br><span class="line"><span class="string">        [0.3302, 0.0393, 0.9822, 0.5305]]) torch.Size([2, 4])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>4）扩展示例:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Q1、现在有一个向量t，其size为 [A,B,C,D,E,F,G] ,然后我们对其应用flatten: torch.flatten(t,2,4),</span><br><span class="line">那么size会变成？</span><br><span class="line">Answer: [A,B,C*D*E,F,G]</span><br><span class="line"></span><br><span class="line">Q2、现在有一个向量t，其size为 [A,B,C,D,E,F,G] ,然后我们对其应用flatten: torch.flatten(t,-2),</span><br><span class="line">那么size会变成？</span><br><span class="line">Answer: [A,B,C,D,E,F*G] 等效于应用torch.flatten(t,5) 等效于应用torch.flatten(t,5,6)</span><br><span class="line"></span><br><span class="line">Q3、现在有一个向量t，其size为 [A,B,C,D,E,F,G] ,然后我们对其应用flatten: torch.flatten(t),</span><br><span class="line">那么size会变成？</span><br><span class="line">Answer: [A*B*C*D*E*F*G] </span><br><span class="line"></span><br><span class="line">Q3、现在有一个向量t，其size为 [A,B,C,D,E,F,G] ,然后我们对其应用flatten: torch.flatten(t,1),</span><br><span class="line">那么size会变成？</span><br><span class="line">Answer: [A,B*C*D*E*F*G] </span><br></pre></td></tr></table></figure>
<h3 id="transpose">8、Transpose</h3>
<p>官方文档：https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.transpose(input, dim0, dim1) → Tensor</span><br><span class="line"></span><br><span class="line">-	Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped.</span><br><span class="line">- input (Tensor) – the input tensor.</span><br><span class="line">- dim0 (int) – the first dimension to be transposed</span><br><span class="line">- dim1 (int) – the second dimension to be transposed</span><br><span class="line"></span><br><span class="line">注：此处的转置通用于高阶矩阵，函数将会对dim0和dim1这两个维度形成的平面进行转置，交换dim0和dim1,而不影响其他维度。</span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t2d = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(t2d, t2d.size())</span><br><span class="line">out = torch.transpose(t2d, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(out, out.size())</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[0.5228, 0.2990, 0.1111],</span></span><br><span class="line"><span class="string">        [0.4857, 0.4479, 0.6637]]) torch.Size([2, 3])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[0.5228, 0.4857],</span></span><br><span class="line"><span class="string">        [0.2990, 0.4479],</span></span><br><span class="line"><span class="string">        [0.1111, 0.6637]]) torch.Size([3, 2])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>4）扩展示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在高维的矩阵中，不要太过注重打印数据显示的变化形式，从维度长度的变化入手，即可清晰的搞懂</span><br><span class="line"></span><br><span class="line">Q1、现在有一个向量t，其size为 [A,B,C,D,E,F,G] ,然后我们对其应用transpose: torch.transpose(t,2,4),</span><br><span class="line">那么size会变成？</span><br><span class="line"></span><br><span class="line">Answer: [A,B,E,D,C,F,G]  可以看到，从size上来讲其实就是交换了第2维和第4维的长度。从数据上来讲，你可以想象其找到了第2维和第4维形成的数据平面，然后应用了正常的转置。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="cat函数">9、Cat函数</h3>
<p>官方文档：https://pytorch.org/docs/stable/generated/torch.cat.html?highlight=cat#torch.cat</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.cat(tensors, dim=0, *, out=None) → Tensor</span><br><span class="line"></span><br><span class="line">- Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</span><br><span class="line"></span><br><span class="line">- tensors (sequence of Tensors) – any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.</span><br><span class="line">- dim (int, optional) – the dimension over which the tensors are concatenated</span><br><span class="line"></span><br><span class="line">注：tensors参数需要输入一个tensor的序列，所有的需要拼接的tensors需要有同样的形状维度（除了拼接的那一维）</span><br><span class="line">dim指定要拼接的维度</span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x, x.size())</span><br><span class="line">out = torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(out, out.size())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[-1.2926, -0.3155,  0.5966],</span></span><br><span class="line"><span class="string">        [ 0.8921, -2.0986,  0.0748]]) torch.Size([2, 3])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([[-1.2926, -0.3155,  0.5966],</span></span><br><span class="line"><span class="string">        [ 0.8921, -2.0986,  0.0748],</span></span><br><span class="line"><span class="string">        [-1.2926, -0.3155,  0.5966],</span></span><br><span class="line"><span class="string">        [ 0.8921, -2.0986,  0.0748],</span></span><br><span class="line"><span class="string">        [-1.2926, -0.3155,  0.5966],</span></span><br><span class="line"><span class="string">        [ 0.8921, -2.0986,  0.0748]]) torch.Size([6, 3])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>4）扩展示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Q1、现在有一个向量t1，其size为 [A,B,C,D] ,向量t2，其size为[A,B,E,D] 然后我们对其应用cat: torch.cat((t1,t2),2),那么size会变成？</span><br><span class="line"></span><br><span class="line">Answer: [A,B,C+E,D]</span><br><span class="line"></span><br><span class="line">Q2、现在有一个向量t1，其size为 [A,B,C,D] ,向量t2，其size为[A,B,E,D] 然后我们对其应用cat: torch.cat((t1,t2),1),那么size会变成？</span><br><span class="line"></span><br><span class="line">Answer: 函数会报错，因为除了维度1以外，t1和t2的维度2的shape不一样</span><br></pre></td></tr></table></figure>
<h3 id="roll">10、Roll</h3>
<p>官方文档：https://pytorch.org/docs/stable/generated/torch.roll.html?highlight=roll#torch.roll</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.roll(input, shifts, dims=None) → Tensor</span><br><span class="line"></span><br><span class="line">	沿着给定的维数滚动张量。移动到最后一个位置以外的元素将在第一个位置重新引入。如果没有指定尺寸，张量在滚动之前将被压平，然后恢复到原来的形状。</span><br><span class="line">	</span><br><span class="line">- input (Tensor) – the input tensor.</span><br><span class="line">- shifts (int or tuple of python:ints) – 张量的元素被移动的位置数。如果shift是一个元组，dim必须是一个相同大小的元组，并且每个维度将被相应的值滚动</span><br><span class="line">- dims (int or tuple of python:ints) – Axis along which to roll</span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]).view(<span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">y1 = torch.roll(x, <span class="number">1</span>, <span class="number">0</span>)  <span class="comment"># 沿着第0维（列），滚动1个元素</span></span><br><span class="line"><span class="built_in">print</span>(y1)</span><br><span class="line">y2 = torch.roll(x, -<span class="number">1</span>, <span class="number">0</span>) <span class="comment"># 沿着第0维（列），滚动-1个元素</span></span><br><span class="line"><span class="built_in">print</span>(y2)</span><br><span class="line">y3 = torch.roll(x, shifts=(<span class="number">2</span>, <span class="number">1</span>), dims=(<span class="number">0</span>, <span class="number">1</span>)) <span class="comment"># 沿着第0维（列），滚动2个元素，沿着第1维（行），滚动1个元素</span></span><br><span class="line"><span class="built_in">print</span>(y3)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">x:tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [3, 4],</span></span><br><span class="line"><span class="string">        [5, 6],</span></span><br><span class="line"><span class="string">        [7, 8]])</span></span><br><span class="line"><span class="string">y1:tensor([[7, 8],</span></span><br><span class="line"><span class="string">        [1, 2],</span></span><br><span class="line"><span class="string">        [3, 4],</span></span><br><span class="line"><span class="string">        [5, 6]])</span></span><br><span class="line"><span class="string">y2:tensor([[3, 4],</span></span><br><span class="line"><span class="string">        [5, 6],</span></span><br><span class="line"><span class="string">        [7, 8],</span></span><br><span class="line"><span class="string">        [1, 2]])</span></span><br><span class="line"><span class="string">y3:tensor([[6, 5],</span></span><br><span class="line"><span class="string">        [8, 7],</span></span><br><span class="line"><span class="string">        [2, 1],</span></span><br><span class="line"><span class="string">        [4, 3]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="contiguous">11、Contiguous</h3>
<p>官方文档：https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html?highlight=contiguous#torch.Tensor.contiguous</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Tensor.contiguous(memory_format=torch.contiguous_format) → Tensor</span><br><span class="line"></span><br><span class="line">返回一个连续的内存张量，其中包含与当前张量相同的数据。 如果当前已经是指定的内存格式，则此函数返回当前张量。 </span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>3）为什么需要Contiguous?</p>
<ul>
<li><strong><code>torch.view</code></strong>等方法操作需要连续的Tensor。详细原因可见：https://zhuanlan.zhihu.com/p/64551412</li>
<li>出于性能考虑，连续的Tensor，语义上相邻的元素，在内存中也是连续的，访问相邻元素是矩阵运算中经常用到的操作，语义和内存顺序的一致性是缓存友好的，在内存中连续的数据可以（但不一定）被高速缓存预取，以提升CPU获取操作数据的速度。</li>
</ul>
<h3 id="section">12、</h3>
<p>官方文档：</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>4）扩展示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="section-1">13、</h3>
<p>官方文档：</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>4）扩展示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="section-2">14、</h3>
<p>官方文档：</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>4）扩展示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="section-3">15、</h3>
<p>官方文档：</p>
<p>1）标准用法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>2）依赖库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>3）简单示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>4）扩展示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch学习笔记7——一个深度学习程序的通用框架</title>
    <url>/2021/12/15/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07%E2%80%94%E2%80%94%E4%B8%80%E4%B8%AA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%A8%8B%E5%BA%8F%E7%9A%84%E9%80%9A%E7%94%A8%E6%A1%86%E6%9E%B6/</url>
    <content><![CDATA[<p>以下是一个深度学习程序的通用框架，大部分基础的深度学习任务都可以按照以下这个框架的步骤去进行书写：</p>
<h4 id="step0-一些依赖项函数">Step0: 一些依赖项函数</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_device</span>():</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Get device (if GPU is available, use GPU) &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">  </span><br><span class="line"> <span class="comment"># fix random seed</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">same_seeds</span>(<span class="params">seed</span>):</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed(seed)</span><br><span class="line">        torch.cuda.manual_seed_all(seed)  </span><br><span class="line">    np.random.seed(seed)  </span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h4 id="step1-下载数据准备数据这一步将硬盘的数据加载到内存中">Step1: 下载数据、准备数据（这一步将硬盘的数据加载到内存中）</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Loading data ...&#x27;</span>)</span><br><span class="line"><span class="comment"># 加载Train Features、Train Labels、Test Features</span></span><br><span class="line">train = np.load(<span class="string">&#x27;train.npy&#x27;</span>)</span><br><span class="line">train_label = np.load(<span class="string">&#x27;train_label.npy&#x27;</span>)</span><br><span class="line">test = np.load(<span class="string">&#x27;test.npy&#x27;</span>)</span><br><span class="line"><span class="comment"># 一般而言可以打印一下数据集的大小，心中有数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Size of training data: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Size of testing data: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(test.shape))</span><br></pre></td></tr></table></figure>
<h4 id="step2-创建dataset类">Step2: 创建Dataset类</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span>   <span class="comment"># 主要就是在init函数里定义好 self.data 和 self.label</span></span><br><span class="line">        self.data = torch.from_numpy(X).<span class="built_in">float</span>()  <span class="comment"># 进行赋值</span></span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:   <span class="comment"># 这儿需要考虑有y的训练集 和 没有y的测试集</span></span><br><span class="line">            self.label = torch.LongTensor(y.astype(np.<span class="built_in">int</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.label = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span>    <span class="comment"># 基本都是如下这个形式，不太需要变化</span></span><br><span class="line">        <span class="keyword">if</span> self.label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.data[idx], self.label[idx]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span>  <span class="comment"># 返回数据集长度，一般都是如下这个形式，也不太需要变化</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="step3-将训练数据分割为-训练集-和-验证集">Step3: 将训练数据分割为 训练集 和 验证集</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">VAL_RATIO = <span class="number">0.2</span>  <span class="comment"># 利用此参数控制 验证集占 整个训练数据的比例</span></span><br><span class="line"></span><br><span class="line">percent = <span class="built_in">int</span>(train.shape[<span class="number">0</span>] * (<span class="number">1</span> - VAL_RATIO))</span><br><span class="line">train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Size of training set: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train_x.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Size of validation set: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(val_x.shape))</span><br></pre></td></tr></table></figure>
<h4 id="step4-实例化dataset对象以及对应的dataloader对象">Step4: 实例化Dataset对象以及对应的DataLoader对象</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">64</span>  <span class="comment"># 设定 Batch_size，此参数DataLoader</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_set = MyDataset(train_x, train_y)</span><br><span class="line">val_set = MyDataset(val_x, val_y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#only shuffle the training data</span></span><br><span class="line">train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>) </span><br><span class="line">val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h4 id="step5-创建网络模型">Step5: 创建网络模型</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Classifier</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span>   <span class="comment"># 定义网络层级</span></span><br><span class="line">        <span class="built_in">super</span>(Classifier, self).__init__()</span><br><span class="line">        self.layer1 = nn.Linear(<span class="number">429</span>, <span class="number">1024</span>)</span><br><span class="line">        self.layer2 = nn.Linear(<span class="number">1024</span>, <span class="number">512</span>)</span><br><span class="line">        self.layer3 = nn.Linear(<span class="number">512</span>, <span class="number">128</span>)</span><br><span class="line">        self.out = nn.Linear(<span class="number">128</span>, <span class="number">39</span>) </span><br><span class="line">        self.act_fn = nn.Sigmoid()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span> <span class="comment"># 定义数据的流转逻辑过程</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.act_fn(x)</span><br><span class="line">        </span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.act_fn(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.act_fn(x)</span><br><span class="line"></span><br><span class="line">        x = self.out(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="step6-设定一些训练参数为训练作准备">Step6: 设定一些训练参数，为训练作准备</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fix random seed for reproducibility</span></span><br><span class="line">same_seeds(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get device </span></span><br><span class="line">device = get_device()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;DEVICE: <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># training parameters </span></span><br><span class="line">num_epoch = <span class="number">20</span>               <span class="comment"># number of training epoch</span></span><br><span class="line">learning_rate = <span class="number">0.0001</span>       <span class="comment"># learning rate</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the path where checkpoint saved</span></span><br><span class="line">model_path = <span class="string">&#x27;./model.ckpt&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create model, define a loss function, and optimizer</span></span><br><span class="line">model = Classifier().to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss() </span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br></pre></td></tr></table></figure>
<h4 id="step7-开启网络训练参数优化">Step7: 开启网络训练，参数优化</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># start training</span></span><br><span class="line"></span><br><span class="line">best_acc = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    val_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># training</span></span><br><span class="line">    model.train() <span class="comment"># set the model to training mode</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        inputs, labels = data  <span class="comment"># 从data中加载inputs和labels</span></span><br><span class="line">        inputs, labels = inputs.to(device), labels.to(device) <span class="comment"># 拷贝至设备</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清零梯度</span></span><br><span class="line">        outputs = model(inputs)   <span class="comment"># 将输入放到模型中拿到输出</span></span><br><span class="line">        batch_loss = criterion(outputs, labels) <span class="comment"># 通过损失函数，计算本次损失值</span></span><br><span class="line">        batch_loss.backward()  <span class="comment"># 损失反向传播，计算梯度</span></span><br><span class="line">        optimizer.step() <span class="comment"># 使用optimizer更新一步模型</span></span><br><span class="line"></span><br><span class="line">        train_loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每一个epoch完成后，如果有验证集的话，就进行validation验证</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(val_set) &gt; <span class="number">0</span>:</span><br><span class="line">        model.<span class="built_in">eval</span>() <span class="comment"># set the model to evaluation mode</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 验证集不需要计算梯度</span></span><br><span class="line">            <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(val_loader):</span><br><span class="line">                inputs, labels = data </span><br><span class="line">                inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">                outputs = model(inputs)</span><br><span class="line">                batch_loss = criterion(outputs, labels) </span><br><span class="line"></span><br><span class="line">                val_loss += batch_loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[&#123;:03d&#125;/&#123;:03d&#125;] Train Loss: &#123;:3.6f&#125; | Val Loss: &#123;:3.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch + <span class="number">1</span>, num_epoch, train_loss/<span class="built_in">len</span>(train_loader), val_loss/<span class="built_in">len</span>(val_loader)</span><br><span class="line">            ))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 必备过程，如果模型比截止当前更优了，那么就保存并记录，然后输出</span></span><br><span class="line">            <span class="keyword">if</span> val_acc &gt; best_acc:</span><br><span class="line">                best_acc = val_acc</span><br><span class="line">                torch.save(model.state_dict(), model_path)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;saving model with acc &#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(best_acc/<span class="built_in">len</span>(val_set)))</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># 没有验证集这一步的话，就可以直接打印输出</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;[&#123;:03d&#125;/&#123;:03d&#125;] Train Acc: &#123;:3.6f&#125; Loss: &#123;:3.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">            epoch + <span class="number">1</span>, num_epoch, train_acc/<span class="built_in">len</span>(train_set), train_loss/<span class="built_in">len</span>(train_loader)</span><br><span class="line">        ))</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 如果没有验证这一步骤的话，我们需要保存最终训练得到的这个模型</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(val_set) == <span class="number">0</span>:</span><br><span class="line">    torch.save(model.state_dict(), model_path)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;saving model at last epoch&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="step8-test-测试计算测试集的预测值">Step8: Test 测试,计算测试集的预测值</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 实例化测试用的DataSet和DataLoader</span></span><br><span class="line">test_set = MyDataset(test, <span class="literal">None</span>)</span><br><span class="line">test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  实例化模型，加载参数</span></span><br><span class="line">model = Classifier().to(device)</span><br><span class="line">model.load_state_dict(torch.load(model_path))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始进行预测， </span></span><br><span class="line">predict = []</span><br><span class="line">model.<span class="built_in">eval</span>() <span class="comment"># 将模型调整至评估状态（必备步骤）</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad(): <span class="comment"># 由于预测不需要跟踪梯度</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader): <span class="comment"># 计算s</span></span><br><span class="line">        inputs = data</span><br><span class="line">        inputs = inputs.to(device)</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> test_pred.cpu().numpy():</span><br><span class="line">            predict.append(y)</span><br><span class="line">           </span><br><span class="line"><span class="comment"># 输出预测的CSV文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;prediction.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i, y <span class="keyword">in</span> <span class="built_in">enumerate</span>(predict):</span><br><span class="line">        f.write(<span class="string">&#x27;&#123;&#125;,&#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(i, y))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch学习笔记6——优化模型参数</title>
    <url>/2021/12/14/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06%E2%80%94%E2%80%94%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0&amp;%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="六优化模型参数">六、优化模型参数</h1>
<p>现在我们有了模型和数据，接下去需要通过优化数据参数来训练、验证和测试我们的模型了。</p>
<p>训练模型是一个迭代过程； 在每次迭代（称为 epoch）中，模型对输出进行猜测，计算其猜测中的误差（损失），收集误差对其参数的导数，并使用梯度下降优化这些参数。 下面的代码是先预定义了一些之前记录过的内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork()</span><br></pre></td></tr></table></figure>
<h3 id="超参数">1、超参数：</h3>
<p>超参数是可调节的参数，可控制模型优化过程。不同的超参数值会影响模型训练和收敛速度</p>
<p>我们为训练定义了以下超参数： - Number of Epochs 迭代数据集的次数 - Batch Size 在更新参数之前通过网络传播的数据样本数量(单次传入网络进行训练的样本数量) - Learning Rate 在每个批次/时期更新模型参数的程度。 较小的值会导致学习速度变慢，而较大的值可能会导致训练过程中出现不可预测的行为。</p>
<p>此处注意区分一些概念： - Batch Size：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练； - iteration：1个iteration等于使用batchsize个样本训练一次； - Epoch：1个epoch等于使用训练集中的全部样本训练一次，通俗的讲epoch的值就是整个数据集被轮几次。</p>
<p>举个例子，我们现在有一个5000个数据的数据集，Batch Size = 100, 我们需要迭代训练50次，那么最终Number of Epoches = 10,Interation = 50.</p>
<h3 id="优化循环">2、优化循环：</h3>
<p>每个Epoch由两个主要部分组成： - Train Loop - 迭代训练数据集并尝试收敛到最佳参数。(由很多次Interation组成) - Validation/Test Loop - 迭代测试数据集以检查模型性能是否正在提高。</p>
<h3 id="损失函数">3、损失函数</h3>
<ul>
<li><p>当提供一些训练数据时，我们未经训练的网络可能没法给出正确的答案。</p></li>
<li><p>损失函数衡量得到的结果与目标值的不相似程度，是我们在训练过程中想要最小化的损失函数。</p></li>
<li><p>为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实数据标签值进行比较。</p></li>
</ul>
<p>常见的损失函数包括用于回归任务的 nn.MSELoss（均方误差）和用于分类的 nn.NLLLoss（负对数似然）。 nn.CrossEntropyLoss 结合了 nn.LogSoftmax 和 nn.NLLLoss。</p>
<h3 id="优化器">4、优化器</h3>
<ul>
<li><p>优化是在每个训练步骤中调整模型参数以减少模型误差的过程。</p></li>
<li><p>优化算法定义了这个过程是如何执行的（在这个例子中我们使用随机梯度下降）。</p></li>
<li><p>所有优化逻辑都封装在优化器对象中。</p></li>
<li><p>在这里，我们使用 SGD 优化器； 此外，PyTorch 中有许多不同的优化器可用，例如 ADAM 和 RMSProp，它们更适用于不同类型的模型和数据。 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br></pre></td></tr></table></figure></p></li>
<li><p>我们通过注册模型需要训练的参数并传入学习率超参数来初始化优化器。</p></li>
</ul>
<h4 id="训练循环中优化的三个步骤">训练循环中优化的三个步骤：</h4>
<ul>
<li><p>调用 optimizer.zero_grad() 来重置模型参数的梯度。 梯度默认情况下会相加； 为了防止重复计算，我们在每次迭代时明确地将它们归零。</p></li>
<li><p>通过调用 loss.backwards() 来反向传播预测损失。PyTorch 将损失的梯度存入 w.r.t. 每个参数。</p></li>
<li><p>一旦我们有了梯度，我们就调用 optimizer.step() 通过backward()中收集的梯度来调整参数。</p></li>
</ul>
<h3 id="train-loop-和-test-loop的整体实现">5、Train Loop 和 Test Loop的整体实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_loop</span>(<span class="params">dataloader, model, loss_fn, optimizer</span>):</span></span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    <span class="keyword">for</span> batch, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        <span class="comment"># Compute prediction and loss</span></span><br><span class="line">        pred = model(X)</span><br><span class="line">        loss = loss_fn(pred, y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backpropagation</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            loss, current = loss.item(), batch * <span class="built_in">len</span>(X)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;loss: <span class="subst">&#123;loss:&gt;7f&#125;</span>  [<span class="subst">&#123;current:&gt;5d&#125;</span>/<span class="subst">&#123;size:&gt;5d&#125;</span>]&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_loop</span>(<span class="params">dataloader, model, loss_fn</span>):</span></span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    num_batches = <span class="built_in">len</span>(dataloader)</span><br><span class="line">    test_loss, correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> dataloader:</span><br><span class="line">            pred = model(X)</span><br><span class="line">            test_loss += loss_fn(pred, y).item()</span><br><span class="line">            correct += (pred.argmax(<span class="number">1</span>) == y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    test_loss /= num_batches</span><br><span class="line">    correct /= size</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Test Error: \n Accuracy: <span class="subst">&#123;(<span class="number">100</span>*correct):&gt;<span class="number">0.1</span>f&#125;</span>%, Avg loss: <span class="subst">&#123;test_loss:&gt;8f&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;t+<span class="number">1</span>&#125;</span>\n-------------------------------&quot;</span>)</span><br><span class="line">    train_loop(train_dataloader, model, loss_fn, optimizer)</span><br><span class="line">    test_loop(test_dataloader, model, loss_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Done!&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1
-------------------------------
loss: 2.305831  [    0/60000]
loss: 2.286264  [ 6400/60000]
loss: 2.275372  [12800/60000]
loss: 2.270570  [19200/60000]
loss: 2.243541  [25600/60000]
loss: 2.228191  [32000/60000]
loss: 2.224830  [38400/60000]
loss: 2.197911  [44800/60000]
loss: 2.188963  [51200/60000]
loss: 2.156472  [57600/60000]
Test Error: 
 Accuracy: 44.4%, Avg loss: 2.153718 

Epoch 2
-------------------------------
loss: 2.164675  [    0/60000]
loss: 2.149587  [ 6400/60000]
loss: 2.099088  [12800/60000]
loss: 2.116142  [19200/60000]
loss: 2.057026  [25600/60000]
loss: 2.006994  [32000/60000]
loss: 2.027068  [38400/60000]
loss: 1.953798  [44800/60000]
loss: 1.955666  [51200/60000]
loss: 1.876484  [57600/60000]
Test Error: 
 Accuracy: 54.6%, Avg loss: 1.882143 

Epoch 3
-------------------------------
loss: 1.914309  [    0/60000]
loss: 1.881881  [ 6400/60000]
loss: 1.770059  [12800/60000]
loss: 1.812282  [19200/60000]
loss: 1.696313  [25600/60000]
loss: 1.650967  [32000/60000]
loss: 1.665257  [38400/60000]
loss: 1.568552  [44800/60000]
loss: 1.594818  [51200/60000]
loss: 1.484761  [57600/60000]
Test Error: 
 Accuracy: 61.4%, Avg loss: 1.509315 

Epoch 4
-------------------------------
loss: 1.575193  [    0/60000]
loss: 1.537402  [ 6400/60000]
loss: 1.389850  [12800/60000]
loss: 1.466425  [19200/60000]
loss: 1.346153  [25600/60000]
loss: 1.345708  [32000/60000]
loss: 1.351373  [38400/60000]
loss: 1.272530  [44800/60000]
loss: 1.314754  [51200/60000]
loss: 1.217384  [57600/60000]
Test Error: 
 Accuracy: 63.7%, Avg loss: 1.243355 

Epoch 5
-------------------------------
loss: 1.322318  [    0/60000]
loss: 1.297594  [ 6400/60000]
loss: 1.132705  [12800/60000]
loss: 1.242817  [19200/60000]
loss: 1.121882  [25600/60000]
loss: 1.150953  [32000/60000]
loss: 1.163450  [38400/60000]
loss: 1.091253  [44800/60000]
loss: 1.141570  [51200/60000]
loss: 1.064053  [57600/60000]
Test Error: 
 Accuracy: 65.1%, Avg loss: 1.080539 

Epoch 6
-------------------------------
loss: 1.154191  [    0/60000]
loss: 1.148584  [ 6400/60000]
loss: 0.965631  [12800/60000]
loss: 1.104692  [19200/60000]
loss: 0.985084  [25600/60000]
loss: 1.020977  [32000/60000]
loss: 1.047947  [38400/60000]
loss: 0.976197  [44800/60000]
loss: 1.029986  [51200/60000]
loss: 0.968344  [57600/60000]
Test Error: 
 Accuracy: 66.1%, Avg loss: 0.976119 

Epoch 7
-------------------------------
loss: 1.037672  [    0/60000]
loss: 1.052149  [ 6400/60000]
loss: 0.852071  [12800/60000]
loss: 1.013271  [19200/60000]
loss: 0.898881  [25600/60000]
loss: 0.930241  [32000/60000]
loss: 0.972335  [38400/60000]
loss: 0.901890  [44800/60000]
loss: 0.953780  [51200/60000]
loss: 0.904363  [57600/60000]
Test Error: 
 Accuracy: 67.0%, Avg loss: 0.905581 

Epoch 8
-------------------------------
loss: 0.952564  [    0/60000]
loss: 0.985621  [ 6400/60000]
loss: 0.771567  [12800/60000]
loss: 0.949356  [19200/60000]
loss: 0.841446  [25600/60000]
loss: 0.864854  [32000/60000]
loss: 0.919260  [38400/60000]
loss: 0.852853  [44800/60000]
loss: 0.899662  [51200/60000]
loss: 0.858536  [57600/60000]
Test Error: 
 Accuracy: 68.2%, Avg loss: 0.855451 

Epoch 9
-------------------------------
loss: 0.887674  [    0/60000]
loss: 0.936516  [ 6400/60000]
loss: 0.712008  [12800/60000]
loss: 0.902735  [19200/60000]
loss: 0.800700  [25600/60000]
loss: 0.816214  [32000/60000]
loss: 0.879292  [38400/60000]
loss: 0.819085  [44800/60000]
loss: 0.859679  [51200/60000]
loss: 0.823750  [57600/60000]
Test Error: 
 Accuracy: 69.5%, Avg loss: 0.817833 

Epoch 10
-------------------------------
loss: 0.836203  [    0/60000]
loss: 0.897646  [ 6400/60000]
loss: 0.666050  [12800/60000]
loss: 0.867214  [19200/60000]
loss: 0.769769  [25600/60000]
loss: 0.778995  [32000/60000]
loss: 0.846987  [38400/60000]
loss: 0.794176  [44800/60000]
loss: 0.828553  [51200/60000]
loss: 0.795993  [57600/60000]
Test Error: 
 Accuracy: 70.8%, Avg loss: 0.788062 

Done!</code></pre>
<h3 id="七保存与加载模型">七、保存与加载模型</h3>
<p>PyTorch 模型将学习到的参数存储在称为 state_dict 的内部状态字典中。 这些可以通过 torch.save 方法持久化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line">model = models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model_weights.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading: &quot;https://download.pytorch.org/models/vgg16-397923af.pth&quot; to C:\Users\14012/.cache\torch\hub\checkpoints\vgg16-397923af.pth
4.4%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

11.4%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

18.5%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

25.5%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

35.0%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

45.9%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

54.3%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

61.2%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

69.2%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

76.2%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

86.6%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)

98.3%IOPub message rate exceeded.
The notebook server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--NotebookApp.iopub_msg_rate_limit`.

Current values:
NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
NotebookApp.rate_limit_window=3.0 (secs)</code></pre>
<p>要加载模型权重，您需要先创建相同模型的实例，然后使用 load_state_dict() 方法加载参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = models.vgg16() <span class="comment"># we do not specify pretrained=True, i.e. do not load default weights</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model_weights.pth&#x27;</span>))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<pre><code>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)</code></pre>
<h4 id="注意一定要在预测之前调用-model.eval-方法以将-dropout-和批量归一化层设置为评估模式-不这样做会产生不一致的推理结果">注意：一定要在预测之前调用 model.eval() 方法，以将 dropout 和批量归一化层设置为评估模式。 不这样做会产生不一致的推理结果。</h4>
<p>在加载模型权重时，我们需要先实例化模型类，因为该类定义了网络的结构。 我们可能希望将此类的结构与模型一起保存，在这种情况下，我们可以将模型（而不是 model.state_dict()）传递给保存函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model, <span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = torch.load(<span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch学习笔记5——使用AutoGrad自动微分</title>
    <url>/2021/12/13/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8AutoGrad%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86/</url>
    <content><![CDATA[<h1 id="五使用autograd计算自动微分">五、使用AutoGrad计算自动微分</h1>
<p>在训练神经网络时，最常用的算法是反向传播。 在该算法中，参数（模型权重）根据损失函数相对于给定参数的梯度进行调整。 为了计算这些梯度，PyTorch 有一个名为 torch.autograd 的内置微分引擎。 它支持任何计算图的梯度自动计算。</p>
<p>考虑最简单的一层神经网络，输入 x，参数 w 和 b，以及一些损失函数。 它可以通过以下方式在 PyTorch 中定义：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">5</span>)  <span class="comment"># input tensor</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>)  <span class="comment"># expected output</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)</span><br></pre></td></tr></table></figure>
<p>以下是计算图，因为我们需要计算Loss函数对w和b的梯度，所以在上面我们将w和b的requires_grad设置为了True</p>
<figure>
<img src="https://pytorch.org/tutorials/_images/comp-graph.png" alt="avatar" /><figcaption aria-hidden="true">avatar</figcaption>
</figure>
<p>我们应用于张量来构建计算图的函数实际上是类 Function 的对象。 该对象知道如何在前向计算函数，以及如何在反向传播步骤中计算其导数。 对反向传播函数的引用存储在张量的 grad_fn 属性中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Gradient function for z =&#x27;</span>, z.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Gradient function for loss =&#x27;</span>, loss.grad_fn)</span><br></pre></td></tr></table></figure>
<pre><code>Gradient function for z = &lt;AddBackward0 object at 0x00000271D5D03D90&gt;
Gradient function for loss = &lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x00000271D5CF0F40&gt;</code></pre>
<h3 id="通过-backward函数计算梯度">通过 backward()函数计算梯度</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.2175, 0.0032, 0.1359],
        [0.2175, 0.0032, 0.1359],
        [0.2175, 0.0032, 0.1359],
        [0.2175, 0.0032, 0.1359],
        [0.2175, 0.0032, 0.1359]])
tensor([0.2175, 0.0032, 0.1359])</code></pre>
<p>注意：我们只能获取计算图的叶子节点的 grad 属性，这些节点的 requires_grad 属性设置为 True。 对于图中的所有其他节点，梯度将不可用。</p>
<p>出于性能原因，我们只能在给定的计算图上使用一次backward()来执行梯度计算。如果我们需要在同一个计算图上进行多次backward()调用，我们需要将 retain_graph=True 传递给反向调用。</p>
<h3 id="如何停止跟踪梯度">如何停止跟踪梯度？</h3>
<p>默认情况下，所有具有 requires_grad=True 的张量都在跟踪它们的计算历史并支持梯度计算。 但是，在某些情况下我们不需要这样做，例如，当我们训练了模型并且只想将其应用于某些输入数据时，即我们只想通过网络进行前向计算。 我们可以通过用 torch.no_grad() 块包围我们的计算代码来停止跟踪计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br></pre></td></tr></table></figure>
<pre><code>True
False</code></pre>
<p>或者使用 detach方法也可以完成此项工作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">z_det = z.detach()</span><br><span class="line"><span class="built_in">print</span>(z_det.requires_grad)</span><br></pre></td></tr></table></figure>
<pre><code>False</code></pre>
<h3 id="停止跟踪梯度的一些情况">停止跟踪梯度的一些情况</h3>
<ul>
<li><p>1、将神经网络中的某些参数标记为冻结参数。（通常见于迁移学习）</p></li>
<li><p>2、在仅进行前向传递时加快计算速度，因为对不跟踪梯度的张量进行计算会更有效。</p></li>
</ul>
]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch学习笔记4——Building NeuralNetwork</title>
    <url>/2021/12/13/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04%E2%80%94%E2%80%94%E5%BB%BA%E7%AB%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="四神经网络建立">四、神经网络建立</h1>
<p>神经网络由对数据执行操作的层/模块组成。 torch.nn 命名空间提供了构建自己的神经网络所需的所有构建块。 PyTorch 中的每个模块都是 nn.Module 的子类。 神经网络是一个模块本身，由其他模块（层）组成。 这种嵌套结构允许轻松构建和管理复杂的架构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br></pre></td></tr></table></figure>
<h4 id="步骤-1如果有条件的话使用cuda设备">步骤 1、如果有条件的话，使用Cuda设备</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Using <span class="subst">&#123;device&#125;</span> device&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Using cuda device</code></pre>
<h4 id="步骤-2我们通过继承nn.module来自定义我们的神经网络类在__ini__中完成神经网络层的定义且在forward函数中完成对输入的处理">步骤 2、我们通过继承nn.Module来自定义我们的神经网络类，在__ini__中完成神经网络层的定义，且在Forward函数中，完成对输入的处理</h4>
<p>nn.Sequential() 是一个有序的层的容器，将一系列的层线性组合在一起，按照顺序执行</p>
<p>nn.Flatten() 将每个 2D 28x28 图像转换为 784 个像素值的连续数组（注意：batches的那个维度被保留了（也就是dim=0的维度被保留了），举例来说，一个[3,28,28]的矩阵，被摊平成了[3,784]的矩阵</p>
<p>nn.Linear(in_features=28*28, out_features=20) 全连接层</p>
<p>nn.ReLU() 非线性激活函数层</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<h4 id="步骤-3实例化一个对象并将其移至对应设备打印结构">步骤 3、实例化一个对象，并将其移至对应设备，打印结构</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<pre><code>NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
<h4 id="步骤-4在输入上调用模型会返回一个-10-维张量其中包含每个类的原始预测值">步骤 4、在输入上调用模型会返回一个 10 维张量，其中包含每个类的原始预测值。</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device)</span><br><span class="line">logits = model(X)</span><br><span class="line">logits</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 0.0394, -0.1041,  0.1203,  0.0809,  0.1222,  0.0305,  0.0539, -0.0241,
          0.0910,  0.0014]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
<h3 id="softmax函数logits-被缩放到值-0-1-dim-参数指示该维度上的值的总和必须为-1">Softmax函数：logits 被缩放到值 [0, 1]。 dim 参数指示该维度上的值的总和必须为 1 。</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits) </span><br><span class="line"><span class="built_in">print</span>(pred_probab)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.0996, 0.0863, 0.1080, 0.1038, 0.1082, 0.0987, 0.1011, 0.0935, 0.1049,
         0.0959]], device=&#39;cuda:0&#39;, grad_fn=&lt;SoftmaxBackward0&gt;)
Predicted class: tensor([4], device=&#39;cuda:0&#39;)</code></pre>
<h3 id="神经网络模型参数">神经网络模型参数：</h3>
<p>神经网络内的许多层都是参数化的，即具有在训练期间优化的相关权重和偏差。 子类 nn.Module 会自动跟踪模型对象中定义的所有字段，并使用模型的 parameters() 或 named_parameters() 方法使所有参数都可以访问。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model structure: &quot;</span>, model, <span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Layer: <span class="subst">&#123;name&#125;</span> | Size: <span class="subst">&#123;param.size()&#125;</span> | Values : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Model structure:  NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
) 


Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0354, -0.0090,  0.0262,  ..., -0.0218, -0.0343,  0.0239],
        [-0.0130,  0.0023, -0.0083,  ..., -0.0314,  0.0088,  0.0303]],
       device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) 

Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0199, -0.0234], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) 

Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0136,  0.0326, -0.0428,  ..., -0.0377, -0.0250,  0.0003],
        [-0.0187, -0.0297, -0.0096,  ...,  0.0277, -0.0320,  0.0395]],
       device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) 

Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0206, -0.0259], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) 

Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0047, -0.0406, -0.0102,  ..., -0.0346,  0.0284,  0.0111],
        [-0.0308, -0.0165, -0.0334,  ...,  0.0257,  0.0339,  0.0208]],
       device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) 

Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0353,  0.0320], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) </code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch学习笔记3——Transform数据变换</title>
    <url>/2021/12/13/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%E2%80%94%E2%80%94Transform%E6%95%B0%E6%8D%AE%E5%8F%98%E6%8D%A2/</url>
    <content><![CDATA[<h1 id="三transform">三、Transform</h1>
<p>通常而言，数据不会以处理好的形式出现，所以我们需要在训练前对数据进行预处理，以适应训练</p>
<p>所有 TorchVision 的 Dataset 都会有两个参数—— transform 用于修改特征，target_transform 用于修改标签——它们接受包含转换逻辑的可调用对象（其实就是接受函数对象）。 torchvision.transforms 模块提供了几种常见的转换。</p>
<h3 id="示例">1、示例：</h3>
<p>如下代码为例，我们所拿到的FashionMNIST的特征是一个PIL Image的格式，它的标签是一个Integer整数。但是我们训练的时候，希望特征是一个正则化后的张量，而标签是一个One-Hot向量张量。所以分别采用ToTensor和Lambda函数来进行处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">ds = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor(),</span><br><span class="line">    target_transform=Lambda(<span class="keyword">lambda</span> y: torch.zeros(<span class="number">10</span>, dtype=torch.<span class="built_in">float</span>).scatter_(<span class="number">0</span>, torch.tensor(y), value=<span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>ToTensor() 函数可以将一个 PIL image 或者 NumPy ndarray 转换至一个 FloatTensor，同时放缩图像的像素值范围至0-1.</p>
<p>Lambda可以用于定义任何一个用户定义的lambda表达式，使其成为函数，在上述定义的表达式中： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))</span><br></pre></td></tr></table></figure> 先初始化了一个维度为10的零向量，然后利用scatter_函数，将某一个维度的值赋为1</p>
<h4 id="scatter函数">Scatter函数</h4>
<p>scatter(dim, index, src) 的参数有 3 个，通过一个张量或标量 src 来修改另一个张量，哪个元素需要修改、用 src 中的哪个元素来修改由 dim 和 index 决定 - dim：沿着哪个维度进行索引 - index：用来 scatter 的元素索引 - src：用来 scatter 的源元素，可以是一个标量或一个张量</p>
]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch学习笔记2——Dataset介绍</title>
    <url>/2021/12/13/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%E2%80%94%E2%80%94Dataset/</url>
    <content><![CDATA[<h1 id="二dataset">二、Dataset</h1>
<p>Pytorch 使用 torch.utils.data.DataLoader 和 torch.utils.data.Dataset 来允许我们使用其预先加载好的数据，或是自己准备的数据集。</p>
<p>Dataset存储样本及其相应的标签，DataLoader在数据集周围包装一个iterable，以便于访问样本。</p>
<p>一些常用的Datasets可以在这边进行查询 https://pytorch.org/tutorials/beginner/basics/data_tutorial.html</p>
<h3 id="下载数据集示例">1、下载数据集示例</h3>
<h4 id="有以下四个参数">有以下四个参数</h4>
<h5 id="root-是训练集测试集数据存储的位置">1、root : 是训练集/测试集数据存储的位置</h5>
<h5 id="train-表明是训练集还是测试集">2、train: 表明是训练集还是测试集</h5>
<h5 id="download-表明如果数据集在root不存在是否从网络下载">3、download： 表明如果数据集在root不存在，是否从网络下载</h5>
<h5 id="transform-和-target_transform-指定特征和标签转换">4、transform 和 target_transform 指定特征和标签转换</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\FashionMNIST\raw\train-images-idx3-ubyte.gz


100.0%


Extracting data\FashionMNIST\raw\train-images-idx3-ubyte.gz to data\FashionMNIST\raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\FashionMNIST\raw\train-labels-idx1-ubyte.gz


100.6%


Extracting data\FashionMNIST\raw\train-labels-idx1-ubyte.gz to data\FashionMNIST\raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\FashionMNIST\raw\t10k-images-idx3-ubyte.gz


100.0%


Extracting data\FashionMNIST\raw\t10k-images-idx3-ubyte.gz to data\FashionMNIST\raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\FashionMNIST\raw\t10k-labels-idx1-ubyte.gz


119.3%

Extracting data\FashionMNIST\raw\t10k-labels-idx1-ubyte.gz to data\FashionMNIST\raw</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>可以使用Index来访问数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img, label = training_data[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h4 id="pytorch中squeeze和unsqueeze函数">pytorch中squeeze()和unsqueeze()函数</h4>
<p>torch.squeeze() 主要对数据的维度进行压缩，去掉维数为1的的维度，比如是一行或者一列，一个一行三列（1,3）的数去掉第一个维数为一的维度之后就变成（3）行</p>
<p>torch.unsqueeze() 给指定位置加上维数为一的维度，比如原本有个三行的数据（3），在0的位置加了一维就变成一行三列（1,3）。a.unsqueeze(N) 就是在a中指定位置N加上一个维数为1的维度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(img.shape)</span><br><span class="line"><span class="built_in">print</span>(img.squeeze().shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([1, 28, 28])
torch.Size([28, 28])</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.imshow(img.squeeze())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="output_14_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<h3 id="创建自定义的数据集">2、创建自定义的数据集</h3>
<p>一个自定义的数据集类必须要实现以下三个函数： <strong>init</strong>, <strong>len</strong>, and <strong>getitem</strong></p>
<h4 id="首先是init函数当实例化数据集对象的时候初始化目录标签文件等等">1）首先是Init函数，当实例化数据集对象的时候，初始化目录、标签文件、等等</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):</span><br><span class="line">    self.img_labels = pd.read_csv(annotations_file, names=[&#x27;file_name&#x27;, &#x27;label&#x27;])</span><br><span class="line">    self.img_dir = img_dir</span><br><span class="line">    self.transform = transform</span><br><span class="line">    self.target_transform = target_transform</span><br></pre></td></tr></table></figure>
<h4 id="len__函数返回数据集中样例的数量">2) __len__函数返回数据集中样例的数量</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def __len__(self):</span><br><span class="line">    return len(self.img_labels)</span><br></pre></td></tr></table></figure>
<h4 id="getitem__函数根据给定的idx从数据集中加载并且返回一个样例">3）__getitem__函数根据给定的idx从数据集中加载并且返回一个样例</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def __getitem__(self, idx):</span><br><span class="line">    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])</span><br><span class="line">    image = read_image(img_path)</span><br><span class="line">    label = self.img_labels.iloc[idx, 1]</span><br><span class="line">    if self.transform:</span><br><span class="line">        image = self.transform(image)</span><br><span class="line">    if self.target_transform:</span><br><span class="line">        label = self.target_transform(label)</span><br><span class="line">    return image, label</span><br></pre></td></tr></table></figure>
<h3 id="使用dataloaders来为训练准备数据">3、使用DataLoaders来为训练准备数据</h3>
<p>先前定义的Dataset能够一次检索一个样本的数据集功能和标签。但是在训练模型时，我们通常希望以“batches”的方式传递样本，在每个epoch重新排列数据以减少模型过度拟合，并使用Python的多处理来加速数据检索。</p>
<p>DataLoader 就是帮助我们完成上述事情的一个东西，下面就是将数据装载进DataLoader的过程，可以迭代访问数据集中的内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>下面的每次迭代都会返回一批训练实例和训练标签（分别包含batch_size=64个特征和标签）。因为我们指定了shuffle=True，所以在对所有批进行迭代之后，数据将被洗牌。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dataloader</span><br></pre></td></tr></table></figure>
<pre><code>&lt;torch.utils.data.dataloader.DataLoader at 0x1f88dccf4f0&gt;</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">iter</span>(train_dataloader)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x1f88e0796d0&gt;</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br></pre></td></tr></table></figure>
<pre><code>[tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           ...,
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],
 
 
         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
           ...,
           [0.0000, 0.0000, 0.4510,  ..., 0.8157, 0.0549, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.4196, 0.0000, 0.0000],
           [0.0000, 0.0078, 0.0275,  ..., 0.0000, 0.0000, 0.0000]]],
 
 
         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           ...,
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],
 
 
         ...,
 
 
         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           ...,
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],
 
 
         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           ...,
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],
 
 
         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           ...,
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]),
 tensor([3, 9, 0, 2, 7, 4, 4, 5, 1, 5, 3, 0, 6, 9, 7, 7, 6, 4, 1, 7, 7, 7, 3, 7,
         4, 4, 9, 9, 5, 1, 4, 2, 2, 0, 9, 3, 9, 9, 9, 8, 2, 4, 9, 3, 2, 0, 0, 0,
         5, 7, 5, 7, 1, 9, 8, 7, 0, 4, 9, 0, 2, 3, 8, 7])]</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_features, train_labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Feature batch shape: <span class="subst">&#123;train_features.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Labels batch shape: <span class="subst">&#123;train_labels.size()&#125;</span>&quot;</span>)</span><br><span class="line">img = train_features[<span class="number">0</span>].squeeze()</span><br><span class="line">label = train_labels[<span class="number">0</span>]</span><br><span class="line">plt.imshow(img, cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Feature batch shape: torch.Size([64, 1, 28, 28])
Labels batch shape: torch.Size([64])</code></pre>
<figure>
<img src="output_31_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<pre><code>Label: 5</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch学习笔记1——Tensor介绍</title>
    <url>/2021/12/13/Pytorch%E7%AC%94%E8%AE%B0/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94Tensor%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<h2 id="一tensor-的初始化">一、Tensor 的初始化</h2>
<h3 id="从数据初始化">1、从数据初始化</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br><span class="line">x_data</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1, 2],
        [3, 4]])</code></pre>
<h3 id="从nparray初始化">2、从nparray初始化</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br><span class="line">x_np</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1, 2],
        [3, 4]], dtype=torch.int32)</code></pre>
<h3 id="从另一个tensor初始化新的tensor会继承作为参数的tensor的-形状和数据类型除非显式声明">3、从另一个Tensor初始化，新的Tensor会继承作为参数的Tensor的 形状和数据类型，除非显式声明</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># retains the properties of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line">x_rand = torch.rand_like(x_data, dtype=torch.<span class="built_in">float</span>) <span class="comment"># overrides the datatype of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Ones Tensor: 
 tensor([[1, 1],
        [1, 1]]) 

Random Tensor: 
 tensor([[0.8288, 0.8223],
        [0.4349, 0.4734]]) </code></pre>
<h3 id="使用随机值进行初始化-rand-ones-zeros函数">4、使用随机值进行初始化 rand() ones() zeros()函数</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shape = (<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Zeros Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Random Tensor: 
 tensor([[0.5132, 0.3130, 0.5135, 0.7438],
        [0.9011, 0.3348, 0.6246, 0.7321]]) 

Ones Tensor: 
 tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.]]) 

Zeros Tensor: 
 tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.]])</code></pre>
<h2 id="二tensor的三个重要属性shapedtypedevice">二、Tensor的三个重要属性:shape,dtype,device</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Shape of tensor: torch.Size([3, 4])
Datatype of tensor: torch.float32
Device tensor is stored on: cpu</code></pre>
<h2 id="三tensor的相关操作">三、Tensor的相关操作</h2>
<p>在默认情况下，Tensor将会被创建于CPU上，我们可以使用以下方法将其复制至GPU中，但是大型的Tensor在拷贝的过程中所耗费的代价是比较昂贵的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    tensor = tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tensor.device</span><br></pre></td></tr></table></figure>
<pre><code>device(type=&#39;cuda&#39;, index=0)</code></pre>
<h3 id="一些常规操作">1、一些常规操作：</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;First row: &#x27;</span>, tensor[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;First column: &#x27;</span>, tensor[:, <span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Last column:&#x27;</span>, tensor[..., -<span class="number">1</span>])  <span class="comment"># 等价于 tensor[:,-1]</span></span><br><span class="line">tensor[:,-<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>
<pre><code>First row:  tensor([1., 1., 1., 1.])
First column:  tensor([1., 1., 1., 1.])
Last column: tensor([1., 1., 1., 1.])
tensor([[1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.]])</code></pre>
<h3 id="torch.cat-和-torch.stack">2、Torch.cat() 和 Torch.stack()</h3>
<h4 id="torch.cat-沿着一个维度进行堆叠不会改变原tensor的维度即原来是2维矩阵堆叠完还是二维矩阵">torch.cat() 沿着一个维度进行堆叠,不会改变原Tensor的维度，即原来是2维矩阵，堆叠完还是二维矩阵</h4>
<p>示例：dim=0的时候，原来的4x4的矩阵 A 会变成 [[A],[A],[A]]纵向叠加，即12x4的矩阵，示例：dim=1的时候，原来的4x4的矩阵 A 会变成 [A,A,A]横向叠加，即4*12的矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="number">0</span>)</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.]])</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>)</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],
        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],
        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.],
        [1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.]])</code></pre>
<h4 id="torch.stack-沿着一个维度进行堆叠但是会在原tensor的维度上增加一个维度即原来是2维矩阵堆叠完会变成3维矩阵">torch.stack() 沿着一个维度进行堆叠,但是会在原Tensor的维度上增加一个维度，即原来是2维矩阵，堆叠完会变成3维矩阵</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = torch.stack([tensor, tensor, tensor], dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(t1.shape)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br><span class="line">t1 = torch.stack([tensor, tensor, tensor], dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1.shape)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br><span class="line">t1 = torch.stack([tensor, tensor, tensor], dim=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(t1.shape)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([3, 4, 4])
tensor([[[1., 1., 1., 0.],
         [1., 1., 1., 0.],
         [1., 1., 1., 0.],
         [1., 1., 1., 0.]],

        [[1., 1., 1., 0.],
         [1., 1., 1., 0.],
         [1., 1., 1., 0.],
         [1., 1., 1., 0.]],

        [[1., 1., 1., 0.],
         [1., 1., 1., 0.],
         [1., 1., 1., 0.],
         [1., 1., 1., 0.]]])
torch.Size([4, 3, 4])
tensor([[[1., 1., 1., 0.],
         [1., 1., 1., 0.],
         [1., 1., 1., 0.]],

        [[1., 1., 1., 0.],
         [1., 1., 1., 0.],
         [1., 1., 1., 0.]],

        [[1., 1., 1., 0.],
         [1., 1., 1., 0.],
         [1., 1., 1., 0.]],

        [[1., 1., 1., 0.],
         [1., 1., 1., 0.],
         [1., 1., 1., 0.]]])
torch.Size([4, 4, 3])
tensor([[[1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.],
         [0., 0., 0.]],

        [[1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.],
         [0., 0., 0.]],

        [[1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.],
         [0., 0., 0.]],

        [[1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.],
         [0., 0., 0.]]])</code></pre>
<h3 id="算术运算">3、算术运算</h3>
<h4 id="计算矩阵乘法的几种方式">计算矩阵乘法的几种方式</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line">y1 = tensor @ tensor.T</span><br><span class="line">y2 = tensor.matmul(tensor.T)</span><br><span class="line">y3 = torch.rand_like(tensor)</span><br><span class="line">torch.matmul(tensor, tensor.T, out=y3)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.]])





tensor([[3., 3., 3., 3.],
        [3., 3., 3., 3.],
        [3., 3., 3., 3.],
        [3., 3., 3., 3.]])</code></pre>
<h4 id="element-wise-product-点对点乘积矩阵对应元素相乘得到结果">element-wise product 点对点乘积（矩阵对应元素相乘得到结果）</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z1 = tensor * tensor</span><br><span class="line">z2 = tensor.mul(tensor)</span><br><span class="line"></span><br><span class="line">z3 = torch.rand_like(tensor)</span><br><span class="line">torch.mul(tensor, tensor, out=z3)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.]])</code></pre>
<h3 id="in-place-操作">4、In-Place 操作</h3>
<p>像一些会将结果存储到操作数里的计算，我们将之称为In-Place操作，它们会在操作符后面加上后缀 "_"</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(tensor, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">tensor.add_(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 0.]]) 

tensor([[6., 6., 6., 5.],
        [6., 6., 6., 5.],
        [6., 6., 6., 5.],
        [6., 6., 6., 5.]])</code></pre>
<h3 id="bridge-with-numpy机制">5、Bridge with NumPy机制</h3>
<p>Tensors和Numpy在计算机底层可能共享同一块内存，改变其中一个变量就会影响另外一个，需要注意</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.ones(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">n = t.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>t: tensor([1., 1., 1., 1., 1.])
n: [1. 1. 1. 1. 1.]</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>t: tensor([2., 2., 2., 2., 2.])
n: [2. 2. 2. 2. 2.]</code></pre>
]]></content>
      <categories>
        <category>Pytorch学习笔记</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer系列笔记4——Swin Transformer思想与架构</title>
    <url>/2021/12/09/Transformer%E7%AC%94%E8%AE%B0/Transformer%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B04%E2%80%94Swin%20Transformer/</url>
    <content><![CDATA[<h4 id="论文名称swin-transformer-hierarchical-vision-transformer-using-shifted-windows">论文名称：《Swin Transformer: Hierarchical Vision Transformer using Shifted Windows》</h4>
<h4 id="论文地址httpsarxiv.orgabs2103.14030">论文地址：https://arxiv.org/abs/2103.14030</h4>
<h4 id="模型swin-transformer">模型：Swin Transformer</h4>
<h3 id="一论文摘要与swin-transformer背景介绍">一、论文摘要与Swin Transformer背景介绍</h3>
<p>​ 论文摘要部分即指出了Transformer在视觉领域应用与NLP领域的一些较大的差别，首先就是在视觉领域，视觉实体的规模尺度会存在较大的变化，举例而言，想要识别同一张图像中的同一距离的果实和汽车，两者的规模尺度大小可能会存在较大的不同。第二点就是相比于文字而言，图像像素的分辨率更高，信息量更多，计算更为复杂，需要消耗更多的计算资源与训练时间。相比于先前的Vision Transformer的架构，Swin Transformer使用Shifted Windows这样一个技巧，不仅大大加速了计算速度，和图像的尺度呈线性计算复杂度，并且其也仍然能够考虑到不同窗口之间的信息交互，在识别质量上也提高了2-3个百分点。故而成为了应用性非常广泛的一个架构。那么具体而言，如何加速计算，并且仍然能够考虑不同窗口的信息交互，后面会详细讲解。</p>
<h3 id="二swin-transformer整体架构介绍">二、Swin Transformer整体架构介绍</h3>
<p>​ 在上一篇笔记中，讲过了ViT网络架构，相比于ViT网络架构，Swin Transformer考虑了多尺度下的图像，可以看到如下图所示：其通过不断的下采样，在多个图像的尺度维度进行检测，这样的话直观上可以很好的解决我们先前说的视觉实体的规模尺度的大小不同的问题。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120312596.png" /></p>
<p>​ 同样，我们先来看一下Swin Transformer整体的一个架构流程，如下所示：可以看到，我们假设输入的图像是高为H，宽为W，RGB三通道24位真彩图，首先其会有一个Patch Partition层，图像宽高减少为原来的1/4，维度由3变至48. 然后后面跟着四个阶段的运算，除了第一个阶段中经过的是Linear Embedding层，后续每个阶段开始都会经过Patch Merging也就是对图像进行下采样，然后经过堆叠的Swin Transformer Block。此处值得注意的是：我们观察到，Swin Transformer Block的堆叠次数都是偶数，原因在于在该架构中，Block是成对出现的，看到下图的右侧部分，有两个不同的Block，我们暂且将之称为Block1和Block2，如果Swin Transformer Block堆叠2次那么就是Block1+Block2,如果Swin Transformer Block堆叠6次那么就是Block1+Block2+Block1+Block2+Block1+Block2。</p>
<p>​ 在每一个Block中，LN就是Layer Norm层，这个层在先前的Transformer架构中已经出现过多次了，然后MLP是多层感知级，通常由全连接层构成，在Swin Transformer的Block中，比较新的就是W-MSA模块和SW-MSA模块，也就是论文标题中所提到的Shift-Window-Multihead-Self-Attention机制。此机制我们在后续一个个模块时会进行详细讲解。</p>
<p>​ 在整体的流程中，还有一个内容值得我们注意，也就是上面显示的每经过一个阶段，矩阵的维度变化。我们发现，每过一个阶段特征图的宽和高都会减半，而随之特征图数量就会翻倍。其实每一个不同的阶段就是在不同的图像尺度下去进行内容模式的识别观察。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/cvbvbng.png" /></p>
<h3 id="三swin-transformer细节介绍">三、Swin Transformer细节介绍</h3>
<h4 id="patch-partition-linear-embedding介绍">1、Patch Partition &amp; Linear Embedding介绍</h4>
<p>​ 如下图所示，对于输入的Images来说，我们假设输入的图像为H*W*3，那么在Patch Partition的过程中会将其分割成4*4大小的块，然后将每一个块在维度方向进行展平，也就是说先前的H*W*3的矩阵经过此步骤后，长宽会变成H/4 * W/4，至于最后一个维度，将会变为3*16 = 48维，因为我们将其分割成4*4大小的块后，是在唯独方向对其进行展平的。展平后，再通过Stage1的一个Linear Embedding层，该层就是对原来的第三维度为48维的三维矩阵进行一个编码（其实就是再进行一个映射），然后三维矩阵的最后一个维度的大小就会变为C。具体C为多少，是不同类型的Swin Transformer的一个参数，后文中会详细提及。在这两层的实际实现中，其实都是通过卷积来实现维度的变化以及编码的，比如在Patch Partition中就可以使用大小为4*4,步长为4的16个卷积核来对原图进行卷积得到结果。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120418491.png" /></p>
<h4 id="patch-merging">2、Patch Merging</h4>
<p>​ Patch Merging层所起到的作用就是将图片下采样，然后在深度方向进行拼接。主要由如下图所示的几步组成：</p>
<p>首先是以2 * 2的格子为一组，将每组中相同位置的像素抽出，形成一个新的矩阵。以下图为例，原特征矩阵为4 * 4，那么以2 * 2格子为1组，会形成4组新的矩阵，每组的矩阵大小为2 * 2，也就是原来的一半。然后将这4组新形成的矩阵，在深度方向做连接，然后完成LayerNorm，最后再在深度维度进行一个线性映射，将深度维度减半。此时就完成了最终的Patch Merging层的输出。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120447510.png" /></p>
<p>​ 对比该模块的输入与输出，可以发现，输出的H和W是输入的一半，深度维度是输入的2倍。符合先前全流程示意图中的H/4 * W/4 * C 变成了H/8 * W/8 * 2C，成功的将图像空间上的尺寸缩小，由较好的保留了信息。</p>
<h4 id="w-msa模块">3、W-MSA模块</h4>
<p>​ W-MSA 全称为Windows Multi-head Self-Attention也就是窗口化的Self-Attention机制，此处以在一个4*4的特征图上做为例子，在上一篇Vision Transformer中的MSA模块，4<em>4中的每个像素都要去和其他像素进行关联度的计算，那么在W-MSA中，其将原4</em>4的特征图首先分割成了4个2*2的Window窗口，然后再在每个窗口内部进行单独的Self-Attention的计算。也就是说，每个像素只需要和自己所属Window内部的像素进行关联度的计算即可。这样一来，确实大大减少了计算量，但是你会发现窗口之间的像素也无法进行通信了，导致我们的感受野变小，对于最终的结果产生影响。优劣势还是非常的明确的。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120506850.png" /></p>
<p>​ 在原论文中，给出了MSA和W-MSA的计算复杂度的推导结果公式，我们假设每个窗口含有M * M个像素，计算图像的宽高为h和w，C即为矩阵的第三维度的大小（也就是深度）。详细的推导此处省略。我们从公式中应该可以看出，相比于MSA平方的复杂度（对于hw），W-MSA相对于hw的复杂度是线性的。能够大大的提高计算效率。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120516476.png" /></p>
<h4 id="sw-msa模块">4、SW-MSA模块</h4>
<p>​ 相比于W-MSA而言，SW-MSA才是本篇论文中的重点模块，该模块弥补了W-MSA窗口与窗口间无法进行信息交互的缺陷，同时也保证了和W-MSA一样的计算复杂度。</p>
<p>​ 如下图所示，我们先前说过，在Swin-Transformer中，堆叠的Block都是偶数次，是两个不同的Block的组合。我们可以看到，下图的Layerl所示是第一次的Block堆叠，使用的是W-MSA，然后在Layerl+1中，我们将window进行了重新的分割，然后在每个window中完成计算，新的window中有些仍然是老的window的一部分，但有些新的window已经含有老的多个window的信息了，也就是完成了window间的信息的交互。</p>
<p>那么现在就让我们来一一解决以下两个问题：</p>
<p>​ 1、这个是如何进行重新的分割的呢？</p>
<p>​ 2、这样子重新分割以后，由于要计算的Window数目变成了9个，如果需要并行计算的话，需要将每个window都填充至4*4，这样子就会加大我们的计算量。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120536278.png" /></p>
<p>​ 首先是如何分割的问题：Shifted Window顾名思义，我们将原来的分割线向右和向下偏移一定的像素，然后对原图形成的分割线，就是新的window的划分线。以上面的示意图为例，就是将原来的分割线向右以及向下移动了2个像素点，就得到了新的划分。这个移动的距离，一般来说是窗口的一半，也就是<span class="math inline">\(Math.floor(M/2)\)</span>。</p>
<p>​ 接下来是第二个问题，原论文中给出了如下的示意图：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120606229.png" /></p>
<p>​ 我们将所有window中的一些window进行一个划分，0所在的部分标志为A，1，2所在的标志为C，3，6所在的标志为B，如图1所示，然后进行一个平移，也就是变成图2的形式，这样子的话我们发现又可以将其组成4个新的4 * 4的window进行self-Attention的计算，计算量和以前一致。到此为止，你可能会问，我们不是要按照原来划分的9个window进行计算吗？如果按照现在这个平移过的4 * 4的window进行计算，原先window和window之间的边缘都是跳跃的，而且我们本来就是要原来每个window内部单独计算的，只是为了减轻计算量才进行的平移，所以计算某一个新的4 * 4的混合window的时候，自然不希望内部的两个原来的window之间信息有交互和混合。（就比如说，我们平移后，变成了4号块自己算自己的，5、3号块合起来算，1、7号块合起来算，2、0、6、8号块合起来算，4号块是没有问题的，内部像素本身就是连续的，但是5、3号块一起算就会出现问题，5号块和3号块边缘处的像素是不连续的，而且理论上而言，我们是要单独计算3、5号块各自的Self-Attention的内部像素关联度的，所以我们在计算区域5的时候不要引入区域3的信息）</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120637174.png" /></p>
<p>​ 所以，平移以后对4个新的4*4的window进行的Self-Attention是我们先前提到过的Masked MSA,也就是让每一个新形成的window中，根据原来的分割规则给它套上一个mask。这样子的话，虽然它们在一起训练，但是通过这个mask，仍然使得原来的9个window划分规则中，不同的window之间的像素是不会计算关联度的，或者说关联度为0.</p>
<p>​ 举例来说，如下图所示：我们计算区域5和区域3的这块的Self-Attention的时候，原先是计算了16个α值，那么我们等它计算完以后，将图中蓝色框圈出来的系数全都减去100，这个减100的含义是什么呢？原先计算出的α系数，一般都是比较小量级的，减去100以后，必定是一个比较大的负数，那么经过SoftMax的计算以后，系数就会变成0，也就实现了区域5和区域3之间的像素如果计算关联度，那么就是0。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120650353.png" /></p>
<p>​ 最后的话，我们要注意，计算完以后将对应的小块移回原位置。</p>
<h4 id="relative-position-bias介绍">5、Relative Position Bias介绍：</h4>
<p>​ 在论文中还简单介绍了这样一种相对位置偏移的计算机制。这种机制应用于计算Self-Attention的时候。如下公式所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120718204.png" /></p>
<p>​ 其给出了一组数据，在不同的学习网络中，是否使用偏移、或者是使用相对/绝对位置偏移来计算Attention会导致的结果误差。我们可以发现相对位置偏差能够将结果提升1个百分点左右。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120725660.png" /></p>
<p>​ 那么相对位置偏差是怎么确定的呢？如下所示，假设我们的特征图是左侧的2*2的格子，下方是我们熟知的绝对位置索引，相对位置索引如右侧上面一排所示，其实就是当前计算格子的绝对位置索引减去其他格子的绝对位置索引。然后将四个像素的相对位置索引展开后拼接在一起形成一个新的矩阵。这个矩阵就是二维的相对位置索引矩阵。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/iiioio.png" /></p>
<p>​ 在作者的源码中，其使用的是1维的的相对位置索引矩阵，我们不能简单的将x,y相加，不然可能导致不同位置的相对位置索引一致，导致出现问题。所以作者在源码中经过了一个简单处理。我们先把所有的行列标加上M-1，然后再将行标乘2M-1，然后再将行列标相加，得到的矩阵。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120758495.png" /></p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120804811.png" alt="image-20220115120804811" /><figcaption aria-hidden="true">image-20220115120804811</figcaption>
</figure>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120807646.png" alt="image-20220115120807646" /><figcaption aria-hidden="true">image-20220115120807646</figcaption>
</figure>
<p>​ 然后我们需要把Relative Position Index通过一张Bias Table映射成relative position bias才是用于计算Self-Attention最终用于计算的Bias值，也就是公式里的矩阵B。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120814669.png" /></p>
<h3 id="四swin-transformer-模型扩展参数">四、Swin Transformer 模型扩展参数</h3>
<p>​ 论文中给出了以下一张表格，里面是四种不同的Swin模型的各个阶段的参数。其中有Swin-T,Swin-S,Swin-B,Swin-L四种不同的Swin Transformer模型，分别代表Tiny。</p>
<p>​ 我们解析某一列的参数，concat 4 * 4 就代表要将高和宽下采样4倍，96-d就代表经过Linear Embedding层以后的C大小。接下来的括号内的东西就代表堆叠的block内的参数，window size = 7*7,通过该Block之后输出维度为96，Multi_Head的Head = 3。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/bvbcf123.png" /></p>
]]></content>
      <categories>
        <category>Transformer系列笔记</category>
      </categories>
      <tags>
        <tag>Swin Transformer</tag>
        <tag>Transformer</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer系列笔记3——Vision Transformer思想与架构</title>
    <url>/2021/12/08/Transformer%E7%AC%94%E8%AE%B0/Transformer%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B03%E2%80%94Vision%20Transformer%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<p>论文名称：《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》</p>
<h4 id="论文地址httpsarxiv.orgabs2010.11929">论文地址：https://arxiv.org/abs/2010.11929</h4>
<h4 id="模型vision-transformer-vit">模型：Vision Transformer (ViT)</h4>
<h3 id="一论文摘要与vision-transformer背景介绍">一、论文摘要与Vision Transformer背景介绍</h3>
<p>​ 自从Transfomer机制出现以后，已经成为了NLP领域的业界标准流程，但是在CV领域的应用较少。CV领域中往往都是会将其和CNN结合使用，或是代替CNN架构中的一部分。这篇论文就提出了其实在CV领域中，Transformer并不需要依赖于CNN来进行使用，其提出的Vision Transformer架构，在基于一些大体量数据的预训练的网络参数然后再迁移学习到一些训练任务时，能够取得非常好的效果，并且也能节省训练的计算资源。所以Vision Transformer是应用于CV领域的，不简单依托于CNN架构的一类Transformer架构的衍生内容。在此论文中，其应用于了CV领域的图象识别任务。</p>
<h3 id="二vision-transformer网络架构">二、Vision Transformer网络架构</h3>
<p>​ 如下图所示，左侧为ViT网络的概览，可以看到，该网络将图片分为一小块一小块的，将每一个小块视为一个向量(被称为Flattened Patches),然后图中的Linear Projection of 这层可以被视为是一个Embedding层，将每一块图像Embedding成为一个对应的向量，然后再在这一系列向量前面，新增一个用于分类的向量，叫做Class Token,再给每一个向量加上一个表示位置信息的向量。随后将这些向量输入进Transformer Encoder中，我们知道，Encoder对于一个输入序列中的每一个向量，都会输出一个对应的向量，我们仅取Class Token输入对应的输出向量，将其放入MLP Head模块（这是一个最终用于分类的层结构）中，然后再由MLP Head模块输出分类类别。Transformer Encoder内部的块结构与Transformer的结构类似，其中MLP是新的模块，后续会详细再讲。到此为止就是整个ViT网络基本的架构。接下来会按照顺序，一个一个模块的进行详细的介绍。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115115915557.png" /></p>
<h4 id="linear-projection-of-flattened-patchesembedding层">1、Linear Projection of Flattened Patches（Embedding层）</h4>
<p>​ 对于标准的Transformer的Encoder模块，一般要求输入的是向量序列，也就是应该是一个二维矩阵[num_token,token_dim]。在该网络架构的代码实现中，可以直接通过一个卷积层来实现，比如说ViT-B/16，B代表Base，16代表分割以16*16为单位分割源图像。在这个网络中，使用卷积核大小为16*16，步长为16。</p>
<p>​ 如果我们使用的图片是[224,224,3]的大小，那么经过卷积以后就会变成[14,14,768]的大小，然后展平就会最终变成[196,768]的二维矩阵。</p>
<p>​ 在输入到Encoder之前，还需要加上 用于分类的向量class token和position embedding，这两者都是可训练参数。</p>
<p>​ 拼接[class]token:Cat([1,768],[196,768]) -&gt;[197,768]</p>
<p>​ 叠加Position Embedding: [197,768] -&gt; [197,768]</p>
<p>​ Position Embedding在此处是通过直接叠加相加的方法完成的，所以矩阵的维度不会发生变化，同时，原论文中给出了不同维度的Position Embedding的效果，我们发现1维和2维的效果差不多，但是都比没有Position Embedding的结果要好，所以在实现过程中就可以实现一维的Pisition Embedding进行叠加。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115115949427.png" /></p>
<p>​ 那么，具体叠加的一维的Position Embedding应该是什么呢？如下图所示，我们对于每个位置计算其和其他位置的余弦相似度。即可得到叠加的具体数值。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120001864.png" /></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120004895.png" /></p>
<h4 id="transformer-encoder层">2、Transformer Encoder层</h4>
<p>​ 其和Transformer中Encoder类似，由许多个Block组成</p>
<p>​ 我们先看Encoder Block的结构，其中基本结构与原始的Transformer的Encoder Block类似，但是其中有一个不太一样的MLP Block结构。MLP Block的结构显示如右侧，其中是几个Linear层，然后配合一些Dropout和激活函数。MLP Block的第一个Linear层，会将原始的输入扩大4倍，就例如如果Linear层的输入是197 * 768的，那么该层的输出是197 * 3072，然后第二个Linear层又会将输出变回197*768的大小。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120029921.png" /></p>
<h4 id="mlp-head层">3、MLP Head层</h4>
<p>​ 此层在训练不同的数据集时，组成模块不太一样。如果训练ImageNet21K或一些更大的数据集的时候，该层由Linear+tanh激活函数+Linear层组成。但是如果迁移到ImageNet1K或者一些小规模的数据集上，那其实只需要一个Linear层即可。具体的内容可以查看论文的源码部分。</p>
<h3 id="三以vit-b16-为例的vision-transformer架构">三、以ViT-B/16 为例的Vision Transformer架构</h3>
<p>​ 我们可以看到先前所讲的各个模块组合在一起以后，整个网络的一个架构。图中的Pre-Logits部分其实就是一个全连接层+激活函数，视情况可以舍弃。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120118127.png" /></p>
<h3 id="四不同vit模型的参数">四、不同ViT模型的参数</h3>
<p>​ 在论文的Table1中有给出三个模型（Base/ Large/ Huge）的参数</p>
<ul>
<li><p>Layers 是Transformer Encoder中重复堆叠Encoder Block的次数。</p></li>
<li><p>Hidden Size是通过Embedding层后每个向量的长度（token 的 dim）。</p></li>
<li><p>MLP Size 是Transformer Encoder中MLP Block第一个全连接的节点个数，一般来说是HiddenSize的4倍。</p></li>
<li><p>Heads 代表Transformer中Multi-Head Attention的heads数。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120147916.png" /></p></li>
</ul>
<h3 id="五hybrid混合模型">五、Hybrid混合模型</h3>
<p>​ 引用：https://blog.csdn.net/qq_37541097/article/details/118242600</p>
<p>在论文中其也提及了Hybrid混合模型，其就是先前摘要中提到的将传统CNN特征提取和Transformer进行结合的做法。</p>
<p>​ 其首先用传统的CNN提取特征，然后再用Vit模型进一步得到最终的结果。</p>
<p>​ 整体的网络架构如下：以ResNet50作为特征提取器的混合模型，但这里的Resnet有些不同。首先这里的R50的卷积层采用的StdConv2d不是传统的Conv2d，然后将所有的BatchNorm层替换成GroupNorm层。同时在原Resnet50网络中，stage1重复堆叠3次，stage2重复堆叠4次，stage3重复堆叠6次，stage4重复堆叠3次，但在这里的R50中，把stage4中的3个Block移至stage3中，所以stage3中共重复堆叠9次。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115120211640.png" /></p>
]]></content>
      <categories>
        <category>Transformer系列笔记</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>Vision Transformer</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer系列笔记2——原始Transformer架构与Seq2Seq问题</title>
    <url>/2021/12/07/Transformer%E7%AC%94%E8%AE%B0/Transformer%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B02%E2%80%94%E5%8E%9F%E5%A7%8BTransformer%E6%9E%B6%E6%9E%84%E4%B8%8ESeq2Seq%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h3 id="一相关背景介绍">一、相关背景介绍</h3>
<p>​ Transformer架构最广泛用在Seq2Seq问题上，也就是Sequence-to-Sequence。区别于GAN所对应的Pix2Pix的问题，Seq2Seq的问题也会有非常多的应用和变式，其应用场景也是非常的广。</p>
<p>​ Seq2Seq问题是指输入一个序列，输出一个序列，同时输出序列的长度由训练好的模型进行决定。比较经典的问题就是语音识别、机器翻译、语音翻译等领域。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112932803.png" /></p>
<p>​ 同时，Seq2Seq也可以用于Chatbot聊天机器人，比如说如下所示：我们可以将Person1和Person2的一组对话视为一个input和一个对应的response，两者都是序列。或者一些其他的Question&amp;Answering问题都可以广泛的应用Seq2Seq的模型解决思想。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112939154.png" /></p>
<p>​ 具体的一些相关论文可以参照下图所示的网址进一步了解。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112946229.png" /></p>
<p>​ 同时，还有更多的可以应用Seq2Seq的例子，比如说对于一个Syntactic Parsing问题，也就是语法解析，我们也可以将其视为Seq2Seq的形式，来对其进行思考。输入是一句话，毫无疑问是一个Seq序列。但是，我们所需要的输出结果看上去像是一个树形结构【deep learning组成名词短语，very powerful 组成形容词短语，is 和 very powerful又组成动词短语，最后deep learning和 is very powerful组成一个句子】，但其实，如下图所示，我们可以将图最上方的这个句子作为输出结果，这样就将一个类似图结构的内容转换成了Seq的形式，同时也包含了所有我们需要的结果信息，然后用Seq2Seq的思想去训练模型即可。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112954818.png" /></p>
<p>​ 再比如，Seq2Seq应用于图像领域的目标检测，感兴趣可以查看以下这篇论文。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113003795.png" /></p>
<h3 id="二seq2seq2结局方案origin-transformer">二、Seq2Seq2结局方案——Origin Transformer</h3>
<p>​ 对于一般的Seq2Seq问题，笼统而言，我们一般就是将输入序列经过一个Encoder然后再通过一个Decoder就得到我们的输出序列。但是这个Encoder和Decoder内部的结构就大有讲究，最经典的架构就是Transformer。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113028194.png" /></p>
<h4 id="encoder">1、Encoder</h4>
<p>​ 首先我们关注Transformer的Encoder部分。下面是从输入输出的结果来看，Encoder的输入是一个序列，输出是一个编码过后的序列。（其实只从输入输出来看，像RNN、CNN都可以做到输入一个序列，输出一个序列，但是此篇讲的是Transformer）</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113035456.png" /></p>
<p>​ Transfomer中的Encoder现在一般都由这样的架构组成，如下图所示，由非常多的Block组成，每一个Block并不是指一层网络，可能是有许多曾网络组成。那么输入经过许多个Block的处理，然后输出。那么在Transformer中，每一个Block内部的架构如右侧所示：输入一排向量，先经过Self-attention的机制，输出一排向量，然后这一排向量再经过全连接层，输出一排向量，这一排带红色框的向量就是Block的输出。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113045808.png" /></p>
<p>​ 但实际上，在原来的Transformer中，Block内的结构稍微更复杂一些，其还增加了一个residual的过程，如下所示：</p>
<p>​ 输入的某个向量b经过Self-attention以后，得到向量a,此向量a还要与向量b相加，得到一个新的向量a+b，然后再做Layer Norm,得到一个深蓝色的向量c。（左上角的那个框框代表的就是，接下去接右侧的过程）</p>
<p>​ 然后这个Norm输出的深蓝色的向量c，经过FC层，得到新的向量d，此处还有一个residual的架构，c和d相加，得到新的向量e，然后e再做一次Layer Norm才会得到最终这个Block输出的结果向量。</p>
<p>（LayerNorm本身的过程如图中央所示，先计算向量数据的均值和标准差，然后依据公式xi’ = (xi-m)/σ即可完成归一化。）</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113058827.png" /></p>
<p>​ 讲完每一个Block内完成的事情，我们就可以看Transfomer中给出的这个完整的带细节的架构示意图，其实两者中有部分过程是重叠的，只是表示形式不一样。</p>
<p>​ 首先将输入经过嵌入层，完成编码。如果对嵌入层没有印象，可以查看如下的文章。然后通过Positional Encoding向输入中增加序列不同地方的位置信息。然后经过多次结构的核心部分Nx。这个Nx其实就是我们刚才提到的一个Block。其中，Multi-Head Attention就是Self-Attention的一种扩展的架构，然后Add&amp;Norm就是上述的Residual + LayerNorm的过程。Feed Forward就是上图中通过FC层的过程。所以这张图中Nx内部的这个框框就是一个Block的过程，然后整个Transformer Encoder就由许多个这样的Block组成。【像先前所讲的】</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113109977.png" /></p>
<p>​ 其实Transformer Encoder的架构也是可以灵活改变的，此处只是讲了最初提出的Transformer时候文章中Encoder的架构。</p>
<h4 id="decoder">2、Decoder</h4>
<p>​ 然后的话我们关注Transformer的Decoder部分。Decoder分为两种，一种是Autoregressive（AT）的Decoder,还有一种是Non-Autoregressive（NAT）的Decoder。首先是Autoregressive的Decoder的介绍：</p>
<p>​ Decoder的输入，有两个内容，一个是Encoder输出的序列，还有一个是START标识符号所对应的One-hot编码，这两样东西经过Decoder后，输出一个各种字的概率分布，其中概率最大的那个字就会被作为最后的输出字符。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113200035.png" /></p>
<p>​ 然后我们将START 和 机 组成的one-hot向量的序列输入，再经过Decoder，得到一个概率分布，然后选出概率分布中最高的那个字“机”。以此类推，直到完成所有的输出。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113209026.png" /></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113211451.png" /></p>
<p>​ 在此处其实存在两个问题 ，一个问题是按照上述的过程，其实Decoder后面每一步的输入都来自于自己前一步的输出，就比如说，中间某一步输入是“机器”，输出了“学”，然后下一步输入是“机器学”，输出是“习”。那么，我们要如何避免一步错步步错的情况发生呢？也就是说，如果其中有一步发生了问题，我们应当怎么让机器在后面的步骤中仍然输出正确的结果。第二个问题是，如果按照上述的步骤生成下去，生成出来的序列是无穷无尽的，它永远不会停止，那么机器怎么去决定输出序列的长度呢？这两个问题，后文会有提及，我们先继续讲述Decoder内部到底经历了什么。</p>
<p>​ 我们先来看一张Encoder和Decoder的对比图：从此图而言，我们发现Decoder除了中间那一块被盖住的，以及最后输出层的一些内容以外，Block内部的内容其实和Encoder是类似的。一个比较重要的差别点就是其使用的是Masked Multi-Head Attention。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113222282.png" alt="image-20220115113222282" /><figcaption aria-hidden="true">image-20220115113222282</figcaption>
</figure>
<p>​ 我们先来看一下，这个Masked Multi-Head Attention是什么意思呢？图1是我们熟悉的Self-Attention，每一个输出的向量都是含有输入的所有向量的咨询的，右图是Masked Self Attention机制，也就是在生成b1的时候，是不能使用a1之后的输入向量的，只能使用a1的资讯，生成b2的时候，不能使用a2之后的输入向量，只能使用a1,a2的资讯，以此类推。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113244478.png" alt="image-20220115113244478" /><figcaption aria-hidden="true">image-20220115113244478</figcaption>
</figure>
<p>下面是一个更为具体的示意图：</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113256453.png" alt="image-20220115113256453" /><figcaption aria-hidden="true">image-20220115113256453</figcaption>
</figure>
<p>​ 那么为什么要使用这样的架构呢？原因其实很简单，因为如果不这样子的话，AT形式的Decoder是没法的运作的。因为先前讲的AT的Decoder运作方式，输出的字是一个一个产生的。第一步只有一个输入的“START”标识符对应的One-Hot向量，生成一个字以后，再将START标识符和第一个字输入，得到第二个字，依次类推。所以在生成的时候，它只能参考它前面的输入的序列的资讯，没法获得它之后的资讯。</p>
<p>​ 然后我们来看一下Encoder间和Decoder间是如何传递资讯的呢？也就是刚才那张图中，暂时被灰色遮住的部分：我们会发现，在这个红色框框框起来的模组中，有两个输入的箭头来自于Encoder，一个输入的箭头来自于Decoder的前一步的输出。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113305054.png" alt="image-20220115113305054" /><figcaption aria-hidden="true">image-20220115113305054</figcaption>
</figure>
<p>​ 那么具体来说，是怎么工作的呢，如下所示：此步中的Multi-Head Attention指的是一个Cross-Attention的过程，具体如下所示：在Decoder解析的第一步时，输入是来自Encoder的一个序列，以及Decoder上一步的输出（因为Decoder上一步无输出，此步为第一步，所以这个应该是一个START标识符，表示句子的开始）。</p>
<p>​ 然后Encoder这边输出的序列，a1,a2,a3向量分别乘以一个权重矩阵，形成k1,v1,k2,v2,k3,v3，Deocder这侧的START标识符对应的One-hot编码经过Masked-Self-Attention得到一个向量，然后该向量乘上一个权重矩阵，得到向量q,计算q和k1,k2,k3的相关性并且归一化，得到系数α1’，α2’，α3’.</p>
<p>​ 得到系数α1’，α2’，α3’后，分别和v1,v2,v3相乘相加，得到向量v，然后再经过FC就是该步骤Cross attention的输出，至于后面就是需要再经过上述网络中描述的Add&amp;Norm，也就是Residual和Layer Norm的过程，才完成Decoder中一个Block的过程。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113324988.png" alt="image-20220115113324988" /><figcaption aria-hidden="true">image-20220115113324988</figcaption>
</figure>
<p>​ 然后第二步以此类推，如下图所示，此处就不再赘述。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113331325.png" alt="image-20220115113331325" /><figcaption aria-hidden="true">image-20220115113331325</figcaption>
</figure>
<p>​ 讲到这里的话，Decoder的基本架构也都已经清晰了，我们来解决一下先前的两个遗留问题之一，也就是机器怎么决定输出的序列的长度的？</p>
<p>​ 在AT的Decoder中，我们其实可以在输出的字符集中增加一个叫做END的字符，它就代表一个句子的结束，如果某次Decoder输出的概率分布中，END字符的概率较高，就说明该句子结束了。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113340249.png" alt="image-20220115113340249" /><figcaption aria-hidden="true">image-20220115113340249</figcaption>
</figure>
<p>​ 那么什么又是NAT的Decoder呢？我们先前看到AT的Decoder是一个个输出字符的，NAT的Decoder则是一次性的输入全是START的序列，然后一次性得到输出的字符序列。那么NAT的Decoder又是如何决定输出序列的长度的呢？有两种办法，一个是专门训练一个分类器或者预测器，去预测输出序列的长度，另一个就是我们先假设这个输出的序列不会超过每个定值，比如说300个字符，那么我们就输入一个300个START组成的序列，然后在输出的字符序列中，忽略END字符后面的字符即可。</p>
<p>​ 相比AT来说，NAT有着并行化，更稳定的优势，但是NAT的效果往往比AT要差一些。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113351438.png" alt="image-20220115113351438" /><figcaption aria-hidden="true">image-20220115113351438</figcaption>
</figure>
<h3 id="三如何训练transformer">三、如何训练Transformer</h3>
<p>​ 首先，当我们丢入第一个START字符的时候，希望Decoder输出的Distribution分布，和我们的Ground Truth的结果，能够越接近越好，也就是要最小化cross entropy的值。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113410387.png" alt="image-20220115113410387" /><figcaption aria-hidden="true">image-20220115113410387</figcaption>
</figure>
<p>​ 由于我们会输出好多次字符，所以我们最终是希望，能够最小化每次输出的Distribution和Ground Truth的cross entropy。不要忘记，最后一步输出段落的结尾符号，也要考虑在内。【此处有一个题外话，在实现的时候，其实有些时候可以将START的标识符和END标识符表示为同一个One-Hot编码，因为反正START标识符只会出现在句子的头部，仍然是可以分辨的，不需要区分START和END这两个字符】</p>
<p>​ 同时，此处我们在训练的时候，Decoder的Input不是上一步输出的内容，而是给它正确的答案。这件事情就被我们叫做Teacher Focing。但是这样子和在实际生产使用的时候存在一个不匹配的问题，这个问题我们之后会有所讨论。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113420273.png" alt="image-20220115113420273" /><figcaption aria-hidden="true">image-20220115113420273</figcaption>
</figure>
<h3 id="四关于训练seq2seq类似模型的tips">四、关于训练Seq2Seq类似模型的Tips：</h3>
<h4 id="copy-mechanism">1、Copy Mechanism</h4>
<p>​ 这一个机制在聊天机器人或文档摘要等领域应用的比较广泛，举例来说，有的时候输出的某些内容可以从输入中Copy进行完成，这样就避免机器去学习一些奇怪的词汇，比如说在聊天机器人中库洛洛这样一个人名信息。感兴趣可以继续查看相关的论文</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113453661.png" alt="image-20220115113453661" /><figcaption aria-hidden="true">image-20220115113453661</figcaption>
</figure>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113456022.png" alt="image-20220115113456022" /><figcaption aria-hidden="true">image-20220115113456022</figcaption>
</figure>
<h4 id="guided-attention">2、Guided Attention</h4>
<p>​ 由于有的时候机器可能会忽略输入的某一些部分，此时就可以使用该种手段，该手段通常应用于语音合成、语音辨识领域。</p>
<p>其就是要求机器在做Attention的时候以一个固定的方式，也就是说如果你对于某一个任务已经有了一些既定的发现，就可以将这种限制加入到训练的过程中，引导机器完成Attention的计算过程。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113509972.png" alt="image-20220115113509972" /><figcaption aria-hidden="true">image-20220115113509972</figcaption>
</figure>
<h4 id="beam-search">3、Beam Search</h4>
<p>​ BeamSearch是为了解决如下的一个问题：如下图所示，举例而言，比如说我们输出的字符只有A和B两种，那么Decoder的输出序列，就可以表示为如下图所示的一颗数状结构。红色的路径是Decoder按照先前的贪婪规则得到的输出。但其实，有一条比红色路径更好的输出，就是绿色的路径。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113522389.png" alt="image-20220115113522389" /><figcaption aria-hidden="true">image-20220115113522389</figcaption>
</figure>
<p>​ 可是，我们如果想要找到一个训练过程中这样一条绿色的最优路径是比较难的，因为我们不可能去穷举搜索每一条路径，因为在字符集比较大的情况下，几层叠加的可能就已经非常之多。这个时候就可以使用Beam Search这样一种技术。具体如何进行可以Google搜索详情</p>
<h4 id="optimizing-evaluation-metric">4、Optimizing Evaluation Metric</h4>
<p>​ 我们在训练的时候，目标是最小化每个一一对应的中文字的输出的分布和Ground Truth，而在评估一个模型好坏的时候，我们往往会使用输出的整句和GroundTruth之间的BLEU score来进行评估，所以我们的验证集应当使用BLEU score。那么我们的训练过程中为什么不使用BLEU score呢？简单来说就是如果在训练过程中我们要做两个句子之间的BLEU score，是根本没有办法做微分的也就没有办法做梯度下降去最优化求解。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113543303.png" alt="image-20220115113543303" /><figcaption aria-hidden="true">image-20220115113543303</figcaption>
</figure>
<h4 id="scheduled-sampling">5、Scheduled Sampling</h4>
<p>​ 这个方法是为了解决我们先前提出的两个问题中的第一个问题，以及在那之后提出的一个训练与实际应用过程中的那个Mismatch(也就是训练的时候Decoder的输入使用的是Ground Truth,可是输出的时候不行)。简单直接的想法，就是训练的时候，我们往输入中加入一些噪声，就可以了。具体而言的话，也有许多论文是做这个方向的，如果有兴趣的话可以自行查阅。</p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113600427.png" alt="image-20220115113600427" /><figcaption aria-hidden="true">image-20220115113600427</figcaption>
</figure>
]]></content>
      <categories>
        <category>Transformer系列笔记</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>Self Attention</tag>
        <tag>Seq2Seq</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer系列笔记1——Self Attention机制</title>
    <url>/2021/12/06/Transformer%E7%AC%94%E8%AE%B0/Transformer%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B01%E2%80%94SelfAttention%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h3 id="一背景介绍问题引入">一、背景介绍问题引入</h3>
<p>​ 在先前的网络架构中，我们一般输入的都是一个向量，但是有的时候我们需要考虑更为复杂的输入，如下图所示，比如说一个向量的集合，并且向量集合的长度还会发生变化。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115111555364.png" /></p>
<p>​ 一般按来说对于输入是向量集合的话，我们先要对其进行编码处理，一种编码方案为One-hot Encoding,称为独热向量编码，另一种编码方案为Word Embedding方法，称为词嵌入方式。</p>
<p>为了更好的理解，输入为一个向量的集合，下面有一些相关的例子：</p>
<p>​ <strong>示例1：</strong>比如说，输入是一段语音，我们可以将一段1s中的语音视为一个向量的集合输入，每25ms视为一个window窗口，称为1帧，将里面的信息提取出来以后，就成为一个向量。我们将这个窗口每次向后移动10ms，每移动10ms就重新取一次样，也就是多输出一个向量。这样预处理的话，其实就是一段1s的语音我们可以把它视作为100个向量的一个集合输入。 （在上述处理的过程中，至于为什么25ms视为一个window，每次要向后移动10ms，这些都需要视具体的语音任务而定，有兴趣可以进一步看语音处理方面相关的文章）</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115111649129.png" /></p>
<p>​ <strong>示例2</strong>：比如说一个图，也可以被视作一个向量的集合，每个节点都被当作一个向量，这个向量中编码了关于这个节点所有的相关信息。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112432503.png" /></p>
<p>​ 那么对于这样复杂的输入而言，输出可能是什么呢？如下图所示，有三种可能，第一个是对于每个输入的向量，都输出一个label结果，比如说POS tagging任务。第二个是整体就输出一个label结果，比如说Sentiment analysis任务。又或者是第三个所示的seq2seq的任务，比如说语句翻译，或者是语音识别的任务。此篇文章聚焦于第一种情况探讨Self-Attention机制与整体的架构。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112440152.png" /></p>
<h3 id="二self-attention解决方案引入">二、Self-Attention解决方案引入：</h3>
<p>​ 为了解决上面我们说的对于每个输入的vector都输出一个label结果的事情，最简单也最为直观的方法就是如下图所示，假设输入的向量集合长度为4，那么我们将每个输入的向量单独考虑，然后进行结果的输出。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112458834.png" /></p>
<p>​ 但是这样一来，我们相当于没有考虑向量与向量之间的关系，那么这种架构下我们可能考虑上下文吗？答案是可以的，如下图所示：我们可以将前后的向量也输入到当前的全连接层中，这就考虑到了上下文信息。但是这样一来问题又发生了，如果我们想要考虑整个序列的上下文呢？那么这个窗口的长度到底应该选多少呢？要知道我们输入的向量的个数是会变化的。同时，如果将每个向量都要连接到每个FC中的话，需要训练的参数就会大大增加，随之而来的就是非常容易过拟合。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112507468.png" /></p>
<p>​ 在这样一个问题背景下，Self-Attention机制应运而生。如下面两张图片所示，我们可以理解为，每一个单独的向量经过Self-Attention机制计算后，得到一个新的向量，这个新的向量中是含有所有的上下文信息的。同时Self-Attention也可以做好多层，也就是说我们可以像第二张图一样，先走一次Self-Attention然后过FC层，然后再走一次Self-Attention层，然后再过全连接层，然后输出。使得Self-Attention发扬光大的是一篇叫做《Attention is All you Need》的论文，那么Self-Attention内部，到底是如何考虑进上下文之间的关系的呢？</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112528529.png" /></p>
<h3 id="三self-attention架构介绍">三、Self-Attention架构介绍</h3>
<p>​ 我们首先从结果来看Self-Attention做了一件什么事情，它的输入可以是原始input，也可以是隐藏层。然后经过该结构后，就可以将向量a1变成一个带着上下文信息的向量b1。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112544934.png" /></p>
<p>​ 首先，我们以生成向量b1为例，记录整一个过程。如下图所示，首先我们需要考虑a1和另外三个向量的相关性，然后用一个变量α去进行衡量。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112553361.png" /></p>
<p>​ 那么如何计算得到这个α的值呢，我们就需要一个如下所示的模块，这个模块输入两个向量，一个向量乘以矩阵Wq得到向量q，一个乘以矩阵Wk得到向量k，然后我们再把q和k做点积，就得到了α，这个α就代表输入的两个向量之间的相关性。至于Wq和Wk内部是什么我们先不用管，因为其内部的参数是我们需要用训练集训练网络得到的。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112601252.png" /></p>
<p>​ 所以，我们就能够依据上述这个框架计算出a1分别和a1,a2,a3,a4的相关性α(1,1),α(1,2),α(1,3),α(1,4)，我们将之称为Attention Score。具体写成公式的形式就是下图下面的半部分。然后的话呢我们需要将之通过Soft-max函数，做一个归约化，使得他们的和为1，得到α’(1,1),α’(1,2),α’(1,3),α’(1,4)。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112614866.png" /></p>
<p>​ 有了α’(1,1),α’(1,2),α’(1,3),α’(1,4)之后，我们先将a1,a2,a3,a4乘上矩阵Wv得到v1,v2,v3,v4，然后用对应的α’和v做叉积，从v中提取信息，最终将所有对应的叉积相加，就得到了最终的结果b1。形式化的公式写在下图的右上方。然后b2,b3,b4也是同样的过程去进行计算。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112623695.png" /></p>
<p>​ 按照我们介绍的顺序b1,b2,b3,b4我们需要一一算出，其实这一计算过程是可以通过矩阵乘法并行执行的。首先，我们将a1,a2,a3,a4拼接组合成一个大的矩阵，称为I。将I分别乘上矩阵Wq，Wk，Wv之后，就会得到大的矩阵Q,K,V。从结果上而言，大的矩阵Q,K,V就是先前一个个向量计算时候得到的结果向量q1,q2,q3,q4拼接组合而成。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112631291.png" /></p>
<p>​ 我们现在拥有Q,K,V三个矩阵，我们可以用矩阵K的转置乘上矩阵Q就会得到一个α的矩阵，其从结果上而言等价于一个个α计算得到的结果组成的矩阵。具体的推导过程见下图，还是比较好理解的，只是把一些重复的矩阵运算进行了合并而已。得到对应的矩阵A以后，我们对每一列做SoftMax归一化操作，得到矩阵A’。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112638932.png" /></p>
<p>​ 最后我们将矩阵A’和矩阵V相乘，就能生成结果矩阵O。O就是b1,b2,b3,b4向量的拼接。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112647706.png" /></p>
<p>​ 最后，我们可以将整个过程表示如下图：Wq,Wk,Wv三个矩阵内部的参数就是我们需要训练的参数，A’就是我们常说的Attention Matrix.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112657080.png" /></p>
<h3 id="四self-attention架构扩展改进">四、Self-Attention架构扩展改进</h3>
<h4 id="multi-head-self-attention-多头注意力机制">1、Multi-head Self-Attention 多头注意力机制</h4>
<p>​ 在上述的计算过程中，我们就是用qi去找相关的ki，但是相关可能会存在很多种，所以我们应该需要有不同的q，然后不同的q去负责不同的相关性，于是乎出现了Multi-head Self-Attention架构。如下所示，相比于Self-Attention，其只是将计算得到的qi和ki进行拆分。我们先从a乘上矩阵Wq变成q，然后再将q乘上两个不同的矩阵W(q,1)，W(q,2)，分别变成两个不同的向量q1和q2。k和v也是同样的进行操作。然后的话用q1,k1,v1去按照之前的方法计算出一个b1出来，再用q2,k2,v2去计算出一个b2出来</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112728011.png" /></p>
<p>最后再将b1和b2组合起来，乘上一个权重矩阵，得到最终的结果向量b.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112737330.png" /></p>
<h4 id="positional-encoding">2、Positional Encoding</h4>
<p>​ 我们会发现，在先前的Self-Attention架构中，其并没有考虑输入向量组之间向量的远近对于其相关性的影响。就比如a1如果和a2换了位置，也不会影响a1和a4的相关性度量。所以我们会想要有如下的想法：对于输入向量集合中的每个向量来说，都先加上一个不同的向量e，然后再去进行先前的操作。这个e可以是人工指定的，也可以是机器学到的规律。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112754051.png" /></p>
<p>​ 关于e究竟需要是怎么样的东西效果才会好，有许许多多的文章研究。此处只是讲解一个基本的做法，具体感兴趣可以查看下面的论文。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112802708.png" /></p>
<h3 id="五self-attention应用">五、Self-Attention应用</h3>
<p>​ Self-Attention机制应用广泛，最多的就是应用在Transformer架构和BERT架构中，广泛的用于NLP领域。但除了NLP领域，其实其在很多领域也适用。</p>
<p>​ 比如说下图所示的语音处理领域，我们最开始就说过如何将一段语音看作是一个向量集合作为输入，然后就可以采用Self-Attention的机制，进行相关的任务。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112819190.png" /></p>
<p>​ 或者在图像处理的领域，我们可以将一张RGB图的每一个像素点的RGB三个值，视为一个向量输入，然后一张Image就可以被视为一个ImageWidth*ImageHeight长度的向量的集合。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112827842.png" /></p>
<p>​ 以下两篇是使用了Self-Attention架构的有关与图像生成和图像检测的论文，后续可能会出相关的论文阅读笔记。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112836400.png" /></p>
<p>​ 那么Self-Attention和CNN到底有啥区别呢？其实按照我们上面对图像的理解，CNN可以看作是一个只有一小部分感知域的Self-Attention。因为CNN的其中一个特征，就是它有独特的感知域，只感知某像素周围一部分和该像素的关联，而Self-Attetion则是会考虑整个序列，也就是该像素和所有像素之间的关联。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112848232.png" /></p>
<p>​ 有人做过研究，在《On the Relationship between Self-Attention and Convolutional Layers》这篇论文中，以严格的数学证明了CNN是Self-Attention的子集。也就是说在一定条件下，Self-Attention可以和CNN实现完全相同的效果。</p>
<p>​ 同时，在《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》这篇文章中，作者也比较了两者的差别。训练数据越多，Self-Attention最后拟合的效果越好，而CNN在训练数据较少时，效果较好。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112858420.png" /></p>
<p>​ 此处还有一些Self-Attention用于RNN和Graph领域的内容，由于与我所学习的领域相关性不大，所以就暂且忽略了，感兴趣可以去李宏毅老师的Attention的课程最后找到对应的论文内容。</p>
]]></content>
      <categories>
        <category>Transformer系列笔记</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>Self Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN系列笔记6——评估GAN的相关方法指标</title>
    <url>/2021/12/05/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B06%E2%80%94%E8%AF%84%E4%BC%B0GAN%E7%9A%84%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<h3 id="前言">前言：</h3>
<p>​ 本篇笔记为观看李宏毅老师的GAN相关课程后所记录，文中所有图片及内容均来源于李宏毅老师的课程，此处只是搬运+以自己的理解进行记录。全文讲述了如何对GAN生成的图像结果进行一个更为客观的评估的相关方法。</p>
<h3 id="一传统方法如何评估结果好坏">一、传统方法如何评估结果好坏？</h3>
<p>​ 在传统方法中，我们一般会采用计算<strong>Likelihood</strong>的方法来进行衡量。我们训练出了一个Generator，然后我们就会用生成器的生成分布概率去进行计算其生成真实数据的可能性。具体的执行步骤就是，先采样部分真实数据点，然后计算G想要生成每个真实图像数据点的概率，然后求和平均即可得到Likelihood。但是在GAN中，这种方法是行不通的，因为GAN训练好以后是一个网络，是没法算出其产生出某一张图像的概率的。（像传统，如果这个Generator是一个高斯生成模型，是可以进行计算的）</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105752191.png" /></p>
<h3 id="二kernel-density-estimation">二、Kernel Density Estimation</h3>
<p>​ 有一种解决方案如下：输入一堆数据Data，经过生成器后，会产生对应的一堆高维向量（其实就是图像），我们将每一个高维向量视为一个高斯分布的均值，这样一来每一个sample就被近似为了一个高斯分布。然后我们再将这些高斯分布进行融合，就能够得到一个高斯混合模型。这一高斯混合模型是可以计算形成某个图像的概率的。但是使用这次步骤做出来的也会有许多难点和问题，比如不知道需要用多少高斯分布去拟合，不知道需要采样多少点。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105815158.png" /></p>
<p>​ 同时，退一万步讲，其实使用Likelihood来进行估计就不是一个很严谨的事情，Likelihood低，对应的图像质量也可能高。Likelihood高，对应的图像质量也可能低。如下图所示，就是两种可能的情况。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105822595.png" /></p>
<h3 id="三常用评估方法">三、常用评估方法：</h3>
<p>​ 现在的文献，大部分常常看到的用于评估GAN的方法是，使用一个已经训练好的Classifier（可以是一个VGG，也可以是Inception Net），来判断这个生成器生成的图像质量好坏。</p>
<p>我们往往有以下两个评估原则，如下图所示：</p>
<p>​ <strong>原则1</strong>:将一个generated image输入分类器，查看分类器输出的属于各个类的概率，如果概率比较集中，说明生成器生成的图片效果比较好，因为分类器可以很明确的分出来。</p>
<p>​ <strong>原则2</strong>:将一系列generated image输入分类器，查看分类器输出的各个类各存在几张图像，如果分完类以后每个类别的实例差不多的话，说明生成器效果较好，如果数量偏向于某种类别，说明生成器会生成偏向于某一个类别的图像。所以我们希望分出来的分布能够尽量的平滑。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105839124.png" /></p>
<p>​ 有了上述原则以后，我们就可以使用下图所示的Inception Score来进行指标的量化。其中的第一项代表原则1，第二项代表原则2.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105847271.png" /></p>
]]></content>
      <categories>
        <category>GAN系列笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>GAN Evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN系列笔记5——GAN应用于特征提取（InfoGAN，VAEGAN，BiGAN）</title>
    <url>/2021/12/04/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B05%E2%80%94GAN%E5%BA%94%E7%94%A8%E4%BA%8E%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%EF%BC%88InfoGAN%EF%BC%8CVAEGAN%EF%BC%8CBiGAN%EF%BC%89/</url>
    <content><![CDATA[<h3 id="前言">前言：</h3>
<p>​ 本篇笔记为观看李宏毅老师的GAN相关课程后所记录，文中所有图片及内容均来源于李宏毅老师的课程，此处只是搬运+以自己的理解进行记录。全文讲述了InfoGAN、VAEGAN、BiGAN，以及GAN在特征提取领域相关的应用。</p>
<h3 id="一infogan">一、InfoGAN</h3>
<p>​ 我们在先前的GAN的介绍中说到过，对于一个训练好的GAN，我们是希望输入一个向量，然后GAN会给我们生成一个Object（图像或者是序列等）。然后我们期望，我们输入的向量，每一个维度可能就会代表输出的图像的某一个特征。这其实是非常理想的情况，但实际上，可能当你改变输入向量某一个维度的值的时候，输出的内容并没有按照某个特征进行变化。如果用图示意的话，如下图所示：两幅图是在低维空间中高维特征的分布示意图，我们本身期望其应该是左图的分布，这样当某个纵轴不变，横轴的数值变化时，高维某个相对应特征也会随之变化，但实际上分布可能像右图一样。InfoGAN就是想要解决这样一个问题。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105527683.png" /></p>
<p>​ InfoGAN的框架如下图所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105540261.png" /></p>
<p>​ 首先会将输入向量z，拆成两部分，一部分是向量c，一部分是向量z’，然后输入G后产生图像x，同时，我们需要训练一个分类器，这个分类器要能从x中识别出，产生这个x所使用的向量c的部分的编码，原本需要训练的判别器也是需要训练的缺一不可。然后我们宏观的看一下这一个架构，Generator和Classifier这一套就像是一个类AutoEncoder的东西，只不过其和正常的AutoEncoder刚好相反。正常的AutoEncoder的编码器是输入图片，输出特征编码向量，然后解码器是再反解回图片。而此处的编码器部分是GAN的Generator部分，输入向量，生成图片，然后decoder部分需要从图片中识别出code。在这个架构中，Discriminator是必须的，不然的话训练Generator的结果很有可能就是会把向量c直接附在图片x的信息中间，这样的话Classifier必定能够反解出c，但是这样的话生成的x是不是一张真实的图片就尚未可知了。有了Discriminator就可以保证生成的x是真实。</p>
<p>​ 同时，Discriminator和Classifier在实际训练时往往共享参数，因为它们的输入都是x，只是最后一层输出层不一样而已。</p>
<p>​ 那么，加上这样一个Classifier以后，为什么先前的原先GAN的输入对输出的影响不明确这个问题就能解决了呢？如下图所示：</p>
<p>​ 因为我们需要分类器能从x中恢复出编码c，所以向量c这一部分必定需要对x的图像的某些维度有明确的影响。也就是说，完成GAN训练以后，c部分的每一个维度，都会对x的图像的某一个特征有明确的影响对应，那么z'部分相对应的可能就是那些较为随机的维度。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105550729.png" /></p>
<p>​ 在这里可能会存在一个疑问，我们怎么划分向量z到c和z’呢？哪些维度应该被划分为c？哪些应该被划分为z’？此处要注意：我们并不是先对z进行手动划分的，是在训练完成之后，那部分能被恢复的向量编码维度，才会被称为c。</p>
<h3 id="二vae-gan">二、VAE GAN</h3>
<p>​ VAE GAN可以看作用GAN强化VAE AutoEncoder。网络架构如下图所示：这样的网络架构完成了两件事，一个是原本的希望重构损失越小越好，另一个是Discriminator会保证Generator生成的图片越真实越好。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105607065.png" /></p>
<p>​ 从VAE角度，因为我们如果只考虑VAE的重建损失，往往产生的图片是不真实的，是会很模糊的。新增了Discriminator可以让输出图像更加真实。</p>
<p>​ 从GAN角度，其不只考虑判别器的反馈，因为其知道真实图片长什么样，需要最小化生成图像和原图的误差，所以VAE GAN训练起来会更加稳定。</p>
<p>​ 如下所示是VAE GAN训练的算法：整体的算法如下所示，在每一轮迭代中，首先是图像的生成：有三类x需要生成：1、第一类x是从数据库中直接采样出来的图片。2、第二类是由数据库中采样出的图片经过Encoder再经过Decoder后重建的图片3、第三类图片是，由随机的概率分布生成的编码z通过Encoder以后产生的图像。（注意图中第10行公式En应为De，图片有误）</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105620194.png" /></p>
<p>​ 然后我们先更新训练EN的参数，希望重建误差越小越好，同时也希望x经过Encoder产生出来的z跟P(z)产生出来的z越接近越好，也就是两者KL散度越小越好。</p>
<p>​ 然后更新训练De的参数，希望重建误差越小越好，同时也希望将第2类和第3类图片，丢入判别器以后，得到的值越大越好。</p>
<p>​ 最后更新训练Discriminator的参数，希望第1类图像进入判别器以后值越大越好，同时也希望将第2类和第3类图片，丢入判别器以后，得到的值越小越好。</p>
<h3 id="三bigan">三、BiGAN</h3>
<p>​ BiGAN的架构如下所示：它修改了传统AutoEncoder的部分，然后新增了判别器。在这个网络中有一个Encoder，输入一张图片x，输出code z。有一个Decoder，输入一个code z，输出一张图片。但是这个输入的code z并不是先前Encoder输出的z，而是从一个事先准备好的分布中取样得到的z。然后我们有一个判别器，输入 向量z和 图像x，要让判别器判断，这一对 （向量z，图像x）是来自于编码器的输入输出，还是来自于解码器的输入输出。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105637394.png" /></p>
<p>​ 训练算法如下所示，整体而言描述和其他的描述语言比较一致，比较好理解，此处就不多赘述，</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105645100.png" /></p>
<p>​ 那么为什么BiGAN能够这样子去进行训练呢？其实BiGAN就是干了一件这样的事情：如果我们把Encoder的输入输出看作是一个Join起来的分布，Decoder的输入输出看作是一个Join起来的分布，那么判别器所干的事情就是区分P和Q之间的分布的差距。想要尽可能的去优化Encoder和Decoder，使得P和Q逐渐变得接近。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105653272.png" /></p>
<p>​ 如果这个模型训练到最优，就应当是Encoder和Decoder可以互相解。那其实你会发现就是做了如下这么一件事情：训练一个AutoEncoder和一个Inverse的AutoEncoder。确实没错，如果如下所示的这样一个网络训练到最优，其和使用BiGAN训练到最优的结果是一样的，但是实际上BiGAN和AutoEncoder都没法训练到最优。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105700505.png" /></p>
<p>​ 所以在不是收敛到最优的情况下，BiGAN会比下面这个架构训练好很多。生成的图片会真实很多，会比AutoEncoder架构生成的图像清晰很多。两者的特性不太一样。举个例子，比如可能用BiGAN，你输入一个鸟的图片，经过Encoder，Decoder后生成的图像也会是一只清晰的真实的鸟，但可能跟输入的鸟不一样，而如果用AutoEncoder架构，输入一个鸟的图片，经过Encoder，Decoder后生成的图像会和原图像比较像，但是会比较模糊，是一个不是很清晰真实的图像。</p>
]]></content>
      <categories>
        <category>GAN系列笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>InfoGAN</tag>
        <tag>VAEGAN</tag>
        <tag>BiGAN</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN系列笔记4——GAN训练过程中的一些Tips与改进的GAN网络（含LSGAN、WGAN、EBGAN）</title>
    <url>/2021/12/03/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B04%E2%80%94%E5%8E%9F%E5%A7%8BGAN%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9BTips%E4%B8%8E%E6%94%B9%E8%BF%9B%E7%9A%84GAN%E7%BD%91%E7%BB%9C%EF%BC%88%E5%90%ABLSGAN%E3%80%81WGAN%E3%80%81EBGAN%EF%BC%89/</url>
    <content><![CDATA[<h3 id="一tips1-有关如何更有效的训练判别器">一、Tips1 有关如何更有效的训练判别器</h3>
<p>​ 在第三份笔记中，我们讲到其实在训练GAN的过程中，我们就是在计算最小化JS divergence的这样一个过程。但是其实JS divergence其实存在一个非常严重的问题，根源在于generation data的分布和real data的分布往往是没有任何重叠的，这个没有任何重叠的问题是由以下两个因素导致的：</p>
<p>1、data本质导致。Data本身，我们认为是一个图像是一个高维空间中的一个点，<span class="math inline">\(P_G\)</span>和<span class="math inline">\(P_{data}\)</span>在这个高维空间中都是属于low-dim manifold。所以基本是不存在重叠的部分的。</p>
<p>2、实际操作的时候，我们是先进行的采样，然后再用discriminator去量他们之间的divergence，而且采样的样例也不会太多。所以离散的采样也会导致，两堆采样的点基本上是不可能重叠的，应该是没有交集的。</p>
<p>​ 那么，为什么当<span class="math inline">\(P_G\)</span>和<span class="math inline">\(P_{data}\)</span>没有重合部分的时候，用JS divergence衡量会出问题呢？精确一点来说，应该是会对你训练时候，造成比较大的障碍。首先我们你需要知道一个事实，有关JS divergence，就是如果两个分布没有任何重合，那么计算出来的JS divergence值就是log2，无论其相距多远。如下图所示，更直观一些：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104959639.png" /></p>
<p>​ 对于这三组数据而言，因为算出来的目标函数都是一样的，所以判别器会认为G0跟G1是一样差的，也就没法把G0更新到G1.这样子的话就会给判别器的训练带来一定的困难</p>
<p>​ 这个问题，其实以一个直观的方法来讲就如下图所示：假设蓝点是generated data，绿色是real data，两组点没有交集，我们现在训练一个判别器（其实就是一个binary classifier）,如果这个判别器训练的太好的话呢，就会导致如下图所示的红色实线的情况，蓝色点上方的区域导数都是零，这就会使的Generator在梯度下降的时候，没法继续下去，generated data也就没法像real data靠近。如果这个判别器训练的不行的话，又很容易导致分辨不出两组data。所以，在原始GAN早期，训练是比较不容易的一件事情。因为你没有办法去分辨，到底什么是训练的太好，什么是训练的不行。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105009691.png" /></p>
<h3 id="二least-square-gan-lsgan">二、Least Square GAN ( LSGAN )</h3>
<p>​ 承接上一段落的内容，为了使得上述描述的梯度为0，最后导致蓝色点不会像绿色点靠近情况出现，所以就出现了LSGAN。它就是将之前Discriminator所作的classifier分类问题，改变成了regression回归问题。从操作上来讲，就是将输出的sigmoid激活函数，变成了linear函数。这样的话，就会使得我们的判别器的得分曲线如下图所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105022595.png" /></p>
<h3 id="三wasserstein-gan-wgan">三、Wasserstein GAN ( WGAN )</h3>
<p>​ 相比于原始的GAN，就是把原来用来衡量的JS divergence换成了Earth Mover’s Distance.</p>
<p>​ 首先我们来介绍一下Earth Mover’s Distance是什么东西，如下所示：下图是一个最简单的示例，我们有两个概率分布P和Q，然后你要想办法把概率分布P移到Q的地方去。如果P和Q如下图所示，都是一个一维空间上的分布，那非常简单，Earth Mover’s Distance就是它们之间的距离d。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105043352.png" /></p>
<p>​ 然后让我们再来考虑一个更为复杂的情形：现在P和Q的概率分布如图所示，那么你会发现我要想把P变成Q，我会有许多种挪动的方式。左边是一种挪动方式，右边是一种挪动方式，每一种挪动方式都会算出一个不同的距离，每一种挪动方式，我们就称之为一种“moving plan”。那么，我们会使用所有的moving plan中计算出最小的那个距离，来将其定义为Earth Mover’s Distance。（通俗一点就是，所有方案算一遍，要最小的那个方案）</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105052710.png" /></p>
<p>​ 比如说，同一堆东西我们用同样的颜色来表示，下图所示的P到Q的挪动方式，就是该示例最佳的一个moving plan。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105101411.png" /></p>
<p>​ 下面这张图是一个更为官方与公式化的定义，我们可以把一个moving plan表示成像左图一样的一个矩阵，对应的交叉的区域就表示该块挪动的量。那么你会发现，左边矩阵中每一行值相加应该都会等于其对应的P的那一条区域的值，每一列值相加起来都会等于其对应的Q的那一条区域的值。所以，给定这样一个moving plan，应该怎么去计算这个方案对应的移动距离呢？就用到下图中B(γ)的公式，遍历所有的xp和xq，其实就是遍历矩阵的每一个值，然后做乘积、求和。</p>
<p>​ 最终，如果我们想要求解P和Q的Earth Mover’s Distance，那么就需要穷举所有的moving plan，然后找到所有B(γ)最小的那一个值,就是P和Q的Earth Mover’s Distance.所以你会发现求解这样一个距离还需要解一个最优化问题.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105115495.png" /></p>
<p>​ OK,到此为止,我们已经知道了Earth Mover’s Distance是怎么样一个东西,那么就先来讨论一下,为什么我们用这个距离来进行衡量?有什么样的好处? 如下图所示,原来的JS Divergence计算出来的,对于G0, G50来说,两个差距一致,生成器就没法从G0 Update到G50,但是如果我们使用W Distance,d50是会比d0好的,那也就是说对于Generator来说,会慢慢的从G0 Update 到G50,也就是之前导致训练不起来的因素会被规避掉.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105125389.png" /></p>
<p>​ 那么在训练过程中,WGAN到底应该如何使用Earth Mover’s Distance呢? 我们将原来的V(G,D)函数替换成如下公式即可:</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105135075.png" /></p>
<p>​ 这个公式其实比较好理解,推导过程非常复杂我们再次略过.就是我们需要让从real data中产生出来的值越大越好,generate 中产生的值越小越好.同时,会有一个额外的constrain,就是D这个函数必须是一个 1-Lipschitz函数,我们暂时先不管这个 名词是什么,只需要先知道,这个代表的就是说D需要是一个非常平滑的函数.从直观的角度,为什么需要增加这个限制呢?如下图所示,如果不增加这个限制,就会像下图所示的红色曲线一样,Discriminator永远都不会收敛,两边分化会越来越大.generated data部分会变成无穷小,real data 就会变得无穷大.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105142990.png" /></p>
<p>​ 那么实际上1-Lipschitz函数是什么呢?定义如下: 右边蓝色函数就不是,而绿色函数就是.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105150180.png" /></p>
<p>​ 那我们应该怎么去进行训练呢? 原始的方法就是使用Weight Clipping 方法,但是其实这个方法并没有完全能够限制住D是一个1-Lipschitz函数,但可以相对来说让你的D变得平滑一些.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105158514.png" /></p>
<p>​ 所以后面就出现了 Improved WGAN ( WGAN-GP )来解决这个问题: 它的作者提出了这样一个观察: 就是说D是一个1-Lipschitz函数是和右侧的不等式等价的，也就是说要对于所有网络可能的输入x来说，如果将它给判别起求它的梯度的Norm范式是小于等于1的话，就可以。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105204998.png" /></p>
<p>​ 那么我们现在的方法，就是在整个的式子后面近似补上一项，</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105211649.png" /></p>
<p>​ 在我们的目标函数中加上这样一项以后，在网络训练的过程中，网络就会希望尽可能的满足下面这个式子，但是问题又来了，我们没办法对所有的x做积分，但是如果想要D是一个1-Lipschitz函数，是需要对所有的x来说的。所以我们又进行了部分的近似：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105219880.png" /></p>
<p>​ 如下所示，将最后补充的那一项变更成如下的一项，意思是我们只从一个叫做<span class="math inline">\(P_{penalty}\)</span>的分布中进行取样，我们只保证从这个分布中取出来的x能够满足，其他的就先不保证了。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105229709.png" /></p>
<p>​ 那么<span class="math inline">\(P_{penalty}\)</span>长什么样子呢？如下图所示，我们从<span class="math inline">\(P_{data}\)</span>中取一个点，从<span class="math inline">\(P_{G}\)</span>中随机取一个点，然后在两点的直线上随机采样一个点。这样子采出得到的点所形成的一个分布我们就认为是<span class="math inline">\(P_{penalty}\)</span>。如图所示，其实就是大概是图中蓝色部分区域。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105307969.png" /></p>
<p>​ 那为什么我们不用考虑整个Image Space空间中的x，只需要考虑<span class="math inline">\(P_{penalty}\)</span>分布上的x了呢？原因其实只是通过实验做出来，这样子效果是好的。其实从直观的角度上也可以有理解，因为我们之前说过其实训练过程就是<span class="math inline">\(P_{G}\)</span>向<span class="math inline">\(P_{data}\)</span>慢慢靠近的这么一个过程，那么也就是说其实只有<span class="math inline">\(P_{penalty}\)</span>这块部分区域的点的梯度才会影响。</p>
<p>​ 同时，我们还要再做一个改进。我们刚才目标函数补充的那一项，将其进一步修改。我们希望训练过程中，梯度越接近1越好。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105332353.png" /></p>
<p>​ 其实在刚才的WGAN-GP的过程中，做了很多的近似，很多结果也是由实验得出的，那么Spectrum Norm提出了一种方法，真的可以限制判别器学习完以后，在每一个位置的Gradient Norm都是可以小于1的。</p>
<p>​ 在最后，我们回到WGAN的算法，看看在实际的过程中，怎么从GAN变成WGAN：整体而言就是修改下图中黑框框的四个部分，1个是在LearningD的过程中，将目标函数部分的sigmoid输出移除掉，同时要记得使用weight cliipping 获得gradient penalty来进行限制。再就是在Learning G的过程中，将目标函数进行部分的修改，就可以了。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105348387.png" /></p>
<h3 id="四energy-based-gan-ebgan">四、Energy-Based GAN ( EBGAN )</h3>
<p>​ 其与原始GAN唯一的不同在于其修改了DIscriminator的架构，Discriminator原本来说是一个Binary Classifier，但是在EBGAN中，将其变成了一个Auto Encoder自动编码器，Generator是跟原来一样的。架构如下图所示：</p>
<p>​ 整一个Discriminator仍旧会输出一个Scalar分数，但现在这个分数是从何而来的呢？他是基于Auto Encoder的重建误差得到的。之所以这样做，EBGAN是基于以下一个假设的：越真实的Image它经过AutoEncoder以后，重建误差越小，越虚假的Image重建误差越大。那么EBGAN的好处在于这个Discriminator是可以预训练的，因为AutoEncoder的训练并不需要用到negative的样例。只需要给positive的样例，然后最小化重建误差就可以了。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105401559.png" /></p>
<p>​ 相比于原来的GAN来说，因为Origin GAN它不能预训练Discriminator，所以一开始DIscriminator就会很弱，训练起来就会很慢。而EBGAN的Discriminator一开始就相对比较强。</p>
<p>​ 在训练EBGAN的时候，需要注意以下一点：以下图为示例，我们会想要让real data最后得到的值比较大，generate data得到的值比较小，但是我们不能让generate data得到的值无限小，不加限制。因为这样的话就会让Discriminator对于generate data的输入而言，其全部输出noise，这样的话既可以把输出分数压的很低。所以我们需要一个margin，让genrate data对应的分数，只要小于margin以下的值就可以，不需要再变得更小。而至于margin的值就是一个我们需要调整的超参数。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115105412942.png" /></p>
]]></content>
      <categories>
        <category>GAN系列笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>LSGAN</tag>
        <tag>WGAN</tag>
        <tag>EBGAN</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN系列笔记3——GAN的数学理论支撑</title>
    <url>/2021/12/02/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B03%E2%80%94GAN%E7%9A%84%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA%E6%94%AF%E6%92%91/</url>
    <content><![CDATA[<h3 id="前言">前言：</h3>
<p>​ 本篇笔记为观看李宏毅老师的GAN相关课程后所记录，文中所有图片及内容均来源于李宏毅老师的课程，此处只是搬运+以自己的理解进行记录。全文讲述了GAN的理论数学部分的支撑，将直观的GAN训练过程背后的原理，为什么能这么做，训练的时候又需要注意的点是为什么这些内容进行了讲解。</p>
<h3 id="一我们通常说的生成问题从本质上来看是什么呢">一、我们通常说的生成问题，从本质上来看是什么呢？</h3>
<p>​ 从生成一张图片x来说，图片x其实就是在图片高维空间中的一个点。比如说64 * 64的图像，其实质就是64*64维空间中的某一个点。</p>
<h3 id="二gan训练的过程到底是在干什么呢">二、GAN训练的过程到底是在干什么呢？</h3>
<p>​ 在1中，我们发现其实在图像的高维空间中，存在意义的图像的点其实是很稀疏的，那么训练的时候，其实是提供了一堆有意义的图像的高维点，然后想要通过GAN来找到这些点的分布情况，也就是说，找到在图像高维空间中，有意义的图像的点所遵循的分布是怎么样的。这种分布情况，我们人是无法直接找到的。</p>
<p>​ 找到这种分布情况以后，其实生成的过程就是在概率分布中较高概率的位置进行取样，得到的可能就是能够令我们满意的图像。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104004253.png" /></p>
<h3 id="三在gan之前我们的训练所依据的理论是什么">三、在GAN之前，我们的训练所依据的理论是什么？</h3>
<p>​ 使用Maximum Likelihood Estimation 最大似然估计。整体的思想与过程如下图所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104025964.png" /></p>
<p>​ 首先我们有一个数据集分布，也就是我们通常使用的训练集<span class="math inline">\(P_{data}\)</span>。</p>
<p>​ 我们还有一个分布<span class="math inline">\(P_G\)</span>，其是θ的函数，就是我们要训练的用于生成的概率分布。我们的目标其实就是说想要找到一个θ值让<span class="math inline">\(P_G\)</span>去接近<span class="math inline">\(P_{data}\)</span>就可以了。但是怎样量化计算这个概率分布的接近呢？</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104038477.png" /></p>
<p>​ 我们可以先从<span class="math inline">\(P_{data}\)</span>中采集m个样本，然后带入到<span class="math inline">\(P_{G}\)</span>这个概率分布里面，希望得到的对应的概率越大越好。然后我们可以将所有的几率相乘，就会得到L。最终目标就是要找最好的θ使得能够让这个L最大。</p>
<p>​ 其实所谓的Maximum Likelihood 等同于机器学习中的 Minimize KL Divergence.下图的话是数学公式相关的推导过程，</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104119273.png" /></p>
<p>​ 那么现在问题来了，我们怎么去定义一个general 的<span class="math inline">\(P_G\)</span>。如果这个<span class="math inline">\(P_G\)</span>是符合高斯分布或者其他简单分布的话，是没有问题的。但是如果<span class="math inline">\(P_G\)</span>要符合这些分布然后再去拟合<span class="math inline">\(P_{data}\)</span>的话，显然就会受到很多的限制。所以我们需要一个更general的分布来去拟合<span class="math inline">\(P_{data}\)</span>。但是如果<span class="math inline">\(P_{G}\)</span>不受限制，是一个很复杂的很general的分布的话，上述计算式子中<span class="math inline">\(P_{G}(X;\theta)\)</span>这一项就很难计算出来，所以我们才有了下述的想法。</p>
<h3 id="四在gan中我们所依据的理论是什么">四、在GAN中，我们所依据的理论是什么？</h3>
<p>​ GAN中的Generator是一个网络，我们假设输入的是一个符合正态分布的各种采样点，这些点通过G以后，会产生一个新的分布，这个新的分布其实就是<span class="math inline">\(P_G(x)\)</span>。随后我们的目标也是让这个<span class="math inline">\(P_G(x)\)</span>和<span class="math inline">\(P_{data}(x)\)</span>这两个分布越接近越好。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104254669.png" /></p>
<p>​ 从公式来看，就是最小化<span class="math inline">\(P_G\)</span>和<span class="math inline">\(P_{data}\)</span>之间的某一种差异。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104316362.png" /></p>
<p>​ 看到这里，如果我们知道<span class="math inline">\(P_G\)</span>和<span class="math inline">\(P_{data}\)</span>的公式，那么我们是能够很方便的代入Divergence的公式中，然后用类似于梯度下降的方法，去最小化它。但是问题是，我们现在并不知道<span class="math inline">\(P_G\)</span>和<span class="math inline">\(P_{data}\)</span>的公式，也就是根本无法计算这个Div。这就是GAN神奇之处：</p>
<p>虽然我们没法知道<span class="math inline">\(P_G\)</span>和<span class="math inline">\(P_{data}\)</span>的公式，但是我们是可以从中进行采样的。从<span class="math inline">\(P_{data}\)</span>中进行采样无非就是从我们的训练集中取数据出来，从<span class="math inline">\(P_G\)</span>中进行采样，无非就是从正态分布中采样然后丢到Generator中，得到的数据。如下所示</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104340643.png" /></p>
<p>​ 那么其实GAN训练Discriminator的目的，就是为了能够通过Discriminator来计算出<span class="math inline">\(P_G\)</span>和<span class="math inline">\(P_{data}\)</span>的Divergence。从宏观上来看，是怎么完成这样一件事情的呢？</p>
<p>​ 我们前面说过训练的Discriminator在训练过程中，就是要学会给从<span class="math inline">\(P_{data}\)</span>的中采样出的数据高分，给从<span class="math inline">\(P_{G}\)</span>中采样出的数据低分。所以我们当时在训练Discriminator的时候，就写了一个目标函数，V(G,D)来完成上述的目标,然后要训练D的参数，找到一组D的参数使得V(D,G)最大。神奇的事情就是，这个V其实就是J-S Divergence有非常密切的关系。</p>
<p>​ 从直观上讲，如果<span class="math inline">\(P_{data}\)</span>和<span class="math inline">\(P_G\)</span>采样出来的数据靠的很近，那Discriminator自然无法区分，也就是没有办法找到一个D使得V变得很大，这个时候其实意味着就是<span class="math inline">\(P_{data}\)</span>和<span class="math inline">\(P_G\)</span>很接近，也就是说这两个概率分布数据的Divergence其实是很小的。所以如何达到一个最大的V值，会跟这两堆数据的Divergence是肯定存在关系的。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104420420.png" /></p>
<p>以下是详细的推导，证明为什么V会跟Divergence有关系？</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104429437.png" /></p>
<p>​ 我们把积分里的每一项都分开来算，为每一项都找一个最好的D(x)。</p>
<p>【因为D(X)假设可以是任何函数，也就是D(x)的输出可以是任意值】</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104440904.png" /></p>
<p>​ 将这个公式回代到V中，将原来V中的D(x) 用我们得到的结果进行替代：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104448302.png" /></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104454398.png" /></p>
<p>​ 所以推导到此，我们就得出了一个结论，当我们train一个Discriminator的时候，我们让目标函数最大化，让目标函数最大化其实就是让<span class="math inline">\(P_{data}\)</span>和<span class="math inline">\(P_G\)</span>这两个概率分布sample出来的数据之间的J-S Divergence最小。</p>
<p>​ 所以整个问题的逻辑变成了现在这样子：我们本来是要使得<span class="math inline">\(P_G\)</span>和<span class="math inline">\(P_{data}\)</span>的Div最小，现在的话呢，由于我们max V(D,G)和J-S Divergence相关，所以可以用max V(D,G)这一项把原来的Div 这一项替换掉。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104505340.png" /></p>
<p>​ 于是乎就变成了我们要解的问题就是如下图所示的一个式子：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104513342.png" /></p>
<p>​ 从直观上理解，我们可以从下面这个示例上来理解：我们假设现在只有G1，G2，G3这三个生成器可以选，D的横坐标变化我们假设就是选择了不同的Discriminator, V(G1,D)的函数如下所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104521610.png" /></p>
<p>​ 我们先来以一个实例，来理解一下我们要实现的目标。首先就是当G固定的时候，要找到一个D，使得V(G,D)最大，所以按照我们的示例，可以在选择不同的G的时候，找到三个对应的点，这三个对应的点所对应的横坐标D，就是能够使得，在当前的G下，让V（G,D）最大的那个D。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104529086.png" /></p>
<p>​ 然后我们要找一个G，最小化各种不同的G的最大V(G,D)，按照上图的示例，那么我们就应该选择G3，因为在三个V(G,D)的红点中，V(G3,D)对应的值最低，所以G3才是能够使得各种不同的G的最大V(G,D)最小。</p>
<p>​ 从意义的角度来看，其实如下图所示，V(G,D)中的这个最大值的函数值，其实就是用这个<span class="math inline">\(P_G\)</span>和<span class="math inline">\(P_{data}\)</span>之间的Divergence。这也就很好理解，在这么多的G中，我们要选择那个最小的，来最小化Divergence.</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104536101.png" /></p>
<p>​ 现在的话，就是要找到一个解决方案，怎么样去数学上或者是程序上，求解这个目标。其实这就是我们最开始讲过的迭代的训练GAN的这样一个过程。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104548084.png" /></p>
<p>​ 接下去我们进一步的解释，为什么这样一个迭代训练的过程，就是在解上面提到的一个min max的问题。</p>
<p>​ 为了简化min,max的求解，使得更好理解，我们先不想max V(G,D)里面的事情，我们直接设maxV(G,D) 为 L(G),因为外面那层min是跟D没关系的。</p>
<p>​ 那问题就变成了如下图所示，那其实就很简单，就是用梯度下降去进行优化。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104602866.png" /></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104605809.png" /></p>
<p>​ 所以整体的求解最优化问题的算法应该是像下图一样：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104613127.png" /></p>
<p>​ 其实在这个迭代的过程中，并不是非常准确的在减少JS divergence,因为在求解的过程中，其实G是在一代一代变化的，也就是我们看到的从G0到G1到G2这样子，那么每次G变化以后呢，其实V(G,D)应该都会发生变化。我们说其完成了Decrease js divergence这件事呢，是基于一个默认近似的，就是我们默认D0* 和 D1* 近似相等，也就是说G0和G1之间不会有太大的变化，也就是说G的每一代的更新不会太大。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104624268.png" /></p>
<p>​ <strong>也就是因为有这样一个假设，所以你每一轮迭代训练生成器的时候，不能够update太多次，而在train判别器的时候，理论上应该训练到底，使得判别器达到最优。</strong></p>
<p>​ <strong>然后我们在实际做的时候，如下图所示，我们的目标函数其实和理论的V还有一定的区别，理论的V是期望值，但是实际上不太可能做的到这一点，所以我们其实是用离散的采样来进行代替的。</strong></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104649400.png" /></p>
<p>​ 所以最后回顾一下训练GAN的算法，对于有一些步骤其实可以理解的更为深入。比较重要的其实就是之前提过的，在每一轮迭代中，你其实可以使用梯度下降去更新很多遍的判别器的参数，但是在更新生成器参数的时候，只能更新一次，如果更新的太大了，G0 不约等于G1了，那么你在做的事情也就是不会在minimize js divergence了，就和理论不符合了。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104659145.png" /></p>
<p>​ 最后还有一个需要提及的内容，在原论文中，其实训练Generator的参数的目标函数并不是我们上面所写的，而是会把某一项进行部分的改变如下所示。但其实两种参数都可以进行训练。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115104707603.png" /></p>
]]></content>
      <categories>
        <category>GAN系列笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>Theoretical Basis</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN系列笔记2——常见GAN变种（ConditionalGAN，PatchGAN，CycleGAN等）</title>
    <url>/2021/12/01/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B02%E2%80%94%E5%B8%B8%E8%A7%81GAN%E5%8F%98%E7%A7%8D%EF%BC%88%E5%90%ABConditionalGAN%EF%BC%8CPatchGAN%EF%BC%8CCycleGAN%E7%AD%89%EF%BC%89/</url>
    <content><![CDATA[<h3 id="前言">前言：</h3>
<p>​ 本篇笔记为观看李宏毅老师的GAN相关课程后所记录，文中所有图片及内容均来源于李宏毅老师的课程，此处只是搬运+以自己的理解进行记录。全文讲述了一些常见的GAN的变种，以及它们的一些核心的思想，包括Conditional GAN用于Text-To-Image和Image-To-Image的任务，期间有提及一些改进诸如Patch GAN。然后简单讲述了StackGAN的思想以及应用场景。之后，是对非监督学习条件生成领域的一些探讨，这一部分最常见的就是风格迁移的应用。讲述了两大类实现非监督学习条件生成的方法思想，第一类是直接生成，其中介绍了CycleGAN和变体StarGAN。第二类是投射至相同公共空间的方法，其中包含了解决此方法中一个关键问题的各种思想方案。看完本篇笔记，你应当会对Conditinal GAN和Unsupervised Conditional Generation领域及该领域下的CycleGAN思想有一个大概的了解，随后你可以选择感兴趣的GAN网络，再去详细的阅读原论文，研究细节部分。</p>
<h3 id="一conditional-gan">一、Conditional GAN</h3>
<p>​ 条件GAN所要完成的目标就是你可以通过一些限制条件，来让GAN网络生成的图像符合你的要求。比较经典的就是Text-To-Image和Image-To-Image两种任务。</p>
<h4 id="text-to-image任务">1、Text-To-Image任务</h4>
<p>​ 该任务目标是，你输入一段话，然后根据输入的语义来生成一张图片。这样一个任务其实我们可以用传统的监督学习方法来完成它，如下所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114223140981.png" /></p>
<p>​ 但是使用这种传统的方法，会出现一些问题，比如说，训练网络完成以后你输入文字"train"，而在我们的训练集里面，关于"train"的图片有很多种，比如说一些正面的火车照片，或者是一些侧面的火车照片。最后会导致的结果就是，这个监督学习生成的结果又要像正面的火车，又要像侧面的火车，最后就会变成一个很多图片的平均，也就是会生成一个非常模糊的图像。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114223153376.png" /></p>
<p>​ 所以我们需要用到Conditional GAN的技术。在前一篇文章中讲过，GAN本身最后实现的一个效果是，输入一个随机的分布向量，或者说Noise，然后其会生成一个对象（图片或其他）。那么在Conditional GAN中呢，从最终实现的效果而言，其实就是多吃了一个内容c，这个内容c也就是我们所指定的条件。在Text-To-Image的任务中，内容c也就是我们所指定的文本信息。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114223205025.png" /></p>
<p>​ 那么Discriminator方面，跟普通的GAN又需要有啥区别呢？如下图所示，对于判别器而言，其输入也需要新增一个内容，也就是我们指定的条件c。</p>
<p>​ 我们为什么需要这么干呢？如果判别器仍然和原来一样，那么其实你会发现，因为判别器只会看生成的图片x是否够真实，而不会看x和内容c匹不匹配。也就是说，如果你输入”cat“作为内容c，但是生成器画了一个很好很真实的火车图片，判别器仍然会给高分。这样到最后的结果其实就是，你会发现你指定的条件c和生成出来的图片完全不匹配，虽然这些生成的图片都很真实。</p>
<p>​ Ok，所以我们回到Conditional GAN判别器的架构，最后输出的值里面，其实含有两部分信息的含义。一个是图像x是不是够真实，一个是内容条件c和图像x是否匹配。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114223223659.png" /></p>
<p>​ 所以在这种判定条件下，什么样的情况要给高分呢？什么样的情况要给低分呢？</p>
<p>​ Case1 : x真实 + c 和 x 匹配 【高分】</p>
<p>​ Case2 : x虚假 【低分】</p>
<p>​ Case3 : x真实 + c 和 x 不匹配 【低分】</p>
<p>​ 所以你会发现，相比普通GAN中的判别器，它多了一种要给低分的情况，这个在之后整个的伪代码中会有所体会。伪代码如下所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114223235243.png" /></p>
<p>​ 整体而言整个过程还是比较好理解的，首先也是从数据库里拿m个正面的样例，然后采样m个noise信号，将这m个信号和生成器生成的数据组合成一组数据。然后再随机的从数据库中取m个图片。注意，这一步目的是取一组真实的图片，但是给这些图片贴上不正确的标签c，后面在目标函数中会有所体现。</p>
<p>​ 整个目标函数有3项，第一项和第二项与GAN类似，也就是刚才上面提到的Case1和Case2。第三项其实就是对应上面提到的Case3，如果向Discriminator中投入ci和随机取的与ci并不匹配的图像，那么判别器也应当给其低分。然后用梯度上升，最大化目标函数即可，这就完成了训练判别器的部分。</p>
<p>​ 在训练生成器的部分，就和GAN几乎一样了，我们就此省略不再细谈。</p>
<p>​ 说完Conditional GAN的整体的训练算法伪代码，再来讨论一下Conditinal GAN 的判别器部分的网络架构应该如何搭建，Conditinal GAN的判别器部分有以下两种常见的网络架构：</p>
<h5 id="架构1最为常见">1、架构1：最为常见</h5>
<p>​ 如下图所示的架构是最为常见的一种架构，先有两个网络，分别将x和c做embedding编码，然后将两个编码输入判别器网络中，其会输出一个分数，就代表我们先前说的x是否真实，或者c和x是否匹配。但是这种架构其实并不能分清楚，到底是因为哪种问题，所以才导致了低分。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115002938439.png" /></p>
<h5 id="架构2有部分论文采用但是也是蛮有道理的一个架构">2、架构2：有部分论文采用，但是也是蛮有道理的一个架构</h5>
<p>​ 此种架构，先把对象x输入一个判别器网络中，然后直接输出一个分数，这个分数代表x是否真实。然后再将条件c和对象x输入另一个判别器网络中，输出一个分数，代表c和x是否匹配。最后再将两个分数综合起来，得到最终判别器的分数。这样子的话，我们就可以知道，到底导致低分是什么原因。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115002950662.png" /></p>
<h4 id="image-to-image-任务">2、Image-To-Image 任务</h4>
<p>​ 和这个任务相关的一篇论文其实是非常经典的一篇论文，CVPR 2017的《Image-to-Image Translation with Conditional Adversarial NetWorks》，有兴趣也可以去阅读原论文。下图是原论文中的一些应用的截图：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003023803.png" /></p>
<p>​ 和先前的Text-To-Image的任务一样，如果采用传统的监督学习的方法，很有可能产生的图像会非常模糊，所以就需要Conditional GAN来完成。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003031956.png" /></p>
<p>​ 如下所示就是使用Conditional GAN的生成器和判别器的一个示意结果图，其实跟Text-To-Image是一样的，此处就不多赘述了。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003040918.png" /></p>
<p>​ 但是其实以这种方法生成出来的图片，也会存在一些小的问题，参见之后的第二幅图。就是比如说它可能会在图像的某些地方产生奇奇怪怪的内容。你可以仔细观察GAN生成图像的左上角，出现了奇怪的内容。这是由于什么产生的呢？就是Generator其实只关心一个是我产生出来的图像好不好，另一个是和输入图像是不是匹配，但不太会关心一个点：就是说和产生的图片和原来的有监督学习中的目标图像是不是相似。</p>
<p>​ 所以，我们往往会加上一个额外的限制，就是我们希望Generator生成的图片和原来的图像越接近越好，也就是说既考虑了原来监督学习中的as close as possible，也考虑了GAN的评分机制。如下所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003054587.png" /></p>
<p>​ 下面这个就是上面所讲的三种方法不同的生成效果示意图</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003103515.png" /></p>
<p>​ 同时，在《Image-to-Image Translation with Conditional Adversarial NetWorks》这篇论文中，它的Discriminator经过了一些额外的设计。这就是其引入的Patch GAN。Patch GAN解决了一件什么事情呢？</p>
<p>​ 如果你要用Conditional GAN产生的图片非常的大，很有可能产生的结果会很差。原因是什么呢？如果图片很大，代表其展开后维度非常的高，那就需要有非常多的参数去训练它，参数一旦非常多，训练以后很容易导致过拟合的情况发生。所以，作者提出了PatchGAN的想法，如下图所示，左边是传统的Discriminator，右边是PatchGAN的。</p>
<p>​ PatchGAN的Discriminator并不是直接检查整张图像，而是只检查一个小窗口内的图像。得出一个分数。然后再综合所有窗口得出的分数。至于检查的小窗口的尺寸为多大，是一个需要调整的超参数，原论文中也对其有一定的分析，此处不多赘述。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003123825.png" /></p>
<h3 id="二stack-gan">二、Stack GAN</h3>
<p>​ StackGAN来源于这篇论文：ICCV2017《StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks》。简单而言，其只是将Conditional GAN的生成分成了好几个阶段，用于能够生成分辨率更高的图片。这是原论文中所显示的流程图</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003145750.png" /></p>
<p>​ 简单而言，步骤如下，首先输入一段文本描述，通过Embedding嵌入层后，经过一系列的Conditioning Augmentation的处理，将各种信息连接在一起，然后在生成器部分，首先生成第一阶段的结果图片。然后在输入到第一阶段的判别器中。当第一阶段的生成器和判别器完成训练以后，我们再将第一阶段的生成结果，与描述文本作为输入，开始训练第二阶段的GAN网络。步骤与第一阶段的GAN网络类似，最终可以得到更高分辨率的第二阶段的结果图像。具体细节部分，我暂时也没有完整的阅读过原论文，暂且只是领会一下这种方法的大致思路。</p>
<h3 id="三unsupervised-conditional-generation">三、Unsupervised Conditional Generation</h3>
<p>​ 非监督的条件生成，通常用于风格迁移。在风格迁移的应用场景中，我们往往能够收集到许多Domain X的信息，也能收集到许多Domain Y的信息，但是不一定能够拥有Domian X和Domain Y的一一对应的pair 数据。像类似这样子的场景中，非监督的条件生成就是一个非常好的解决方案。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003209328.png" /></p>
<p>​ 非监督的条件生成，我们往往会有两种方法思路去完成，如下所示：</p>
<p>​ 方法1是Direct Transformation 直接转换，此种直接通过一个生成器进行转换的方法，往往两个域内的图像不能够有太过巨大的差距，通常用于做纹理或者颜色上的一些改变，以达到风格的变化。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003225024.png" /></p>
<p>​ 方法2是Projection to Common Space 投射到公共空间，此种方法通过将两个Domain投射到一个公共空间中，完成转换。具体而言，对于下述的任务来说，先将DomainX真实人脸的信息通过一个Domain X的编码器转换到公共空间，也就是我们的脸部属性空间，然后再将脸部属性空间，通过一个Domain Y的解码器，解码出我们的DomainY动漫人脸的信息。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003237748.png" /></p>
<p>接下去我们会对两种方法进行详细的讨论：</p>
<h4 id="direct-transformation内含cyclegan网络讲解"><strong>1、Direct Transformation（内含CycleGAN网络讲解）</strong></h4>
<p>​ 我们要训练一个生成器，能够完成将图片从DomainX直接转换至DomainY的任务，但是该生成器产生的图片我们怎么去评价呢？在有监督学习中，我们有一对一的数据，但是在无监督学习中，对于这个输入而言，我们并没有pair可以提供给它。所以此时，我们需要一个Discriminator，这个判别器用于判断一个图片，是否属于DomainY，然后输出一个分数。</p>
<p>​ 那么这样一来，Generator的目标就是如何将DomainX的图片转换成一个新的图片，能够骗过Discriminator，而Discriminator也要努力鉴别这个图像到底是不是属于DomainY。下图，就是整个过程的一个直观的示意图。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003304210.png" /></p>
<p>​ 在这样一个过程中，好像问题已经解决了，实则不然。你会发现，生成器只学会了怎么生成一个DomainY的图片去骗过判别器，但是我们并没有给它限制，说要让这个生成器生成的DomainY的图片和原来的DomainX的图片要相似啊。那万一，输入是一张DomainX的风景图，生成器输出了一张DomainY的人像图，不照样能够获得很高的分数，这当然就不对啦。</p>
<p>​ 所以，我们就希望生成器生成的DomainY的输出，应该和DomainX的输入应当会存在一定的关系才对，而不是毫无关系可言。针对于这个问题而言，有许多解决方案，叙述如下，其中最著名的就是熟知的cycleGAN。</p>
<h5 id="解决方案1不管这个问题">1）解决方案1：不管这个问题</h5>
<p>​ 听上去这个解决方案非常的扯淡，但其实在某种条件下它确实work。具体而言是什么意思呢？当我们的网络层数不是很深的时候，其实网络本身也倾向于不对input做太大的改变，有人也做过实验，不去管这个问题，在某些情况下也不会得到太差的结果，得到的结果和input其实也类似。</p>
<p>​ 但是，如果我们的网络比较深的话，经过非常多的非线性变换与线性变换，其实就并不好说，最后输出的output，会和input相差不大了。这个时候的话，我们可能就需要其他的解决方案。</p>
<h5 id="解决方案2利用预训练的编码器网络如下图所示">2）解决方案2：利用预训练的编码器网络，如下图所示：</h5>
<p>​ 我们利用两个预训练好的DomainX和DomainY的编码器网络，在GAN训练的时候呢，生成器训练的目标任务又多了一项。之前生成器只需要生成图片，努力的去骗过Discriminator就可以，也就是说只要生成的图片越来越像DomainY的图片就可以，但是我现在希望生成器生成的图片，经过DomainY预训练的编码器网络后，生成的特征向量A，和原图经过DomainX预训练的编码器网络后生成的特征向量B，也尽可能地相似。这样一来，你会发现，如果两个编码后的内容越接近越好的话，DomainX在转换至DomainY的时候，产生的图片，应该是保留了原来DomainX图片的某些特征的。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003341802.png" /></p>
<h5 id="解决方案3cyclegan">3）解决方案3：CycleGAN</h5>
<p>​ 原论文题目为《Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks》，有兴趣的同学可以看原论文，其主要大致思想如下图所示：</p>
<p>​ 我们不仅训练一个DomainX到DomainY的生成器，同时也训练一个DomainY到DomainX的生成器，生成器X-&gt;Y在原有的需要骗过Dy的基础上，还需要使得原始输入图像和经过两个生成器后产生的图像越接近越好，这也就是我们常说的Cycle consistency，循环一致性。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003358128.png" /></p>
<p>​ 对于CycleGAN来说，我们可以训练双向的一个网络，两个一起去做训练。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003406257.png" /></p>
<p>​ 其实CycleGAN也存在一些问题，在论文《CycleGAN, a Master of Steganography》中，其作者提到，在DomianX的输入图像经过两个生成器的过程中，出现问题。具体而言，输入的图像的有些信息会在经过第一个生成器的时候被隐藏起来，此处的被隐藏起来也就是指人肉眼观察不到，而当其经过第二个生成器的时候，又会被还原出来。如下图红色方框中所示。这就意味着，生成器网络有隐藏信息的能力，我们如果只控制input和经过两个生成器的output的一致性，中间产生的我们真正要的图像还是有可能丢失原图的部分信息。这是一个尚待研究的问题。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003414078.png" /></p>
<p>​ 此处的话补充一下另外一个网络，是在CycleGAN的基础上进行的一个升级，叫做StarGAN。其应用领域在于，当你需要多个Domain的信息风格互相转换的时候，怎么进行网络的学习与处理。</p>
<p>​ 按照之前CycleGAN的思路，如果我们想要在4个Domain内做互相转换，需要在两两之间训练一组生成器。StarGAN就是可以之训练一个生成器，就完成在多个Domain之间的信息互相转换。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003422218.png" /></p>
<p>​ 原论文为《StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation》，其主要思想及训练过程如下：</p>
<ul>
<li><p>A)步骤，首先训练判别器，判别器会完成两件事，一是要判别这个图像他是真实的还是虚假的，二是要做一个分类的问题，它是属于哪一个Domain，给出两个结果。</p></li>
<li><p>B)C)步骤, 在训练生成器时，要完成这样一个任务。首先输入我们的输入图片，以及目标Domain，然后生成器会生成一个Fake Image，我们将这个Fake Image和原来的Domain作为输入，再次送入生成器中，会得到一张Reconstructed Image，我们希望这张重建的图像和输入的图像之间的差距越小越好，其实这就是Cycle Consistency类似的思想。</p></li>
</ul>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003432585.png" /></p>
<ul>
<li><ol start="4" type="A">
<li>步骤，同时在训练生成器的时候，还有一个目标，就是要让这个Fake Image骗过我们之前训练好的Discriminator，使得Discriminator的输出越大越好。</li>
</ol></li>
</ul>
<p>​ 下图是一个训练应用实例的示意图，在StarGAN的实际训练过程中，其实域可以是由一组编码决定的。就比如说00101代表一个域，10011也代表一个域。然后00101有一个参照表，也就是label，代表相对的含义。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003446510.png" /></p>
<h4 id="projection-to-common-space">2、Projection to Common Space</h4>
<p>​ 在此类方法，如果我们想要完成DomainX和DomainY两种风格的互相转换，我们其实需要的就是如下图所示的2个Encoder和2个Decoder。例如，如果要从X转换至Y，那么图像应当先通过EncoderX，得到一个Face Attribute，应该是一个Latent Vector，然后再经过DecoderY，最终得到DomainY的图像。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003534380.png" /></p>
<p>​ 这样来看，如果我们拥有一对一的pair数据，那么其实完成这个任务非常的容易，只需要正常的监督学习训练即可。但是我们现在是无监督场景下的训练，应当如何完成训练呢？</p>
<p>​ 我们可以像下图所示这样进行训练，其实就是训练了两个自动编码器，通过最小化重建误差来进行优化。然后在此基础上我们可以加两个判别器。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003544456.png" /></p>
<p>​ 如此而言，我们确实可以得到2个Encoder和2个Decoder。但是这两组AutoEncoder（VAEGAN）是完全没有关系的。这意味着什么呢？你看上图中，中间的这个Latent vector，两边共用一个，其实来说，两边训练出来以后，得到的中间的这个特征向量，每一个维度所代表的含义会是完全不同的。其实就变成了如下图所示的这样子一个效果。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003552083.png" /></p>
<p>​ 所以最终导致的结果就是：你将一个DomainX的图片丢入X编码器，得到一个向量，然后将向量丢入Y解码器，可能生成的是一个完全不相关的DomainY的图片。</p>
<p>​ 接下来我们就是要解决这个问题，也会有比较多的文献解决方案：</p>
<h5 id="解决方案1共用部分隐藏层参数">1）解决方案1：共用部分隐藏层参数</h5>
<p>​ 共用EncoderX和EncoderY的后面几层，同时也共用DecoderX和DecoderY的前面几层。这样子做其实就是希望，接近中间这个特征向量的层共用参数以后，两边的编码能够在中间的特征向量的维度上达成一致。</p>
<p>​ 这种解决方案比较极端的情况时，可以让EncoderX和EncoderY共用所有参数，DecoderX和DecoderY共用所有参数，只是在训练和使用的时候，我输入一个标签，代表这个是X Domain还是Y Domain。这个方法用于CoupleGAN和UNIT</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003616483.png" /></p>
<h5 id="解决方案2domain-discriminator">2）解决方案2：Domain Discriminator</h5>
<p>​ 我们在网络架构中，新增一个Domain Discriminator。在训练的过程中，EncoderX和EncoderY还有一个任务，就是说他们所生成的中间的特征向量，还要能骗过Domain Discriminator，让这个判别器无法鉴别，这个特征向量到底是从EncoderX来的，还是从EncoderY来的。如此一来，完成训练后，相当于说是两个编码器产生的中间的特征向量应该遵循着类似的分布，能够在中间的特征向量的维度表示上达成一致。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003651260.png" /></p>
<h5 id="解决方案3利用cycle-consistency的类似思想">3）解决方案3：利用Cycle Consistency的类似思想</h5>
<p>​ 如图红线路径所示，图像经过EncoderX，产生特征向量，经过DecoderY产生Image，再将这个Image作为输入进入EncoderY，产生特征向量，经过DecoderX产生output，然后最小化input和output的重建误差即可。该方法被用于ComboGAN。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003717264.png" /></p>
<h5 id="解决方案4利用semantic-consistency的类似思想">4）解决方案4：利用Semantic Consistency的类似思想</h5>
<p>​ 按照图中红线路径，input输入经过EncoderX，产生特征向量A，经过DecoderY产生Image，再将这个Image作为输入进入EncoderY，产生特征向量B。然后最小化A和B的差别即可。相比于CycleConsistency，此种思想直接在特征向量的空间中去考虑相似度，避免了比如说可能input和output表象上很像，但其实中间特征向量代表维度仍然不同这种问题。该种思想在DTN和XGAN网络中使用。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115003743216.png" /></p>
]]></content>
      <categories>
        <category>GAN系列笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>ConditionalGAN</tag>
        <tag>PatchGAN</tag>
        <tag>CycleGAN</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN系列笔记1——GAN的基本理念与思想</title>
    <url>/2021/11/30/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/GAN%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B01%E2%80%94%E5%8E%9F%E5%A7%8BGAN%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%90%86%E5%BF%B5%E4%B8%8E%E6%80%9D%E6%83%B3/</url>
    <content><![CDATA[<h3 id="前言">前言：</h3>
<p>​ 本篇笔记为观看李宏毅老师的GAN相关课程后所记录，文中所有图片及内容均来源于李宏毅老师的课程，此处只是搬运+以自己的理解进行记录。全文按照李宏毅老师讲课的逻辑进行记录，先从GAN的基本理论概念讲起，中间提及了Structure Learning的问题，然后回答了GAN中的两个经典的问题，然后对VAE和GAN的结果做了简单的比较，最后做了总结，看完以后应该会对GAN有一个完整的概念与理解。全文共4800字左右。</p>
<h3 id="一basic-idea-基本概念">一、Basic Idea 基本概念</h3>
<p>​ GAN是完成了一个Generation的任务，从目标上而言，其实就是我们要训练一个NN的generator，这个generator可以完成下述任务：你给机器一个随机的向量，机器就可以给你输出你想要的物体（可以是图像也可以是其他）。（当然，这种类型的generator看上去是没什么用的，因为你不知道随机向量到底会生成什么样子的东西，但通常向量的每个维度可能就是用于控制输出图像的一个特征部分，现在暂时我们只专注于完成这样一个随机的任务部分）。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114214941737.png" /></p>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114214945273.png" alt="image-20220114214945273" /><figcaption aria-hidden="true">image-20220114214945273</figcaption>
</figure>
<figure>
<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114214948402.png" alt="image-20220114214948402" /><figcaption aria-hidden="true">image-20220114214948402</figcaption>
</figure>
<p>​ GAN比较神奇的点在于其还会同时训练一个Discriminator，即判别器。从目标上而言，其要完成如下任务:输入一个图像，输出一个值，这个值越大，代表图像越真实。值越小，代表图像越虚假。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114214959500.png" /></p>
<p>​ 那么在GAN中，Generator和Discriminator的关系其实就是对抗竞争，或者说互相帮助互相进步的关系。如下图所示：第一代的生成器生成的东西，交给第一代判别器。第一代生成器在第一代判别器的反馈下，进化为第二代的生成器，然后判别器也随之进化为第二代，循环往复下去（有一种道高一尺，魔高一丈的感觉）</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215010569.png" /></p>
<p>​ 一个更好的比方，下图中，Generator和Discriminator就像是学生和老师在对话一样，在这个过程中，学生逐渐进步。老师的要求标准也越来越高。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215031820.png" /></p>
<p>​ 在这样一个过程中，留下两个疑问：Generator做生成为什么不自己学提高水平呢？Discriminator这么会批评，为什么不自己做生成呢？【解释与回答见第四章】</p>
<p>​ 现在我们先来讨论一下整个训练的算法伪代码过程，如下所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215049291.png" /></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215052840.png" /></p>
<p>​ 首先我们初始化生成器和判别器，然后在每一个训练迭代阶段，进行两件事：</p>
<p>​ 1、我们将Generator固定住，训练Discriminator的参数。具体而言，我们首先随机的输入一些向量，通过G生成一些随机的generated objects，然后再从真实数据库中取出一些objects，将这两部分图片作为D的输入，来训练D。目的是要让D看到generated objects 就给出尽可能接近0的分数，让D看到真实的objects就给出尽可能接近1的分数。(其实就是给两部分图片打个标，然后训练一个分类器，如上图所示)</p>
<p>​ 2、固定住Discriminator的参数，调整生成器的参数。具体而言就是：将一个随机的vector丢入生成器里，生成一张图片，将图片放入判别器中，得到一个分数，我们训练此步的目标是要使最后输出的那个分数越接近1越好。从代码上，这个要怎么进行操作呢？我们往往会将生成器的NN和判别器的NN合并成一个大的网络，锁住判别器部分层的参数，这相当于这个大的网络中间有一层很宽的隐藏层（就是输出图片那一层），然后我们要使得最后生成的结果越大越好，就是Gradient Ascent，其实就是在Objective Function上加一个负号，就可以完成这个任务。</p>
<p>​ 下述是更为正式的算法伪代码：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215114053.png" /></p>
<p>要注意，在这个算法中，有一些超参数是需要手工去调整以找到最佳效果的：</p>
<p>​ 1、输入的向量的维度，5维、10维、……</p>
<p>​ 2、在每一个迭代过程中：更新判别器的次数，可以是1~n次。</p>
<p><strong>算法中的一些解释：</strong></p>
<p>​ 1、更新判别器参数的时候的Objective Function：</p>
<p>​ 第一项：<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215147446.png" />代表真实数据集的图像进到判别器中的分数均值。</p>
<p>​ 第二项：<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215201515.png" />代表生成的图像进到判别器中的分数被1减去后的均值。</p>
<p>​ 我们要让这个目标函数最大，其实就是完成了最开始说的，真实数据集图像进到判别器中，分数越接近1越好，生成图像进入，分数越接近0越好。</p>
<p>​ 2、更新生成器参数的时候的Objective Function：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215238651.png" /></p>
<p>​ 其实就是随机向量通过生成器，再通过判别器后，得到的均值越大越好。然后用梯度上升方法，去最大化这个目标函数。</p>
<h3 id="二structure-learning">二、Structure Learning</h3>
<p>​ 首先，什么是Structure Learning？机器学习本身其实就是找到一个函数f，去完成X-&gt;Y这样一件事情。回归问题，其实就是输出一个分数。分类问题就是输出一个类别，结构化学习其实就是输出一个更复杂的比如说一个序列，一个矩阵，一个图，一个树等等这样的问题。往往这种复杂的结构是由很多个组件组成，并且这些组件之间是有关联关系的。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215309664.png" /></p>
<p>​ 为什么Structure Learning是很Challenging的呢？其实就是如下图所说的，一个的话是它可以被视为One-shot Learning或者是Zero-shot Learning。那什么是One-Shot Learning 或者说Zero-shot Learning呢？例如，在分类作业中，我们如果需要训练，每个类别都需要给出一系列的数据范例。而One-Shot Learning 或者Zero-shot Learning就是指，可能有些类别完全没有范例，或者说只有很少的范例。</p>
<p>​ Structure Learning里面，输出的是一个结构体，也就是说很有可能训练的句子里面是完全不会有重复的东西的。如果我们把所有的可能输出都视为一个类的话，输出的空间是很巨大的。并且大多数的类都是没有任何训练数据的。所以机器需要一定的创造性。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215320786.png" /></p>
<p>​ 第二点的话，在Structure Learning中，机器必须学会规划。机器可以一个个组件的去产生，但是它一定要学会组件和组件之间的关系与限制。需要全局的去进行考虑。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215331213.png" /></p>
<h3 id="三structure-learning的一些解决方案">三、Structure Learning的一些解决方案</h3>
<p>​ 在传统的方法分类中，其实有两类：一类是Bottom Up的方法，一类是Top Down的方法。自底而上的方法主要是学习怎么去生成Component，然后再进行合并，这个很像Generator做的事情，但是这样很容易失去大局观。自上而下的方法是学会评估一个物体，然后找到最好的那个，这个很像Discriminator做的事情，但是这样做的问题是很难做生成。【为什么这么说见第四章】其实GAN就是解决Structure Learning的一个解决方案，他就像是把两种方法做了结合。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215356504.png" /></p>
<h3 id="四为什么generator不能自己学为什么discriminator不能自己生成">四、为什么Generator不能自己学？为什么Discriminator不能自己生成？</h3>
<h4 id="generator为什么不能自己学">1、Generator为什么不能自己学？</h4>
<p>​ 在传统的有监督学习中，我们只要有大量的真实图片数据，以及他们对应的标签，我就可以完成生成图片这样一个任务。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215423815.png" /></p>
<p>​ 但是问题就来了，我们怎么给这些真实图片数据打标签呢？如果是随机赋给他们一些向量标签，那最后的训练是会很困难的。因为比如说我们看两个1的图片比较相似，当然希望向量中有一部分的标签内容是类似的，而不是说这两个1的图片对应的向量标签完全不一致。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215433669.png" /></p>
<p>​ 也就是说，我们总是希望这个input和output的内容是存在一些联系的，而不是毫无联系。说到这里，其实问题还是可以解决的，怎么去解决这个编码问题呢？</p>
<p>​ 正巧，我的上一篇文章中讲了各种自动编码器的相关知识，这个编码问题，我们可以训练一个NN编码器来进行解决。这个编码器完成一件什么事呢？给它一个图片，它输出一个向量，来表示这个图片的相关特征。具体而言，自动编码器怎么进行训练，详见自动编码器的文章，如下：这里的话也给一张李宏毅老师课程的截图示意：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215447872.png" /></p>
<p>​ 仔细想来，其实这个Decoder干的是不是就是Generator的事情？是的！因为它也是接收一个Vector向量，输出一张图片，如下图所示：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215456226.png" /></p>
<p>​ 这样看来，其实理论上来讲，只要我们训练好了一个自动编码器，把Decoder部分拿出来，就是一个生成器。然后我们随便给一个向量，他就会输出一个对应的图像。</p>
<p>​ 但是Auto-Encoder存在什么样的问题呢？如下图所示：比如说，我们已知a向量可以产生一个图片，b向量可以产生一个图片，那么0.5<em>a+0.5</em>b会产生什么呢？很有可能会产生Noise，因为这样的生成器本身是一个神经网络，不是线性的。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215508501.png" /></p>
<p>​ 说到这里，其实还没有正面回答Generator为什么不能自己学这个问题，但大家不要着急。我们继续说，这个问题其实我们可以用VAE来解决，也就是上篇文章末尾重点讲过的变分自动编码器。</p>
<p>​ 如下图所示：有了变分编码器，因为变分自动编码器是对一个概率分布进行了编码，而不是固定的。所以我们能够将这个decoder训练的更为稳定。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215517660.png" /></p>
<p>​ 那么，现在看来，好像Auto-Encoder就能解决Structure Learning这样一个任务了，那为什么我们还需要GAN这个技术呢？接下去要说的就是Auto-Encoder这套技术所缺少的内容。</p>
<p>​ 在Auto-Encoder的技术中，我们实现的目标是：让生成的图片和目标图像越像越好。那么问题来了，什么叫越像越好？正常来说，我们就是把这个图像展开成一个vector，然后逐像素的计算距离差。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215528542.png" /></p>
<p>​ 我们先前说过，自动编码器是不能让它完整的保留所有的信息的，不然就失去了编码的意义，机器只要学习复制就可以了。所以自动编码器的生成器部分，生成的目标总是和源图像会有一些差距，会有一些取舍。这个时候，取舍在哪里就很重要！如下图所示：右上角是目标图像，下面四个是生成的图像。生成图像第一排，只偏差了1个像素，第二排，偏差了6个像素。从计算相似度而言，第一排优于第二排，但是从人的感知而言，第二排优于第一排。所以我们如果只是单纯的计算距离差，让两者越像越好，就有可能会产生第一排的结果。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215538491.png" /></p>
<p>​ 在做Structure Learning的过程中，我们说过组件和组件之间的关系是很重要的，需要考虑全局观念。这个偏差出来的多的像素本身没有错误，有错的是它如果是有像素值的，那么它周边的组件，也应当是有填充的，而不是空白的。这个像素要和周围的像素一致，这个其实就是组件和组件的一种关联。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215546546.png" /></p>
<p>​ 但是我们训练一个生成器的时候，当我们训练一个网络的时候，是很难把组件和组件的关系放进去的。因为在NN网络这个架构里面，如果前一层的权重都已经确定了，最后的输出之间是独立的。每个输出神经元之间没法互相进行配合。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215603961.png" /></p>
<p>​ 其实像这种也可以解决，只不过需要更深的网络结构。更深的网络结构能够帮助你把这种组件与组件之间的相关性考虑进去。所以，如果有一个GAN网络，一个Auto-Encoder网络，要完成一个目标，往往Auto-Encoder的网络需要更深，才能达到和GAN类似的效果。</p>
<p>​ 其实讲到这里，都没有明确的回答Generator为什么不能自己学这个问题，但是其实已经回答掉了。因为我们刚才讲的，Auto-Encoder就是训练了一个单的Generator，是抛弃了Discriminator的一个架构。如果这样子让Generator自己学，当然可以，只不过会存在上述所说的种种问题和难处。</p>
<h4 id="discriminator为什么不能自己产生image">2、Discriminator为什么不能自己产生Image？</h4>
<p>​ Discriminator总的来说干了一件什么事呢？它其实就是输入一个对象，输出一个分数，这个分数表示这个对象多好或多差，进行评估。对于Discriminator来说，它要考虑组件和组件之间的关系就很容易了，因为他是看到整个图片的。</p>
<p>​ 那么我们其实是可以用Discriminator来产生Image的，只不过会非常的卡，非常的慢，为什么这么说呢，我们先看，如果硬要用Discriminator来产生Image，步骤应该是怎样的：</p>
<p>​ 假设我们已经训练好了一个好的判别器，那么我们只需要执行如下公式即可：</p>
<p>​ 穷举所有的x，然后一个个丢到判别器里，让它看是不是高分。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215739478.png" /></p>
<p>​ 虽然这看上去不是很合理，对于穷举而言。但是这其实并不是关键所在。我们现在先假设这个方法确实可行，那难道就可以了吗？别忘了，我们先前的假设：“假设我们已经训练好了一个好的判别器”，那么我们怎么训练这个判别器呢？这才是问题的关键！</p>
<p>​ 理论上讲很简单，我给他好的样例，让他输出高分，烂的样例，让他输出低分。但是实际上呢，我们手上只有好的样例。如果只有这样的训练实例，会导致网络看到啥都给高分。所以，怎么去产生不好的样例，产生怎样不好的样例，就是一个很关键的问题：</p>
<p>​ 如果不好的样例就只是一些噪音，那其实机器很容易会给一些尚可的模糊的图片打高分，这并不是我们希望看到的。如下图所示。所以<strong>只有差的样例足够的好，才能让机器真正的学会评判好的图片和坏的图片</strong>，</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215749015.png" /></p>
<p>​ 所以现在如何生成足够好的 差的样例就很重要，那么我们就需要一个生成差的样例的过程，在没有Generator的前提下，这就是一个鸡生蛋蛋生鸡的循环问题了。所以如果真的想要完成这件事情，只能通过一个迭代的形式去完成：</p>
<p>​ 我先学一个最差的Discriminator出来，然后用这个去做生成，生成出来的样例用来训练更好的Discriminator，循环往复。你会发现这个过程好像跟GAN差不多，但是如果没有generator，要比GAN累好多，慢很多。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215808954.png" /></p>
<p>​ 这时候再来看如果我们把Generator加上，GAN就是利用Generator来求解这个argmax的问题，用于生成一些负面的样例。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215815222.png" /></p>
<h3 id="五比较vae和gan的结果">五、比较VAE和GAN的结果：</h3>
<p>​ 下图是各个GAN和VAE在不同参数下，所表现出来的性能分数的值的区间。还附带了一个VAE和GAN生成人物头像的直观结果。</p>
<p>1、VAE比较稳定，给它不同的参数，所表现出来的性能相差不大。</p>
<p>2、GAN相对而言比较吃参数，但是只要参数正确，所生成的最好结果也是远远优于VAE产生的结果的。<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/2222sdsa.png" /></p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114215842339.png" /></p>
<h3 id="六总结">六、总结</h3>
<p>1）生成器：</p>
<p>​ 优势：很快就能生成一个东西</p>
<p>​ 劣势：很难察觉组件和组件之间的关系，只是模仿一个表象</p>
<p>2）判别器：</p>
<p>​ 优势：可以考虑大局观</p>
<p>​ 劣势：生成一个东西非常的困难</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114222543440.png" /></p>
<p>3）GAN的优势：</p>
<p>1、从判别器的角度而言，使用Generator来进行negative example的生成，更为高效。</p>
<p>2、从生成器的角度而言，它虽然仍然是一个组件一个组件的生成对象，但是相比传统的计算L1,L2的损失而言，它会从Discriminator那边得到全局视野的反馈，更能理解组件与组件之间的关系。</p>
]]></content>
      <categories>
        <category>GAN系列笔记</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记2——数据表征与自动编码器</title>
    <url>/2021/11/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B02%E2%80%94%E6%95%B0%E6%8D%AE%E8%A1%A8%E5%BE%81%E4%B8%8E%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8/</url>
    <content><![CDATA[<h2 id="数据表征与自动编码器">数据表征与自动编码器</h2>
<h3 id="前言">前言：</h3>
<p>​ 本文简单讲解了数据表征与自动编码器的相关知识，主要来说涵盖数据表征的概念与理解，简单自动编码器的组成，堆叠式自动编码器的组成与训练方法，卷积、循环、去噪、稀疏自动编码器的简单介绍，以及变分自动编码器较为详细的概念与一个有趣的应用。</p>
<h3 id="什么是数据表征">1、什么是数据表征？</h3>
<p>以下是两组数字顺序：</p>
<p>​ 40，27，25，36，81，57，10，73</p>
<p>​ 1，2，3，4，5，6，7，8，9，10，11，12，13，14，15，16，17</p>
<p>​ 简单来看，第一行的数据更少，信息量更小，但是其实第二行的数据具有很好的递增特征，反而更容易进行记忆。数据中的类似于这种的模式被我们称为数据表征。模式可以更好的帮助我们有效的存储信息。</p>
<h3 id="一个简单自动编码器的组成">2、一个简单自动编码器的组成</h3>
<p>​ 自动编码器由两部分组成：将输入转换为潜在表征的编码器（识别网络），以及将内部表征转换为输出的解码器（生成网络）。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114205901462.png" alt="image-20220114205901462" style="zoom:150%;" /></p>
<p>​ 上述示例就是一个由3个神经元组成的隐藏层（编码器），以及由6个神经元组成的输出层（解码器）。输出又通常称为重构，自动编码器会试图重构输入，且成本函数包含一个重构损失，重构和输入不一致的时候会进行惩罚。</p>
<p>​ 注意：自动编码器的输出数量应等于输入数量。同时，自动编码器本身其实是一个在学习数据潜在表征，然后再由表征还原数据的一个过程。它通常用于降维或者无监督的预训练。</p>
<h3 id="堆叠式自动编码器">3、堆叠式自动编码器</h3>
<p>​ 自动编码器可以有许多个隐藏层，就成为了堆叠式自动编码器（深度自动编码器）。但是我们需要注意不能使编码器太过于强大了。为什么呢？仍然以下述为例：</p>
<p>​ 40，27，25，36，81，57，10，73</p>
<p>​ 1，2，3，4，5，6，7，8，9，10，11，12，13，14，15，16，17</p>
<p>​ 如果编码器太过于强大了，可能他就会准确的记住每一个数字（也就是做了一个简单的一一映射，而没有去学习潜在的模式）。就好比一个记忆力非常好的人看到这两串数字，说我不需要去看这个数字里面到底存在什么样的模式，只要直接背就完了，没有区别。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114205946184.png" alt="image-20220114205946184" style="zoom: 150%;" /></p>
<p>​ 上图是一个常见的堆叠式自动编码器的示意图，往往会是这样一个类三明治结构。下图是一组输入与重构的可视化结果：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210006336.png" /></p>
<p>​ 为了更好的帮助理解自动编码器降维的用处，我们举一个简单的例子：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210017508.png" /></p>
<p>​ 在上图中这样一个简单的自动编码器网络中，其实质就是将一组3维数据通过编码器降到了2维。然后在通过解码器将2维数据重构至3维。实际操作过程中，构建好这样一组自动编码器网络后，你可以先使用解码器将数据降维，然后做一些学习，然后再将数据进行重构，能够帮助进行更好的实例泛化。</p>
<p>​ 同样，为了更好的帮助理解自动编码器用于无监督预训练的用处，我们也举一个简单的例子：</p>
<p>​ 先我们学过如果要处理一个复杂的有监督任务，但是没有很多标记好的数据，那么可以在网上寻找已知的类似任务的神经网络，并且重用其较低层的网络，然后用少量数据对高层网络进行训练即可。这样其就不必从底层特征开始学起，只会重用原有网络的底层特征检测。</p>
<p>​ 讲到这里，应该明白了，自动编码器其实也是同一个原理。刚才说了，自动编码器的第一步是编码器，编码器的本质是寻找数据的潜在表征，那不也就相当于做了底层的特征检测嘛，而且训练数据还是可以不需要标签的。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210030274.png" /></p>
<p>​</p>
<p>​ 所以，如上图所示，如果我们手上有一个大型数据集，但是难受的是大部分的数据都是未标记状态的，只有少数打上了标签。那么我们可以先使用所有的数据来训练一个自动编码器，训练完以后，将编码器部分的网络复制到我们新的神经网络中，然后在此基础上构建高层的神经网络，这样的话只要再用少部分带标记的数据训练一下该神经网络的高层部分就可以得到一个较好的效果了。</p>
<p>### 4、如何训练堆叠式自动编码器：</p>
<p>一般情况下，我们有两种方式可以用于训练自动编码器。</p>
<p>​ 1. 将解码器层的权重与编码器层的权重绑定起来。这样的话可以将模型中整体的训练参数减半，并且加快训练速度并且降低了过拟合的风险。</p>
<ol start="2" type="1">
<li><p>一次训练一个浅层自动编码器然后将他们进行堆叠。（这种技术现在用的较少），示意图如下：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210104140.png" /></p>
<p>​ 示意图本身已经较清楚的讲解了整个过程，我再做一遍简略的讲述。比如说，我们现在要训练最右侧这样一个堆叠式自动编码器，他的编码器部分有两层，我们将其分为两阶段来看这个问题。首先按训练第一个自动编码器，将隐藏层1的参数训练好。然后再训练第二个自动编码器，将隐藏层2的参数训练好。注意，在训练第二个自动编码器的时候，用到的输入其实是训练好的第一个自动编码器将输入编码后的结果。以公式的形式表达的话就是：</p>
<p>​ 我们假设输入为i，第一个自动编码器的编码函数为 <strong>f_encoder1() </strong>,解码函数为<strong>f_decoder1()</strong>,第二个自动编码器的编码函数为<strong>f_encoder2()</strong>,解码函数为<strong>f_decoder2()</strong>。</p>
<p>​ 那么在训练第一个自动编码器的时候，输入为i，重构输出为 <strong>f_decoder1(f_encoder1(i))</strong>。在训练第二个自动编码器的时候，输入为<strong>f_encoder1(i)</strong>，重构输出为<strong>f_decoder2( f_encoder2( f_encoder1(i) ))</strong></p></li>
</ol>
<h3 id="其他类型的自动编码器">5、其他类型的自动编码器</h3>
<p>​ <strong>(1) 卷积自动编码器</strong></p>
<p>​ 用于为图像构建自动编码器。一般编码器层是由卷积层和池化层组成的常规CNN。会减少输入的空间尺寸，增加深度即特征图的数量。解码器则进行相反的操作。</p>
<p>​ <strong>(2)循环自动编码器</strong></p>
<p>​ 用于为序列构建自动编码器，例如时间序列或文本。编码器通常为序列到向量的RNN，能够将输入序列压缩为单个向量。其可以处理任何长度的序列。</p>
<p>​ <strong>(3)去噪自动编码器</strong></p>
<p>​ 到目前为止，为了让自动编码器不过于强大，也就是为了让他学习一些数据的浅层特征，我们通过限制编码曾大小让它成为不完整的自动编码器。其实，我们还可以通过向其输入中添加噪声，训练它来恢复无噪声输入，也可以让其学习数据的有用特征。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210254970.png" /></p>
<p>​ 可以在编码器的输入中叠加高斯噪声，也可以加一层dropout层。去噪编码器不仅可以用于降维数据可视化或者无监督预训练，也可以用于图像去噪。</p>
<p>​ <strong>(4)稀疏自动编码器</strong></p>
<p>​ 使用稀疏性来对编码器进行约束，使其对良好的特征进行提取学习。通过在成本函数中添加适当的函数项，来强迫自动编码器减少编码层中活动神经元的数量。</p>
<p>​ 另一种方法是在每次训练迭代时测量编码层的实际稀疏度，并在测量稀疏度预目标稀疏度不同时对模型进行惩罚。</p>
<p>​ <strong>(5)变分自动编码器</strong></p>
<p>​ 变分自动编码器较为特殊，具体而言有两大特点：</p>
<p>​ <strong>特点1</strong>:概率自动编码器：意思是即使在训练好以后，它们的输出部分也会由概率决定。</p>
<p>​ <strong>特点2</strong>:生成式自动编码器：指它们可以生成看上去像是从训练集中采样的新实例。</p>
<p>​ 在先前讲到的自动编码器中，我们往往以一个数值来表示一个特征维度的值，如下 所示，将人脸图像编码成了一个六维向量，每个维度一个值。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210408918.png" /></p>
<p>​ 在上面的示例中，我们使用单个值来描述输入图像在潜在特征上的表现。但在实际情况中，我们可能更多时候倾向于将每个潜在特征表示为可能值的范围。</p>
<p>​ 而变分自编码器便是用“取值的概率分布”代替原先的单值来描述对特征的观察的模型，如下图的右边部分所示，经过变分自编码器的编码，每张图片的微笑特征不再是自编码器中的单值而是一个概率分布。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210429150.png" /></p>
<p>​ 所以在变分自动编码器中，我们现在将给定输入的每个潜在特征表示为概率分布。当从潜在状态解码时，我们将从每个潜在状态分布中随机采样，生成一个向量作为解码器模型的输入。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210436379.png" /></p>
<p>​ 那么如何训练变分自动编码器呢？其成本函数由应当如何设计？</p>
<p>​ 变分自动编码器学习的是隐变量（特征）Z的概率分布，因此在给定输入数据X的情况下，变分自动编码器的推断网络输出的应该是Z的后验分布p（z|x）。 但是这个p（z|x）后验分布本身是不好求的。所以有学者就想出了使用另一个可伸缩的分布q（z|x）来近似p（z|x）。通过深度网络来学习q（z|x）的参数，一步步优化q使其与p(z|x)十分相似，就可以用它来对复杂的分布进行近似的推理。</p>
<p>​ 这个时候就出现了成本函数的设计理念，成本函数由两部分组成，第一部分是通常的重构损失，我们可以使用交叉熵来衡量。第二部分是潜在损失。它就是上述提到的目标分布q和编码的实际分布p之间的KL散度。通过简化后，就可以得到以下公式：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210448726.png" /></p>
<p>​ L1是潜在损失，n是编码的维度，u(j)和sigma(j) 是第j个分量的均值和标准差。向量u和sigma由编码器进行输出。KL散度及如何通过KL散度推导出简化的公式部分本文进行了省略，详细可以看引用（2）。</p>
<p>​ 最后提一下整个变分自动编码器的网络结构如下：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210503507.png" /></p>
<p>​ 编码器产生平均编码u和标准差sigma，然后实际编码是从u和sigma的高斯分布中进行随机采样之后得到的。解码器正常解码采样得到的编码即可。</p>
<h4 id="一些有趣的应用">一些有趣的应用：</h4>
<p>​ 变分自动编码器使得语义插值成为了可能，可以在编码级别进行插值，而非在数据级别进行插值。总体而言就是如下图所示的一个过程：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114210518169.png" /></p>
<p><strong>文中引用与部分图片来源：</strong></p>
<p>1）《机器学习实战 基于Scikit-learn、keras、tensorflow》机械工业出版社</p>
<p>2） 一文理解变分自编码器（VAE）https://zhuanlan.zhihu.com/p/64485020</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Basic Concepts</tag>
        <tag>Auto Encoder</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记1——One-Hot编码与Word Embedding</title>
    <url>/2021/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B01%E2%80%94One-Hot%E7%BC%96%E7%A0%81%E4%B8%8EWord%20Embedding/</url>
    <content><![CDATA[<h3 id="一one-hot-encoding独热向量">一、One-Hot Encoding独热向量</h3>
<p>​ 独热向量是指使用N位0或1来对N个状态进行编码，每个状态都有它独立的表示形式，并且其中只有一位为1，其他位都为0。</p>
<p>​ 比如我们现在要编码apple，我们用5位向量来进行编码，如下所示：</p>
<ul>
<li><p>apple [1 0 0 0 0]</p></li>
<li><p>bag [0 1 0 0 0]</p></li>
<li><p>cat [0 0 1 0 0]</p></li>
<li><p>dog [0 0 0 1 0]</p></li>
<li><p>elephant [0 0 0 0 1]</p></li>
</ul>
<p>​ 如果我们现在想要编码其他另外的单词，那就需要更多位参与编码，但是这五个单词的编码前5位仍然能够是这样，只不过后面省略号省略的部分都为0罢了。</p>
<p>​ 使用这种独热向量的表示形式能够很好的对各种内容进行编码，但是它并没有考虑编码内容与内容之间的关联。就比如说，在上述例子中，cat，都属于动物，他们之间应当存在联系，apple 和 其它四个内容没有联系。但是，从上述的编码中我们没法看出cat。</p>
<p>​ 同时使用独热向量形成的特征矩阵会非常的稀疏，占用的空间非常的大。</p>
<h3 id="二word-embedding-词嵌入">二、Word Embedding 词嵌入</h3>
<p>​ Word Embedding就是为了解决One-hot编码的缺陷，其用一个向量来对一个词进行表示。其具有很强的表达关联特征的能力。</p>
<p>​ 比如说，我们以如下为例，我们现在要使用两种编码表示公主、王妃</p>
<ol type="1">
<li>使用One-Hot:</li>
</ol>
<p>​ 公主 [1 0]</p>
<p>​ 王妃 [0 1]</p>
<ol start="2" type="1">
<li><p>使用 Word Embedding:</p>
<p>公主 = 0.5 * 皇帝 + 0.125 * 宫里 + 0.5 * 女</p>
<p>王妃 = 0.3 * 皇帝 + 0.375 * 宫里 + 0.5 * 女</p>
<p>​ 因为我们就以三个额外的维度来进行编码，所以两者应当表示为三维的向量。于是，公主表示为 [0.5 0.125 0.5]，王妃表示为 [0.3,0.375,0.5]。这样一来，我们就可以知道公主和王妃在内在中，存在着某种意义上的关联。</p>
<p>​ 当然在上述例子中，系数都是举个例子随便写的，我们想要表达的意思就是，比如我们将要词映射到一个二维的特征空间中，每个词就可以表示成一个二维的点，那么我们就能知道词和词之间存在的某些关系。同理，如果词映射到一个三维的特征空间，那就是表示成三维空间中的一个点。</p>
<p>​ 如下图所示，就是一个直观的例子，我们将这么多词映射到二维空间中，就可以找到它们在二维空间的分布，找到其内在的词与词之间的关系，比如说，在图中我们就看到了三个明显的聚类。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114205646350.png" /></p>
<p>​ 那么Word Embedding一般是怎么做的呢？其就是一个全连接的神经网络层。如下所示：左边是一个以2×6的one-hot矩阵为输入，中间层节点数为3的全连接的神经网络层。从右边可以看出，这个计算过程就相当于从 w i,j 矩阵中取出第1,2行，跟字向量的查表操作是一样的（从表中找出对应的向量）。这样子编码得到的对应向量就是3维的。如果你想要编码得到n维的向量，那么中间层就应该为节点数为n的全连接神经网络层。一般我们在神经网络中看到的Embedding层就是这样子设计的。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114205700989.png" /></p>
<p>因此，Embedding层其实就是以one hot为输入、中间层节点为字向量维数的全连接层！而这个全连接层的参数，就是一个“字向量表”！这其实就是将读热编码后的稀疏矩阵经过一个线性变化（其实就是查表）将其转换成一个密集矩阵的过程。</p>
<p>最后总结一下，Embedding层最终完成的工作：</p>
<ul>
<li>将稀疏矩阵经过线性变换（查表）变成一个密集矩阵</li>
<li>这个密集矩阵用了N个特征来表示所有的词。密集矩阵中表象上是一个词和特征的关系系数，实际上蕴含了大量的词与词之间的内在关系。</li>
<li>它们之间的权重参数，用的是嵌入层学习来的参数进行表征的编码。在神经网络反向传播优化的过程中，这个参数也会不断的更新优化。</li>
</ul></li>
</ol>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Basic Concepts</tag>
        <tag>Info Encoding</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础系列笔记0——常见概念与函数（更新中）</title>
    <url>/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B00%E2%80%94%E5%B8%B8%E8%A7%81%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%87%BD%E6%95%B0%EF%BC%88%E6%9B%B4%E6%96%B0%E4%B8%AD%EF%BC%89/</url>
    <content><![CDATA[<h3 id="softmax和cross-entropy交叉熵函数">1、SoftMax和Cross-Entropy交叉熵函数</h3>
<p>​ 我们常常使用交叉熵（cross entropy）来进行判别分布的相似性，交叉熵公式如下图所示：公式中pi和qi<img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/wpsCF9D.tmp.jpg" alt="img" />为真实的样本分布和生成器的生成分布。</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114201037115.png" /></p>
<p>​ <strong><em>*Soft-Max*</em></strong>内部对输入进行的处理如下图所示，我们假设输入y1,y2,y3，我们先将三个输入计算exp(y1),exp(y2),exp(y3),然后求和，然后计算输出。</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114201047994.png" /></p>
<h3 id="kl散度">2、KL散度</h3>
<h3 id="sigmoid-function-s型激活函数">3、Sigmoid Function ( S型激活函数 )</h3>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114201125789.png" /></p>
<h3 id="rectified-linear-unit-relu激活函数">4、Rectified Linear Unit ( ReLu激活函数 )</h3>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114201156330.png" alt="image-20220114201156330" /><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114201159496.png" /></p>
<h3 id="update-epoch-batch-size-training-data">5、Update / Epoch / Batch Size / Training Data<br />
</h3>
<ul>
<li><p><strong>Update</strong>: 每做一次梯度下降，更新一次参数，就叫做一次Update</p></li>
<li><p><strong>Batch Size</strong>: 每次用来计算梯度下降更新参数用到的训练集中样本的个数</p></li>
<li><p><strong>Training Data</strong>: 所有的训练数据集样本，我们在训练时往往会将其分为一个Batch一个Batch，每次使用一个Batch来计算梯度，更新参数。</p></li>
<li><p><strong>Epoch</strong>: 每当使用过一轮所有的训练数据集样本以后，叫做一个Epoch</p></li>
</ul>
<p>整个的Optimization的过程如下所示：</p>
<p>​ <img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114201333605.png" /></p>
<p>下面是一些帮助理解的例子：</p>
<p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220114201341888.png" /></p>
<h3 id="backbone-head-neck-bottleneck">6、BackBone / Head / Neck / BottleNeck</h3>
<ul>
<li>BackBone：翻译为主干网络，其是神经网络的主干部分。这个主干网络大多时候指的是提取特征的网络，其作用就是提取图片中的信息，以供后面的网络使用。这些网络经常使用的是ResNet、VGG等，而不是由我们自己设计的网络，因为这些主干网络已经证明了它们在分类等问题上的特征提取能力是很强的。在用这些网络作为backbone的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的backbone模型已经具有提取特征的能力了，在我们的训练过程中，会对参数进行微调，使得其更适合于我们自己的任务。</li>
<li>Head：是获取网络输出内容的网络，head利用之前网络提取的这些特征，做出预测。</li>
<li>Neck: 是放在Backbone和Head之间的，是为了更好的利用Backbone提取的特征</li>
<li>Bottleneck: 瓶颈，通常指的是网络输入的数据维度和输出的维度不同，输出的维度比输入的小了许多.</li>
</ul>
<h3 id="ablation-study">7、Ablation Study</h3>
<p>​ 消融研究，消融研究通常是指删除模型或算法的某些“功能”，并查看其如何影响性能。</p>
<p>​ 在论文中一般来说会提出多个创新方法，或者新型结构模块，或注意力模块等。这些东西在一起为模型的性能作出了贡献。然而为了了解每个部分单独能发挥的作用，常常会在论文中提出消融研究。</p>
<p>​ 例如某论文提出了方法A,B,C。而该论文是基于某个baseline的改进。因此，在消融研究部分，会进行以下实验，baseline ，baseline+A，baseline+B, baseline+C, baseline+A+B+C等实验的各个评价指标有多少，从而得出每个部分所能发挥的作用有多大。</p>
<p>​ 比较简单来说，就是控制变量法，来判断某一个模块是否真的有作用。</p>
]]></content>
      <categories>
        <category>机器学习基础系列笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Basic Concepts</tag>
      </tags>
  </entry>
</search>
