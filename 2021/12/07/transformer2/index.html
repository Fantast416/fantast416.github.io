<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"blog.slks.xyz","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="Transformer架构最广泛用在Seq2Seq问题上，也就是Sequence-to-Sequence。区别于GAN所对应的Pix2Pix的问题，Seq2Seq的问题也会有非常多的应用和变式，其应用场景也是非常的广。Seq2Seq问题是指输入一个序列，输出一个序列，同时输出序列的长度由训练好的模型进行决定。比较经典的问题就是语音识别、机器翻译、语音翻译等领域。">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer系列笔记2——原始Transformer架构与Seq2Seq问题">
<meta property="og:url" content="https://blog.slks.xyz/2021/12/07/transformer2/index.html">
<meta property="og:site_name" content="Fantast&#39;s Blog">
<meta property="og:description" content="Transformer架构最广泛用在Seq2Seq问题上，也就是Sequence-to-Sequence。区别于GAN所对应的Pix2Pix的问题，Seq2Seq的问题也会有非常多的应用和变式，其应用场景也是非常的广。Seq2Seq问题是指输入一个序列，输出一个序列，同时输出序列的长度由训练好的模型进行决定。比较经典的问题就是语音识别、机器翻译、语音翻译等领域。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112932803.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112939154.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112946229.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112954818.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113003795.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113028194.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113035456.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113045808.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113058827.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113109977.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113200035.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113209026.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113211451.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113222282.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113244478.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113256453.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113305054.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113324988.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113331325.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113340249.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113351438.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113410387.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113420273.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113453661.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113456022.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113509972.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113522389.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113543303.png">
<meta property="og:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113600427.png">
<meta property="article:published_time" content="2021-12-07T13:31:19.000Z">
<meta property="article:modified_time" content="2022-01-18T05:52:47.321Z">
<meta property="article:author" content="ShenLvkesheng">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="Self Attention">
<meta property="article:tag" content="Seq2Seq">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112932803.png">


<link rel="canonical" href="https://blog.slks.xyz/2021/12/07/transformer2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://blog.slks.xyz/2021/12/07/transformer2/","path":"2021/12/07/transformer2/","title":"Transformer系列笔记2——原始Transformer架构与Seq2Seq问题"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Transformer系列笔记2——原始Transformer架构与Seq2Seq问题 | Fantast's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Fantast's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Fantast's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E7%9B%B8%E5%85%B3%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">一、相关背景介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8Cseq2seq2%E7%BB%93%E5%B1%80%E6%96%B9%E6%A1%88------origin-transformer"><span class="nav-number">2.</span> <span class="nav-text">二、Seq2Seq2结局方案——Origin Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1encoder"><span class="nav-number">2.1.</span> <span class="nav-text">1、Encoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2decoder"><span class="nav-number">2.2.</span> <span class="nav-text">2、Decoder</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83transformer"><span class="nav-number">3.</span> <span class="nav-text">三、如何训练Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E5%85%B3%E4%BA%8E%E8%AE%AD%E7%BB%83seq2seq%E7%B1%BB%E4%BC%BC%E6%A8%A1%E5%9E%8B%E7%9A%84tips"><span class="nav-number">4.</span> <span class="nav-text">四、关于训练Seq2Seq类似模型的Tips：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1copy-mechanism"><span class="nav-number">4.1.</span> <span class="nav-text">1、Copy Mechanism</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2guided-attention"><span class="nav-number">4.2.</span> <span class="nav-text">2、Guided Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3beam-search"><span class="nav-number">4.3.</span> <span class="nav-text">3、Beam Search</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4optimizing-evaluation-metric"><span class="nav-number">4.4.</span> <span class="nav-text">4、Optimizing Evaluation Metric</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5scheduled-sampling"><span class="nav-number">4.5.</span> <span class="nav-text">5、Scheduled Sampling</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ShenLvkesheng"
      src="/images/avatar.webp">
  <p class="site-author-name" itemprop="name">ShenLvkesheng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fantast416" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fantast416" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fantast416@gmail.com" title="E-Mail → mailto:fantast416@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.zhihu.com/people/shuo-shuo-3-41" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;shuo-shuo-3-41" rel="noopener" target="_blank">ZhiHu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.next.zju.edu.cn/" title="http:&#x2F;&#x2F;www.next.zju.edu.cn&#x2F;" rel="noopener" target="_blank">NextLab</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blog.slks.xyz/2021/12/07/transformer2/">
    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.webp">
      <meta itemprop="name" content="ShenLvkesheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Fantast's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Transformer系列笔记2——原始Transformer架构与Seq2Seq问题
        </h1>

        <div class="post-meta-container">
          
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-12-07 21:31:19" itemprop="dateCreated datePublished" datetime="2021-12-07T21:31:19+08:00">2021-12-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-01-18 13:52:47" itemprop="dateModified" datetime="2022-01-18T13:52:47+08:00">2022-01-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Transformer%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">Transformer系列笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

            <div class="post-description">Transformer架构最广泛用在Seq2Seq问题上，也就是Sequence-to-Sequence。区别于GAN所对应的Pix2Pix的问题，Seq2Seq的问题也会有非常多的应用和变式，其应用场景也是非常的广。Seq2Seq问题是指输入一个序列，输出一个序列，同时输出序列的长度由训练好的模型进行决定。比较经典的问题就是语音识别、机器翻译、语音翻译等领域。</div>
            <div class="post-remind">Google Chrome is recommended, otherwise the image size may be wrong</div>
            <div class="post-remind">FireFox is not recommended,Because it does not support the image zoom property</div>
        </div>
        
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>

<link href='https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; --title-bar-height:20px; }
.mac-os-11 { --title-bar-height:28px; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; inset: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 36px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
.typora-export .task-list-item input { pointer-events: none; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; image-orientation: from-image; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 4; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-fences-adv-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li blockquote { margin: 1rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; font-variant-ligatures: no-common-ligatures; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  .typora-export #write { break-after: avoid; }
  .typora-export #write::after { height: 0px; }
  .is-mac table { break-inside: avoid; }
  .typora-export-show-outline .typora-export-sidebar { display: none; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
#write.first-line-indent p > .md-image:only-child:not(.md-img-error) img { left: -2em; position: relative; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
.MathJax_ref { fill: currentcolor; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.6; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
.md-expand mark .md-meta { opacity: 0.3 !important; }
mark .md-meta { color: rgb(0, 0, 0); }
@media print {
  .typora-export h1, .typora-export h2, .typora-export h3, .typora-export h4, .typora-export h5, .typora-export h6 { break-inside: avoid; }
}
.md-diagram-panel .messageText { stroke: none !important; }
.md-diagram-panel .start-state { fill: var(--node-fill); }
.md-diagram-panel .edgeLabel rect { opacity: 1 !important; }
.md-fences.md-fences-math { font-size: 1em; }
.md-fences-advanced:not(.md-focus) { padding: 0px; white-space: nowrap; border: 0px; }
.md-fences-advanced:not(.md-focus) { background: inherit; }
.typora-export-show-outline .typora-export-content { max-width: 1440px; margin: auto; display: flex; flex-direction: row; }
.typora-export-sidebar { width: 300px; font-size: 0.8rem; margin-top: 80px; margin-right: 18px; }
.typora-export-show-outline #write { --webkit-flex:2; flex: 2 1 0%; }
.typora-export-sidebar .outline-content { position: fixed; top: 0px; max-height: 100%; overflow: hidden auto; padding-bottom: 30px; padding-top: 60px; width: 300px; }
@media screen and (max-width: 1024px) {
  .typora-export-sidebar, .typora-export-sidebar .outline-content { width: 240px; }
}
@media screen and (max-width: 800px) {
  .typora-export-sidebar { display: none; }
}
.outline-content li, .outline-content ul { margin-left: 0px; margin-right: 0px; padding-left: 0px; padding-right: 0px; list-style: none; }
.outline-content ul { margin-top: 0px; margin-bottom: 0px; }
.outline-content strong { font-weight: 400; }
.outline-expander { width: 1rem; height: 1.42857rem; position: relative; display: table-cell; vertical-align: middle; cursor: pointer; padding-left: 4px; }
.outline-expander::before { content: ""; position: relative; font-family: Ionicons; display: inline-block; font-size: 8px; vertical-align: middle; }
.outline-item { padding-top: 3px; padding-bottom: 3px; cursor: pointer; }
.outline-expander:hover::before { content: ""; }
.outline-h1 > .outline-item { padding-left: 0px; }
.outline-h2 > .outline-item { padding-left: 1em; }
.outline-h3 > .outline-item { padding-left: 2em; }
.outline-h4 > .outline-item { padding-left: 3em; }
.outline-h5 > .outline-item { padding-left: 4em; }
.outline-h6 > .outline-item { padding-left: 5em; }
.outline-label { cursor: pointer; display: table-cell; vertical-align: middle; text-decoration: none; color: inherit; }
.outline-label:hover { text-decoration: underline; }
.outline-item:hover { border-color: rgb(245, 245, 245); background-color: var(--item-hover-bg-color); }
.outline-item:hover { margin-left: -28px; margin-right: -28px; border-left: 28px solid transparent; border-right: 28px solid transparent; }
.outline-item-single .outline-expander::before, .outline-item-single .outline-expander:hover::before { display: none; }
.outline-item-open > .outline-item > .outline-expander::before { content: ""; }
.outline-children { display: none; }
.info-panel-tab-wrapper { display: none; }
.outline-item-open > .outline-children { display: block; }
.typora-export .outline-item { padding-top: 1px; padding-bottom: 1px; }
.typora-export .outline-item:hover { margin-right: -8px; border-right: 8px solid transparent; }
.typora-export .outline-expander::before { content: "+"; font-family: inherit; top: -1px; }
.typora-export .outline-expander:hover::before, .typora-export .outline-item-open > .outline-item > .outline-expander::before { content: "−"; }
.typora-export-collapse-outline .outline-children { display: none; }
.typora-export-collapse-outline .outline-item-open > .outline-children, .typora-export-no-collapse-outline .outline-children { display: block; }
.typora-export-no-collapse-outline .outline-expander::before { content: "" !important; }
.typora-export-show-outline .outline-item-active > .outline-item .outline-label { font-weight: 700; }
.md-inline-math-container mjx-container { zoom: 0.95; }


:root {
    --side-bar-bg-color: #fafafa;
    --control-text-color: #777;
}

@include-when-export url(https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext);

/* open-sans-regular - latin-ext_latin */
  /* open-sans-italic - latin-ext_latin */
    /* open-sans-700 - latin-ext_latin */
    /* open-sans-700italic - latin-ext_latin */
  html {
    font-size: 16px;
    -webkit-font-smoothing: antialiased;
}

body {
    font-family: "Open Sans","Clear Sans", "Helvetica Neue", Helvetica, Arial, 'Segoe UI Emoji', sans-serif;
    color: rgb(51, 51, 51);
    line-height: 1.6;
}

#write {
    max-width: 860px;
  	margin: 0 auto;
  	padding: 30px;
    padding-bottom: 100px;
}

@media only screen and (min-width: 1400px) {
	#write {
		max-width: 1024px;
	}
}

@media only screen and (min-width: 1800px) {
	#write {
		max-width: 1200px;
	}
}

#write > ul:first-child,
#write > ol:first-child{
    margin-top: 30px;
}

a {
    color: #4183C4;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 1rem;
    font-weight: bold;
    line-height: 1.4;
    cursor: text;
}
h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}
h1 tt,
h1 code {
    font-size: inherit;
}
h2 tt,
h2 code {
    font-size: inherit;
}
h3 tt,
h3 code {
    font-size: inherit;
}
h4 tt,
h4 code {
    font-size: inherit;
}
h5 tt,
h5 code {
    font-size: inherit;
}
h6 tt,
h6 code {
    font-size: inherit;
}
h1 {
    font-size: 2.25em;
    line-height: 1.2;
    border-bottom: 1px solid #eee;
}
h2 {
    font-size: 1.75em;
    line-height: 1.225;
    border-bottom: 1px solid #eee;
}

/*@media print {
    .typora-export h1,
    .typora-export h2 {
        border-bottom: none;
        padding-bottom: initial;
    }

    .typora-export h1::after,
    .typora-export h2::after {
        content: "";
        display: block;
        height: 100px;
        margin-top: -96px;
        border-top: 1px solid #eee;
    }
}*/

h3 {
    font-size: 1.5em;
    line-height: 1.43;
}
h4 {
    font-size: 1.25em;
}
h5 {
    font-size: 1em;
}
h6 {
   font-size: 1em;
    color: #777;
}
p,
blockquote,
ul,
ol,
dl,
table{
    margin: 0.8em 0;
}
li>ol,
li>ul {
    margin: 0 0;
}
hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

li p.first {
    display: inline-block;
}
ul,
ol {
    padding-left: 30px;
}
ul:first-child,
ol:first-child {
    margin-top: 0;
}
ul:last-child,
ol:last-child {
    margin-bottom: 0;
}
blockquote {
    border-left: 4px solid #dfe2e5;
    padding: 0 15px;
    color: #777777;
}
blockquote blockquote {
    padding-right: 0;
}
table {
    padding: 0;
    word-break: initial;
}
table tr {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}
table tr:nth-child(2n),
thead {
    background-color: #f8f8f8;
}
table th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    margin: 0;
    padding: 6px 13px;
}
table td {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 6px 13px;
}
table th:first-child,
table td:first-child {
    margin-top: 0;
}
table th:last-child,
table td:last-child {
    margin-bottom: 0;
}

.CodeMirror-lines {
    padding-left: 4px;
}

.code-tooltip {
    box-shadow: 0 1px 1px 0 rgba(0,28,36,.3);
    border-top: 1px solid #eef2f2;
}

.md-fences,
code,
tt {
    border: 1px solid #e7eaed;
    background-color: #f8f8f8;
    border-radius: 3px;
    padding: 0;
    padding: 2px 4px 0px 4px;
    font-size: 0.9em;
}

code {
    background-color: #f3f4f4;
    padding: 0 2px 0 2px;
}

.md-fences {
    margin-bottom: 15px;
    margin-top: 15px;
    padding-top: 8px;
    padding-bottom: 6px;
}


.md-task-list-item > input {
  margin-left: -1.3em;
}

@media print {
    html {
        font-size: 13px;
    }
    table,
    pre {
        page-break-inside: avoid;
    }
    pre {
        word-wrap: break-word;
    }
}

.md-fences {
	background-color: #f8f8f8;
}
#write pre.md-meta-block {
	padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block>.code-tooltip {
	bottom: .375rem;
}

.md-mathjax-midline {
    background: #fafafa;
}

#write>h3.md-focus:before{
	left: -1.5625rem;
	top: .375rem;
}
#write>h4.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h5.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h6.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
.md-image>.md-meta {
    /*border: 1px solid #ddd;*/
    border-radius: 3px;
    padding: 2px 0px 0px 4px;
    font-size: 0.9em;
    color: inherit;
}

.md-tag {
    color: #a7a7a7;
    opacity: 1;
}

.md-toc { 
    margin-top:20px;
    padding-bottom:20px;
}

.sidebar-tabs {
    border-bottom: none;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

/** focus mode */
.on-focus-mode blockquote {
    border-left-color: rgba(85, 85, 85, 0.12);
}

header, .context-menu, .megamenu-content, footer{
    font-family: "Segoe UI", "Arial", sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state{
    visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
    background-color: #fafafa;
    background-color: var(--side-bar-bg-color);
}

.md-lang {
    color: #b4654d;
}

/*.html-for-mac {
    --item-hover-bg-color: #E6F0FE;
}*/

#md-notification .btn {
    border: 0;
}

.dropdown-menu .divider {
    border-color: #e5e5e5;
    opacity: 0.4;
}

.ty-preferences .window-content {
    background-color: #fafafa;
}

.ty-preferences .nav-group-item.active {
    color: white;
    background: #999;
}

.menu-item-container a.menu-style-btn {
    background-color: #f5f8fa;
    background-image: linear-gradient( 180deg , hsla(0, 0%, 100%, 0.8), hsla(0, 0%, 100%, 0)); 
}



</style><title>Transformer系列笔记2—原始Transformer架构与Seq2Seq问题</title>
</head>
<body class='typora-export os-windows'><div class='typora-export-content'>
<div id='write'  class=''><h3 id='一相关背景介绍'><span>一、相关背景介绍</span></h3><p><span>	</span><span>Transformer架构最广泛用在Seq2Seq问题上，也就是Sequence-to-Sequence。区别于GAN所对应的Pix2Pix的问题，Seq2Seq的问题也会有非常多的应用和变式，其应用场景也是非常的广。</span></p><p><span>	</span><span>Seq2Seq问题是指输入一个序列，输出一个序列，同时输出序列的长度由训练好的模型进行决定。比较经典的问题就是语音识别、机器翻译、语音翻译等领域。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112932803.png" referrerpolicy="no-referrer" alt="image-20220115112932803"></p><p><span>	</span><span>同时，Seq2Seq也可以用于Chatbot聊天机器人，比如说如下所示：我们可以将Person1和Person2的一组对话视为一个input和一个对应的response，两者都是序列。或者一些其他的Question&amp;Answering问题都可以广泛的应用Seq2Seq的模型解决思想。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112939154.png" referrerpolicy="no-referrer" alt="image-20220115112939154"></p><p><span>	</span><span>具体的一些相关论文可以参照下图所示的网址进一步了解。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112946229.png" referrerpolicy="no-referrer" alt="image-20220115112946229"></p><p><span>	</span><span>同时，还有更多的可以应用Seq2Seq的例子，比如说对于一个Syntactic Parsing问题，也就是语法解析，我们也可以将其视为Seq2Seq的形式，来对其进行思考。输入是一句话，毫无疑问是一个Seq序列。但是，我们所需要的输出结果看上去像是一个树形结构【deep learning组成名词短语，very powerful 组成形容词短语，is 和 very powerful又组成动词短语，最后deep learning和 is very powerful组成一个句子】，但其实，如下图所示，我们可以将图最上方的这个句子作为输出结果，这样就将一个类似图结构的内容转换成了Seq的形式，同时也包含了所有我们需要的结果信息，然后用Seq2Seq的思想去训练模型即可。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115112954818.png" referrerpolicy="no-referrer" alt="image-20220115112954818"></p><p><span>	</span><span>再比如，Seq2Seq应用于图像领域的目标检测，感兴趣可以查看以下这篇论文。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113003795.png" referrerpolicy="no-referrer" alt="image-20220115113003795"></p><p>&nbsp;</p><h3 id='二seq2seq2结局方案------origin-transformer'><span>二、Seq2Seq2结局方案——Origin Transformer</span></h3><p><span>	</span><span>对于一般的Seq2Seq问题，笼统而言，我们一般就是将输入序列经过一个Encoder然后再通过一个Decoder就得到我们的输出序列。但是这个Encoder和Decoder内部的结构就大有讲究，最经典的架构就是Transformer。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113028194.png" referrerpolicy="no-referrer" alt="image-20220115113028194"></p><h4 id='1encoder'><span>1、Encoder</span></h4><p><span>	</span><span>首先我们关注Transformer的Encoder部分。下面是从输入输出的结果来看，Encoder的输入是一个序列，输出是一个编码过后的序列。（其实只从输入输出来看，像RNN、CNN都可以做到输入一个序列，输出一个序列，但是此篇讲的是Transformer）</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113035456.png" referrerpolicy="no-referrer" alt="image-20220115113035456"></p><p><span>	</span><span>Transfomer中的Encoder现在一般都由这样的架构组成，如下图所示，由非常多的Block组成，每一个Block并不是指一层网络，可能是有许多曾网络组成。那么输入经过许多个Block的处理，然后输出。那么在Transformer中，每一个Block内部的架构如右侧所示：输入一排向量，先经过Self-attention的机制，输出一排向量，然后这一排向量再经过全连接层，输出一排向量，这一排带红色框的向量就是Block的输出。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113045808.png" referrerpolicy="no-referrer" alt="image-20220115113045808"></p><p><span>	</span><span>但实际上，在原来的Transformer中，Block内的结构稍微更复杂一些，其还增加了一个residual的过程，如下所示：</span></p><p><span>	</span><span>输入的某个向量b经过Self-attention以后，得到向量a,此向量a还要与向量b相加，得到一个新的向量a+b，然后再做Layer Norm,得到一个深蓝色的向量c。（左上角的那个框框代表的就是，接下去接右侧的过程）</span></p><p><span>	</span><span>然后这个Norm输出的深蓝色的向量c，经过FC层，得到新的向量d，此处还有一个residual的架构，c和d相加，得到新的向量e，然后e再做一次Layer Norm才会得到最终这个Block输出的结果向量。</span></p><p><span>（LayerNorm本身的过程如图中央所示，先计算向量数据的均值和标准差，然后依据公式xi’ = (xi-m)/σ即可完成归一化。）</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113058827.png" referrerpolicy="no-referrer" alt="image-20220115113058827"></p><p><span>	</span><span>讲完每一个Block内完成的事情，我们就可以看Transfomer中给出的这个完整的带细节的架构示意图，其实两者中有部分过程是重叠的，只是表示形式不一样。</span></p><p><span>	</span><span>首先将输入经过嵌入层，完成编码。如果对嵌入层没有印象，可以查看如下的文章。然后通过Positional Encoding向输入中增加序列不同地方的位置信息。然后经过多次结构的核心部分Nx。这个Nx其实就是我们刚才提到的一个Block。其中，Multi-Head Attention就是Self-Attention的一种扩展的架构，然后Add&amp;Norm就是上述的Residual + LayerNorm的过程。Feed Forward就是上图中通过FC层的过程。所以这张图中Nx内部的这个框框就是一个Block的过程，然后整个Transformer Encoder就由许多个这样的Block组成。【像先前所讲的】</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113109977.png" referrerpolicy="no-referrer" alt="image-20220115113109977"></p><p><span>	</span><span>其实Transformer Encoder的架构也是可以灵活改变的，此处只是讲了最初提出的Transformer时候文章中Encoder的架构。</span></p><h4 id='2decoder'><span>2、Decoder</span></h4><p><span>	</span><span>然后的话我们关注Transformer的Decoder部分。Decoder分为两种，一种是Autoregressive（AT）的Decoder,还有一种是Non-Autoregressive（NAT）的Decoder。首先是Autoregressive的Decoder的介绍：</span></p><p><span>	</span><span>Decoder的输入，有两个内容，一个是Encoder输出的序列，还有一个是START标识符号所对应的One-hot编码，这两样东西经过Decoder后，输出一个各种字的概率分布，其中概率最大的那个字就会被作为最后的输出字符。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113200035.png" referrerpolicy="no-referrer" alt="image-20220115113200035"></p><p><span>	</span><span>然后我们将START 和 机 组成的one-hot向量的序列输入，再经过Decoder，得到一个概率分布，然后选出概率分布中最高的那个字“机”。以此类推，直到完成所有的输出。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113209026.png" referrerpolicy="no-referrer" alt="image-20220115113209026"></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113211451.png" referrerpolicy="no-referrer" alt="image-20220115113211451"></p><p><span>	</span><span>在此处其实存在两个问题 ，一个问题是按照上述的过程，其实Decoder后面每一步的输入都来自于自己前一步的输出，就比如说，中间某一步输入是“机器”，输出了“学”，然后下一步输入是“机器学”，输出是“习”。那么，我们要如何避免一步错步步错的情况发生呢？也就是说，如果其中有一步发生了问题，我们应当怎么让机器在后面的步骤中仍然输出正确的结果。第二个问题是，如果按照上述的步骤生成下去，生成出来的序列是无穷无尽的，它永远不会停止，那么机器怎么去决定输出序列的长度呢？这两个问题，后文会有提及，我们先继续讲述Decoder内部到底经历了什么。</span></p><p><span> </span></p><p><span>	</span><span>我们先来看一张Encoder和Decoder的对比图：从此图而言，我们发现Decoder除了中间那一块被盖住的，以及最后输出层的一些内容以外，Block内部的内容其实和Encoder是类似的。一个比较重要的差别点就是其使用的是Masked Multi-Head Attention。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113222282.png" referrerpolicy="no-referrer" alt="image-20220115113222282"></p><p><span>	</span><span>我们先来看一下，这个Masked Multi-Head Attention是什么意思呢？图1是我们熟悉的Self-Attention，每一个输出的向量都是含有输入的所有向量的咨询的，右图是Masked Self Attention机制，也就是在生成b1的时候，是不能使用a1之后的输入向量的，只能使用a1的资讯，生成b2的时候，不能使用a2之后的输入向量，只能使用a1,a2的资讯，以此类推。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113244478.png" referrerpolicy="no-referrer" alt="image-20220115113244478"></p><p><span>下面是一个更为具体的示意图：</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113256453.png" referrerpolicy="no-referrer" alt="image-20220115113256453"></p><p><span>	</span><span>那么为什么要使用这样的架构呢？原因其实很简单，因为如果不这样子的话，AT形式的Decoder是没法的运作的。因为先前讲的AT的Decoder运作方式，输出的字是一个一个产生的。第一步只有一个输入的“START”标识符对应的One-Hot向量，生成一个字以后，再将START标识符和第一个字输入，得到第二个字，依次类推。所以在生成的时候，它只能参考它前面的输入的序列的资讯，没法获得它之后的资讯。</span></p><p><span>	</span><span>然后我们来看一下Encoder间和Decoder间是如何传递资讯的呢？也就是刚才那张图中，暂时被灰色遮住的部分：我们会发现，在这个红色框框框起来的模组中，有两个输入的箭头来自于Encoder，一个输入的箭头来自于Decoder的前一步的输出。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113305054.png" referrerpolicy="no-referrer" alt="image-20220115113305054"></p><p><span>	</span><span>那么具体来说，是怎么工作的呢，如下所示：此步中的Multi-Head Attention指的是一个Cross-Attention的过程，具体如下所示：在Decoder解析的第一步时，输入是来自Encoder的一个序列，以及Decoder上一步的输出（因为Decoder上一步无输出，此步为第一步，所以这个应该是一个START标识符，表示句子的开始）。</span></p><p><span>	</span><span>然后Encoder这边输出的序列，a1,a2,a3向量分别乘以一个权重矩阵，形成k1,v1,k2,v2,k3,v3，Deocder这侧的START标识符对应的One-hot编码经过Masked-Self-Attention得到一个向量，然后该向量乘上一个权重矩阵，得到向量q,计算q和k1,k2,k3的相关性并且归一化，得到系数α1’，α2’，α3’.</span></p><p><span>	</span><span>得到系数α1’，α2’，α3’后，分别和v1,v2,v3相乘相加，得到向量v，然后再经过FC就是该步骤Cross attention的输出，至于后面就是需要再经过上述网络中描述的Add&amp;Norm，也就是Residual和Layer Norm的过程，才完成Decoder中一个Block的过程。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113324988.png" referrerpolicy="no-referrer" alt="image-20220115113324988"></p><p><span>	</span><span>然后第二步以此类推，如下图所示，此处就不再赘述。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113331325.png" referrerpolicy="no-referrer" alt="image-20220115113331325"></p><p><span>	</span><span>讲到这里的话，Decoder的基本架构也都已经清晰了，我们来解决一下先前的两个遗留问题之一，也就是机器怎么决定输出的序列的长度的？</span></p><p><span>	</span><span>在AT的Decoder中，我们其实可以在输出的字符集中增加一个叫做END的字符，它就代表一个句子的结束，如果某次Decoder输出的概率分布中，END字符的概率较高，就说明该句子结束了。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113340249.png" referrerpolicy="no-referrer" alt="image-20220115113340249"></p><p><span>	</span><span>那么什么又是NAT的Decoder呢？我们先前看到AT的Decoder是一个个输出字符的，NAT的Decoder则是一次性的输入全是START的序列，然后一次性得到输出的字符序列。那么NAT的Decoder又是如何决定输出序列的长度的呢？有两种办法，一个是专门训练一个分类器或者预测器，去预测输出序列的长度，另一个就是我们先假设这个输出的序列不会超过每个定值，比如说300个字符，那么我们就输入一个300个START组成的序列，然后在输出的字符序列中，忽略END字符后面的字符即可。</span></p><p><span>	</span><span>相比AT来说，NAT有着并行化，更稳定的优势，但是NAT的效果往往比AT要差一些。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113351438.png" referrerpolicy="no-referrer" alt="image-20220115113351438"></p><p>&nbsp;</p><h3 id='三如何训练transformer'><span>三、如何训练Transformer</span></h3><p><span>	</span><span>首先，当我们丢入第一个START字符的时候，希望Decoder输出的Distribution分布，和我们的Ground Truth的结果，能够越接近越好，也就是要最小化cross entropy的值。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113410387.png" referrerpolicy="no-referrer" alt="image-20220115113410387"></p><p><span>	</span><span>由于我们会输出好多次字符，所以我们最终是希望，能够最小化每次输出的Distribution和Ground Truth的cross entropy。不要忘记，最后一步输出段落的结尾符号，也要考虑在内。【此处有一个题外话，在实现的时候，其实有些时候可以将START的标识符和END标识符表示为同一个One-Hot编码，因为反正START标识符只会出现在句子的头部，仍然是可以分辨的，不需要区分START和END这两个字符】</span></p><p><span>	</span><span>同时，此处我们在训练的时候，Decoder的Input不是上一步输出的内容，而是给它正确的答案。这件事情就被我们叫做Teacher Focing。但是这样子和在实际生产使用的时候存在一个不匹配的问题，这个问题我们之后会有所讨论。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113420273.png" referrerpolicy="no-referrer" alt="image-20220115113420273"></p><p>&nbsp;</p><h3 id='四关于训练seq2seq类似模型的tips'><span>四、关于训练Seq2Seq类似模型的Tips：</span></h3><h4 id='1copy-mechanism'><span>1、Copy Mechanism</span></h4><p><span>	</span><span>这一个机制在聊天机器人或文档摘要等领域应用的比较广泛，举例来说，有的时候输出的某些内容可以从输入中Copy进行完成，这样就避免机器去学习一些奇怪的词汇，比如说在聊天机器人中库洛洛这样一个人名信息。感兴趣可以继续查看相关的论文</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113453661.png" referrerpolicy="no-referrer" alt="image-20220115113453661"></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113456022.png" referrerpolicy="no-referrer" alt="image-20220115113456022"></p><h4 id='2guided-attention'><span>2、Guided Attention</span></h4><p><span>	</span><span>由于有的时候机器可能会忽略输入的某一些部分，此时就可以使用该种手段，该手段通常应用于语音合成、语音辨识领域。</span></p><p><span>其就是要求机器在做Attention的时候以一个固定的方式，也就是说如果你对于某一个任务已经有了一些既定的发现，就可以将这种限制加入到训练的过程中，引导机器完成Attention的计算过程。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113509972.png" referrerpolicy="no-referrer" alt="image-20220115113509972"></p><p>&nbsp;</p><h4 id='3beam-search'><span>3、Beam Search</span></h4><p><span>	</span><span>BeamSearch是为了解决如下的一个问题：如下图所示，举例而言，比如说我们输出的字符只有A和B两种，那么Decoder的输出序列，就可以表示为如下图所示的一颗数状结构。红色的路径是Decoder按照先前的贪婪规则得到的输出。但其实，有一条比红色路径更好的输出，就是绿色的路径。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113522389.png" referrerpolicy="no-referrer" alt="image-20220115113522389"></p><p><span>	</span><span>可是，我们如果想要找到一个训练过程中这样一条绿色的最优路径是比较难的，因为我们不可能去穷举搜索每一条路径，因为在字符集比较大的情况下，几层叠加的可能就已经非常之多。这个时候就可以使用Beam Search这样一种技术。具体如何进行可以Google搜索详情</span></p><p>&nbsp;</p><h4 id='4optimizing-evaluation-metric'><span>4、Optimizing Evaluation Metric</span></h4><p><span>	</span><span>我们在训练的时候，目标是最小化每个一一对应的中文字的输出的分布和Ground Truth，而在评估一个模型好坏的时候，我们往往会使用输出的整句和GroundTruth之间的BLEU score来进行评估，所以我们的验证集应当使用BLEU score。那么我们的训练过程中为什么不使用BLEU score呢？简单来说就是如果在训练过程中我们要做两个句子之间的BLEU score，是根本没有办法做微分的也就没有办法做梯度下降去最优化求解。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113543303.png" referrerpolicy="no-referrer" alt="image-20220115113543303"></p><p>&nbsp;</p><h4 id='5scheduled-sampling'><span>5、Scheduled Sampling</span></h4><p><span>	</span><span>这个方法是为了解决我们先前提出的两个问题中的第一个问题，以及在那之后提出的一个训练与实际应用过程中的那个Mismatch(也就是训练的时候Decoder的输入使用的是Ground Truth,可是输出的时候不行)。简单直接的想法，就是训练的时候，我们往输入中加入一些噪声，就可以了。具体而言的话，也有许多论文是做这个方向的，如果有兴趣的话可以自行查阅。</span></p><p><img src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220115113600427.png" referrerpolicy="no-referrer" alt="image-20220115113600427"></p></div></div>
</body>
</html>
    </div>

    
    
    
      
  <div class="popular-posts-header">Related Posts</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\12\06\transformer1\" rel="bookmark">Transformer系列笔记1——Self Attention机制</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\12\09\transformer4\" rel="bookmark">Transformer系列笔记4——Swin Transformer思想与架构</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\12\08\transformer3\" rel="bookmark">Transformer系列笔记3——Vision Transformer思想与架构</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          <div class="reward-container">
  <div></div>
  <button>
    Donate
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.png" alt="ShenLvkesheng WeChat Pay">
        <span>WeChat Pay</span>
      </div>

  </div>
</div>

          <div class="post-tags">
              <a href="/tags/Transformer/" rel="tag"># Transformer</a>
              <a href="/tags/Self-Attention/" rel="tag"># Self Attention</a>
              <a href="/tags/Seq2Seq/" rel="tag"># Seq2Seq</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/12/06/transformer1/" rel="prev" title="Transformer系列笔记1——Self Attention机制">
                  <i class="fa fa-chevron-left"></i> Transformer系列笔记1——Self Attention机制
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/12/08/transformer3/" rel="next" title="Transformer系列笔记3——Vision Transformer思想与架构">
                  Transformer系列笔记3——Vision Transformer思想与架构 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ShenLvkesheng</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"fantast416","repo":"myblog-comment","client_id":"5889deaac4d791879f9a","client_secret":"ab0fce7b7a7466f56f12be1dff5b4def256ea772","admin_user":"fantast416","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"b9fdedb409ad14754e9c92703c09bd95"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
